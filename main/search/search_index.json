{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GFM-RAG Documentation","text":"<p>Welcome to the documentation for GFM-RAG project.</p>"},{"location":"#overview","title":"Overview","text":"<p>The GFM-RAG is the first graph foundation model-powered RAG pipeline that combines the power of graph neural networks to reason over knowledge graphs and retrieve relevant documents for question answering.</p> <p></p> <p>We first build a knowledge graph index (KG-index) from the documents to capture the relationships between knowledge. Then, we feed the query and constructed KG-index into the pre-trained graph foundation model (GFM) retriever to obtain relevant documents for LLM generation. The GFM retriever experiences large-scale training and can be directly applied to unseen datasets without fine-tuning.</p> <p>For more details, please refer to our project and paper.</p> <p>[\u4e2d\u6587\u89e3\u8bfb]</p>"},{"location":"#news","title":"\ud83c\udf89 News","text":"<ul> <li>[2025-02-06] We have released the GFM-RAG codebase and a 8M pre-trained model. \ud83d\ude80</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Graph Foundation Model (GFM): A graph neural network-based retriever that can reason over the KG-index.</li> <li>Knowledge Graph Index: A knowledge graph index that captures the relationships between knowledge.</li> <li>Efficiency: The GFM-RAG pipeline is efficient in conducting multi-hop reasoning with single-step retrieval.</li> <li>Generalizability: The GFM-RAG can be directly applied to unseen datasets without fine-tuning.</li> <li>Transferability: The GFM-RAG can be fine-tuned on your own dataset to improve performance on specific domains.</li> <li>Compatibility: The GFM-RAG is compatible with arbitrary agent-based framework to conduct multi-step reasoning.</li> <li>Interpretability: The GFM-RAG can illustrate the captured reasoning paths for better understanding.</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#013","title":"0.1.3","text":"<ul> <li>Add NVIDIA backend for LLM api</li> <li>Fix Ollama Error in OpenIE extraction</li> <li>Replace torch_scatter with native torch scatter</li> <li>Improve documentation</li> </ul>"},{"location":"CHANGELOG/#012","title":"0.1.2","text":"<ul> <li>Fix typos</li> <li>Remove example data from git</li> </ul>"},{"location":"CHANGELOG/#011","title":"0.1.1","text":"<ul> <li>Fix typos</li> <li>Fix pypi publish</li> </ul>"},{"location":"CHANGELOG/#010","title":"0.1.0","text":"<ul> <li>Initial release</li> </ul>"},{"location":"DEVELOPING/","title":"GFM-RAG Development","text":""},{"location":"DEVELOPING/#requirements","title":"Requirements","text":"Name Installation Purpose Python 3.12 Download The library is Python-based. Poetry Instructions Poetry is used for package management and virtualenv management in Python codebases"},{"location":"DEVELOPING/#getting-started","title":"Getting Started","text":""},{"location":"DEVELOPING/#install-dependencies","title":"Install Dependencies","text":"Bash<pre><code># install python dependencies\npoetry install\n</code></pre>"},{"location":"DEVELOPING/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<p>Set up pre-commit hooks for development:</p> Bash<pre><code>pre-commit install\n</code></pre>"},{"location":"DEVELOPING/#cuda-installation","title":"CUDA Installation","text":"<p>GFM-RAG require the <code>nvcc</code> compiler to compile the <code>rspmm</code> kernel. If you encounter errors related to CUDA, make sure you have the CUDA toolkit installed and the <code>nvcc</code> compiler is in your PATH. Meanwhile, make sure your CUDA_HOME variable is set properly to avoid potential compilation errors, e.g.,</p> Bash<pre><code>export CUDA_HOME=/usr/local/cuda-12.4\n</code></pre>"},{"location":"DEVELOPING/#repository-structure","title":"Repository Structure","text":"<p>An overview of the repository's top-level folder structure is provided below, detailing the overall design and purpose.</p> Bash<pre><code>gfm_rag/                     # Root directory\n\u251c\u2500\u2500 docs/                    # Documentation\n|   \u251c\u2500\u2500 DEVELOPING.md         # Development guide\n|   |\u2500\u2500 CHANGELOG.md             # Project changelog\n\u2502   \u251c\u2500\u2500 config/             # Configuration documentation\n\u2502   \u2502   \u251c\u2500\u2500 kg_index_config.md\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 workflow/           # Workflow documentation\n\u2502       \u251c\u2500\u2500 kg_index.md\n\u2502       \u251c\u2500\u2500 training.md\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 gfmrag/                 # Main package\n|   \u251c\u2500\u2500 gfmrag_retriever.py # GFM-RAG retriever\n|   \u251c\u2500\u2500 kg_indexer.py       # KG-index builder\n|   \u251c\u2500\u2500 models.py          # GFM models\n|   \u251c\u2500\u2500 losses.py       # Training losses\n|   \u251c\u2500\u2500 doc_rankers.py   # Document rankers\n\u2502   \u251c\u2500\u2500 datasets/           # Dataset implementations\n\u2502   \u2502   \u251c\u2500\u2500 qa_dataset.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 kg_construction/    # Knowledge graph construction\n\u2502   \u2502   \u251c\u2500\u2500 entity_linking_model/ # Entity linking models\n\u2502   \u2502   \u251c\u2500\u2500 ner_model/ # Named entity recognition models\n\u2502   \u2502   \u251c\u2500\u2500 openie_model/ # OpenIE models\n\u2502   \u2502   \u251c\u2500\u2500 kg_constructor.py # KG constructor\n\u2502   \u2502   \u251c\u2500\u2500 qa_constructor.py # QA constructor\n\u2502   \u2502   \u2514\u2500\u2500 utils.py\n\u2502   \u251c\u2500\u2500 ultra/             # ultra models\n\u2502   \u2502   \u251c\u2500\u2500 models.py\n\u2502   \u2502   \u251c\u2500\u2500 layers.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n|   \u251c\u2500\u2500 workflow/              # Training and inference scripts\n|   \u2502   \u251c\u2500\u2500 config/           # Configuration files\n|   \u2502   \u2502   \u251c\u2500\u2500 stage1_index_dataset.yaml\n|   \u2502   \u2502   \u251c\u2500\u2500 stage2_qa_finetune.yaml\n|   \u2502   \u2502   \u251c\u2500\u2500 stage3_qa_inference.yaml\n|   \u2502   \u2502   \u2514\u2500\u2500 ...\n|   \u2502   \u251c\u2500\u2500 stage1_index_dataset.py\n|   \u2502   \u251c\u2500\u2500 stage2_qa_finetune.py\n|   \u2502   \u2514\u2500\u2500 stage3_qa_inference.py\n\u2502   \u251c\u2500\u2500 llms/              # Language models\n\u2502   \u251c\u2500\u2500 evaluation/         # Evaluator for QA\n\u2502   \u2514\u2500\u2500 utils/             # Utility functions\n\u251c\u2500\u2500 tests/                  # Test cases\n\u251c\u2500\u2500 scripts/                  # Scripts for running experiments\n\u251c\u2500\u2500 mkdocs.yml           # Documentation configuration\n\u251c\u2500\u2500 poetry.lock         # Poetry lock file\n\u2514\u2500\u2500 pyproject.toml      # Project configuration\n</code></pre>"},{"location":"DEVELOPING/#common-commands","title":"Common Commands","text":"<p>Serve the documentation locally:</p> Bash<pre><code>mkdocs serve\n</code></pre> <p>Run the pre-commit hooks:</p> Bash<pre><code>pre-commit run --all-files\n</code></pre> <p>Build package:</p> Bash<pre><code>poetry build\n</code></pre>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#prerequisites","title":"Prerequisites","text":"<p>Before installing GFM-RAG, make sure your system meets these requirements:</p> <ul> <li>Python 3.12 or higher</li> <li>CUDA 12 or higher (for GPU support)</li> <li>Poetry (recommended for development)</li> </ul>"},{"location":"install/#installation-methods","title":"Installation Methods","text":""},{"location":"install/#install-via-conda","title":"Install via Conda","text":"<p>Conda provides an easy way to install the CUDA development toolkit which is required by GFM-RAG:</p> Bash<pre><code>conda create -n gfmrag python=3.12\nconda activate gfmrag\nconda install cuda-toolkit -c nvidia/label/cuda-12.4.1 # Replace with your desired CUDA version\npip install gfmrag\n</code></pre>"},{"location":"install/#install-via-pip","title":"Install via Pip","text":"Bash<pre><code>pip install gfmrag\n</code></pre>"},{"location":"install/#install-from-source","title":"Install from Source","text":"<p>For contributors or those who want to install from source, follow these steps:</p> <ol> <li> <p>Clone the repository: Bash<pre><code>git clone https://github.com/RManLuo/gfm-rag.git\ncd gfm-rag\n</code></pre></p> </li> <li> <p>Install Poetry:</p> </li> <li> <p>Create and activate a conda environment: Bash<pre><code>conda create -n gfmrag python=3.12\nconda activate gfmrag\nconda install cuda-toolkit -c nvidia/label/cuda-12.4.1 # Replace with your desired CUDA version\n</code></pre></p> </li> <li> <p>Install project dependencies: Bash<pre><code>poetry install\n</code></pre></p> </li> </ol>"},{"location":"install/#optional-components","title":"Optional Components","text":""},{"location":"install/#llamacpp-integration","title":"Llama.cpp Integration","text":"<p>If you plan to use locally host LLMs via Llama.cpp:</p> <p>Install llama-cpp-python: Bash<pre><code>pip install llama-cpp-python\n</code></pre></p> <p>For more information, visit the following resources: - LangChain Llama.cpp - llama-cpp-python repository</p>"},{"location":"install/#ollama-integration","title":"Ollama Integration","text":"<p>If you plan to use Ollama for hosting LLMs:</p> <p>Install Ollama: Bash<pre><code>pip install langchain-ollama\npip install ollama\n</code></pre></p> <p>For more information, visit the following resources: - LangChain Ollama</p>"},{"location":"install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"install/#cuda-errors-when-compiling-rspmm-kernel","title":"CUDA errors when compiling <code>rspmm</code> kernel","text":"<p>GFM-RAG requires the <code>nvcc</code> compiler to compile the <code>rspmm</code> kernel. If you encounter errors related to CUDA, make sure you have the CUDA toolkit installed and the <code>nvcc</code> compiler is in your PATH. Meanwhile, make sure your CUDA_HOME variable is set properly to avoid potential compilation errors, eg</p> Bash<pre><code>export CUDA_HOME=/usr/local/cuda-12.4\n</code></pre> <p>Usually, if you install CUDA toolkit via conda, the CUDA_HOME variable is set automatically.</p>"},{"location":"install/#stuck-when-compiling-rspmm-kernel","title":"Stuck when compiling <code>rspmm</code> kernel","text":"<p>Sometimes the compilation of the <code>rspmm</code> kernel may get stuck. If you encounter this issue, try to manually remove the compilation cache under <code>~/.cache/torch_extensions/</code> and recompile the kernel.</p> <p>For more help, please check our GitHub issues or create a new one.</p>"},{"location":"api/datasets/","title":"Datasets","text":""},{"location":"api/datasets/#gfmrag.datasets","title":"<code>gfmrag.datasets</code>","text":""},{"location":"api/datasets/#gfmrag.datasets.KGDataset","title":"<code>KGDataset</code>","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for processing and managing Knowledge Graph (KG) data.</p> <p>This class extends InMemoryDataset to handle knowledge graph data, including entity-relation-entity triplets, and supports processing of both direct and inverse relations.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset should be saved.</p> required <code>data_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>text_emb_model_cfgs</code> <code>DictConfig</code> <p>Configuration for the text embedding model.</p> required <code>force_rebuild</code> <code>bool</code> <p>Whether to force rebuilding the processed data. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>str</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the dataset.</p> <code>fingerprint</code> <code>str</code> <p>MD5 hash of the text embedding model configuration.</p> <code>delimiter</code> <code>str</code> <p>Delimiter used in the KG text file.</p> <code>data</code> <code>Data</code> <p>Processed graph data object.</p> <code>slices</code> <code>dict</code> <p>Data slices for batching.</p> Note <ul> <li>The class expects a 'kg.txt' file in the raw directory containing triplets.</li> <li>Processes both direct and inverse relations.</li> <li>Generates and stores relation embeddings using the specified text embedding model.</li> <li>Saves processed data along with entity and relation mappings.</li> </ul> Source code in <code>gfmrag/datasets/kg_dataset.py</code> Python<pre><code>class KGDataset(InMemoryDataset):\n    \"\"\"A dataset class for processing and managing Knowledge Graph (KG) data.\n\n    This class extends InMemoryDataset to handle knowledge graph data, including entity-relation-entity triplets,\n    and supports processing of both direct and inverse relations.\n\n    Args:\n        root (str): Root directory where the dataset should be saved.\n        data_name (str): Name of the dataset.\n        text_emb_model_cfgs (DictConfig): Configuration for the text embedding model.\n        force_rebuild (bool, optional): Whether to force rebuilding the processed data. Defaults to False.\n        **kwargs (str): Additional keyword arguments.\n\n    Attributes:\n        name (str): Name of the dataset.\n        fingerprint (str): MD5 hash of the text embedding model configuration.\n        delimiter (str): Delimiter used in the KG text file.\n        data (Data): Processed graph data object.\n        slices (dict): Data slices for batching.\n\n    Note:\n        - The class expects a 'kg.txt' file in the raw directory containing triplets.\n        - Processes both direct and inverse relations.\n        - Generates and stores relation embeddings using the specified text embedding model.\n        - Saves processed data along with entity and relation mappings.\n    \"\"\"\n\n    delimiter = KG_DELIMITER\n\n    def __init__(\n        self,\n        root: str,\n        data_name: str,\n        text_emb_model_cfgs: DictConfig,\n        force_rebuild: bool = False,\n        **kwargs: str,\n    ) -&gt; None:\n        self.name = data_name\n        self.force_rebuild = force_rebuild\n        # Get fingerprint of the model configuration\n        self.fingerprint = hashlib.md5(\n            json.dumps(\n                OmegaConf.to_container(text_emb_model_cfgs, resolve=True)\n            ).encode()\n        ).hexdigest()\n        self.text_emb_model_cfgs = text_emb_model_cfgs\n        super().__init__(root, None, None)\n        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n\n    @property\n    def raw_file_names(self) -&gt; list:\n        return [\"kg.txt\"]\n\n    def load_file(\n        self, triplet_file: str, inv_entity_vocab: dict, inv_rel_vocab: dict\n    ) -&gt; dict:\n        \"\"\"Load a knowledge graph file and return the processed data.\"\"\"\n\n        triplets = []  # Triples with inverse relations\n        entity_cnt, rel_cnt = len(inv_entity_vocab), len(inv_rel_vocab)\n\n        with open(triplet_file, encoding=\"utf-8\") as fin:\n            for line in fin:\n                try:\n                    u, r, v = (\n                        line.split()\n                        if self.delimiter is None\n                        else line.strip().split(self.delimiter)\n                    )\n                except Exception as e:\n                    logger.error(f\"Error in line: {line}, {e}, Skipping\")\n                    continue\n                if u not in inv_entity_vocab:\n                    inv_entity_vocab[u] = entity_cnt\n                    entity_cnt += 1\n                if v not in inv_entity_vocab:\n                    inv_entity_vocab[v] = entity_cnt\n                    entity_cnt += 1\n                if r not in inv_rel_vocab:\n                    inv_rel_vocab[r] = rel_cnt\n                    rel_cnt += 1\n                u, r, v = inv_entity_vocab[u], inv_rel_vocab[r], inv_entity_vocab[v]\n\n                triplets.append((u, v, r))\n\n        return {\n            \"triplets\": triplets,\n            \"num_node\": len(inv_entity_vocab),  # entity_cnt,\n            \"num_relation\": rel_cnt,\n            \"inv_entity_vocab\": inv_entity_vocab,\n            \"inv_rel_vocab\": inv_rel_vocab,\n        }\n\n    def _process(self) -&gt; None:\n        if is_main_process():\n            logger.info(f\"Processing KG dataset {self.name} at rank {get_rank()}\")\n            f = osp.join(self.processed_dir, \"pre_transform.pt\")\n            if osp.exists(f) and torch.load(f, weights_only=False) != _repr(\n                self.pre_transform\n            ):\n                warnings.warn(  # noqa:B028\n                    f\"The `pre_transform` argument differs from the one used in \"\n                    f\"the pre-processed version of this dataset. If you want to \"\n                    f\"make use of another pre-processing technique, make sure to \"\n                    f\"delete '{self.processed_dir}' first\",\n                    stacklevel=1,\n                )\n\n            f = osp.join(self.processed_dir, \"pre_filter.pt\")\n            if osp.exists(f) and torch.load(f, weights_only=False) != _repr(\n                self.pre_filter\n            ):\n                warnings.warn(\n                    f\"The `pre_filter` argument differs from the one used in \"\n                    f\"the pre-processed version of this dataset. If you want to \"\n                    f\"make use of another pre-fitering technique, make sure to \"\n                    f\"delete '{self.processed_dir}' first\",\n                    stacklevel=1,\n                )\n\n            if self.force_rebuild or not files_exist(self.processed_paths):\n                if self.log and \"pytest\" not in sys.modules:\n                    print(\"Processing...\", file=sys.stderr)\n\n                makedirs(self.processed_dir)\n                self.process()\n\n                path = osp.join(self.processed_dir, \"pre_transform.pt\")\n                torch.save(_repr(self.pre_transform), path)\n                path = osp.join(self.processed_dir, \"pre_filter.pt\")\n                torch.save(_repr(self.pre_filter), path)\n\n                if self.log and \"pytest\" not in sys.modules:\n                    print(\"Done!\", file=sys.stderr)\n        else:\n            logger.info(\n                f\"Rank [{get_rank()}]: Waiting for main process to finish processing KG dataset {self.name}\"\n            )\n        synchronize()\n\n    def process(self) -&gt; None:\n        \"\"\"Process the knowledge graph dataset.\n\n        This method processes the raw knowledge graph file and creates the following:\n\n        1. Loads the KG triplets and vocabulary\n        2. Creates edge indices and types for both original and inverse relations\n        3. Saves entity and relation mappings to JSON files\n        4. Generates relation embeddings using a text embedding model\n        5. Builds relation graphs\n        6. Saves the processed data and model configurations\n\n        The processed data includes:\n\n        - Edge indices and types for both original and inverse edges\n        - Target edge indices and types (original edges only)\n        - Number of nodes and relations\n        - Relation embeddings\n        - Relation graphs\n\n        Files created:\n\n        - ent2id.json: Entity to ID mapping\n        - rel2id.json: Relation to ID mapping (including inverse relations)\n        - text_emb_model_cfgs.json: Text embedding model configuration\n        - Processed graph data file at self.processed_paths[0]\n        \"\"\"\n        kg_file = self.raw_paths[0]\n\n        kg_result = self.load_file(kg_file, inv_entity_vocab={}, inv_rel_vocab={})\n\n        # in some datasets, there are several new nodes in the test set, eg 123,143 YAGO train and 123,182 in YAGO test\n        # for consistency with other experimental results, we'll include those in the full vocab and num nodes\n        num_node = kg_result[\"num_node\"]\n        # the same for rels: in most cases train == test for transductive\n        # for AristoV4 train rels 1593, test 1604\n        num_relations = kg_result[\"num_relation\"]\n\n        kg_triplets = kg_result[\"triplets\"]\n\n        train_target_edges = torch.tensor(\n            [[t[0], t[1]] for t in kg_triplets], dtype=torch.long\n        ).t()\n        train_target_etypes = torch.tensor([t[2] for t in kg_triplets])\n\n        # Add inverse edges\n        train_edges = torch.cat([train_target_edges, train_target_edges.flip(0)], dim=1)\n        train_etypes = torch.cat(\n            [train_target_etypes, train_target_etypes + num_relations]\n        )\n\n        with open(self.processed_dir + \"/ent2id.json\", \"w\") as f:\n            json.dump(kg_result[\"inv_entity_vocab\"], f)\n        rel2id = kg_result[\"inv_rel_vocab\"]\n        id2rel = {v: k for k, v in rel2id.items()}\n        for etype in train_etypes:\n            if etype.item() &gt;= num_relations:\n                raw_etype = etype - num_relations\n                raw_rel = id2rel[raw_etype.item()]\n                rel2id[\"inverse_\" + raw_rel] = etype.item()\n        with open(self.processed_dir + \"/rel2id.json\", \"w\") as f:\n            json.dump(rel2id, f)\n\n        # Generate relation embeddings\n        logger.info(\"Generating relation embeddings\")\n        text_emb_model: BaseTextEmbModel = instantiate(self.text_emb_model_cfgs)\n        rel_emb = text_emb_model.encode(list(rel2id.keys()), is_query=False).cpu()\n\n        kg_data = Data(\n            edge_index=train_edges,\n            edge_type=train_etypes,\n            num_nodes=num_node,\n            target_edge_index=train_target_edges,\n            target_edge_type=train_target_etypes,\n            num_relations=num_relations * 2,\n            rel_emb=rel_emb,\n        )\n\n        # build graphs of relations\n        kg_data = build_relation_graph(kg_data)\n\n        torch.save((self.collate([kg_data])), self.processed_paths[0])\n\n        # Save text embeddings model configuration\n        with open(self.processed_dir + \"/text_emb_model_cfgs.json\", \"w\") as f:\n            json.dump(OmegaConf.to_container(self.text_emb_model_cfgs), f, indent=4)\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.name}()\"\n\n    @property\n    def num_relations(self) -&gt; int:\n        return int(self.data.edge_type.max()) + 1\n\n    @property\n    def raw_dir(self) -&gt; str:\n        return os.path.join(str(self.root), str(self.name), \"processed\", \"stage1\")\n\n    @property\n    def processed_dir(self) -&gt; str:\n        return os.path.join(\n            str(self.root),\n            str(self.name),\n            \"processed\",\n            \"stage2\",\n            self.fingerprint,\n        )\n\n    @property\n    def processed_file_names(self) -&gt; str:\n        return \"data.pt\"\n</code></pre>"},{"location":"api/datasets/#gfmrag.datasets.KGDataset.load_file","title":"<code>load_file(triplet_file, inv_entity_vocab, inv_rel_vocab)</code>","text":"<p>Load a knowledge graph file and return the processed data.</p> Source code in <code>gfmrag/datasets/kg_dataset.py</code> Python<pre><code>def load_file(\n    self, triplet_file: str, inv_entity_vocab: dict, inv_rel_vocab: dict\n) -&gt; dict:\n    \"\"\"Load a knowledge graph file and return the processed data.\"\"\"\n\n    triplets = []  # Triples with inverse relations\n    entity_cnt, rel_cnt = len(inv_entity_vocab), len(inv_rel_vocab)\n\n    with open(triplet_file, encoding=\"utf-8\") as fin:\n        for line in fin:\n            try:\n                u, r, v = (\n                    line.split()\n                    if self.delimiter is None\n                    else line.strip().split(self.delimiter)\n                )\n            except Exception as e:\n                logger.error(f\"Error in line: {line}, {e}, Skipping\")\n                continue\n            if u not in inv_entity_vocab:\n                inv_entity_vocab[u] = entity_cnt\n                entity_cnt += 1\n            if v not in inv_entity_vocab:\n                inv_entity_vocab[v] = entity_cnt\n                entity_cnt += 1\n            if r not in inv_rel_vocab:\n                inv_rel_vocab[r] = rel_cnt\n                rel_cnt += 1\n            u, r, v = inv_entity_vocab[u], inv_rel_vocab[r], inv_entity_vocab[v]\n\n            triplets.append((u, v, r))\n\n    return {\n        \"triplets\": triplets,\n        \"num_node\": len(inv_entity_vocab),  # entity_cnt,\n        \"num_relation\": rel_cnt,\n        \"inv_entity_vocab\": inv_entity_vocab,\n        \"inv_rel_vocab\": inv_rel_vocab,\n    }\n</code></pre>"},{"location":"api/datasets/#gfmrag.datasets.KGDataset.process","title":"<code>process()</code>","text":"<p>Process the knowledge graph dataset.</p> <p>This method processes the raw knowledge graph file and creates the following:</p> <ol> <li>Loads the KG triplets and vocabulary</li> <li>Creates edge indices and types for both original and inverse relations</li> <li>Saves entity and relation mappings to JSON files</li> <li>Generates relation embeddings using a text embedding model</li> <li>Builds relation graphs</li> <li>Saves the processed data and model configurations</li> </ol> <p>The processed data includes:</p> <ul> <li>Edge indices and types for both original and inverse edges</li> <li>Target edge indices and types (original edges only)</li> <li>Number of nodes and relations</li> <li>Relation embeddings</li> <li>Relation graphs</li> </ul> <p>Files created:</p> <ul> <li>ent2id.json: Entity to ID mapping</li> <li>rel2id.json: Relation to ID mapping (including inverse relations)</li> <li>text_emb_model_cfgs.json: Text embedding model configuration</li> <li>Processed graph data file at self.processed_paths[0]</li> </ul> Source code in <code>gfmrag/datasets/kg_dataset.py</code> Python<pre><code>def process(self) -&gt; None:\n    \"\"\"Process the knowledge graph dataset.\n\n    This method processes the raw knowledge graph file and creates the following:\n\n    1. Loads the KG triplets and vocabulary\n    2. Creates edge indices and types for both original and inverse relations\n    3. Saves entity and relation mappings to JSON files\n    4. Generates relation embeddings using a text embedding model\n    5. Builds relation graphs\n    6. Saves the processed data and model configurations\n\n    The processed data includes:\n\n    - Edge indices and types for both original and inverse edges\n    - Target edge indices and types (original edges only)\n    - Number of nodes and relations\n    - Relation embeddings\n    - Relation graphs\n\n    Files created:\n\n    - ent2id.json: Entity to ID mapping\n    - rel2id.json: Relation to ID mapping (including inverse relations)\n    - text_emb_model_cfgs.json: Text embedding model configuration\n    - Processed graph data file at self.processed_paths[0]\n    \"\"\"\n    kg_file = self.raw_paths[0]\n\n    kg_result = self.load_file(kg_file, inv_entity_vocab={}, inv_rel_vocab={})\n\n    # in some datasets, there are several new nodes in the test set, eg 123,143 YAGO train and 123,182 in YAGO test\n    # for consistency with other experimental results, we'll include those in the full vocab and num nodes\n    num_node = kg_result[\"num_node\"]\n    # the same for rels: in most cases train == test for transductive\n    # for AristoV4 train rels 1593, test 1604\n    num_relations = kg_result[\"num_relation\"]\n\n    kg_triplets = kg_result[\"triplets\"]\n\n    train_target_edges = torch.tensor(\n        [[t[0], t[1]] for t in kg_triplets], dtype=torch.long\n    ).t()\n    train_target_etypes = torch.tensor([t[2] for t in kg_triplets])\n\n    # Add inverse edges\n    train_edges = torch.cat([train_target_edges, train_target_edges.flip(0)], dim=1)\n    train_etypes = torch.cat(\n        [train_target_etypes, train_target_etypes + num_relations]\n    )\n\n    with open(self.processed_dir + \"/ent2id.json\", \"w\") as f:\n        json.dump(kg_result[\"inv_entity_vocab\"], f)\n    rel2id = kg_result[\"inv_rel_vocab\"]\n    id2rel = {v: k for k, v in rel2id.items()}\n    for etype in train_etypes:\n        if etype.item() &gt;= num_relations:\n            raw_etype = etype - num_relations\n            raw_rel = id2rel[raw_etype.item()]\n            rel2id[\"inverse_\" + raw_rel] = etype.item()\n    with open(self.processed_dir + \"/rel2id.json\", \"w\") as f:\n        json.dump(rel2id, f)\n\n    # Generate relation embeddings\n    logger.info(\"Generating relation embeddings\")\n    text_emb_model: BaseTextEmbModel = instantiate(self.text_emb_model_cfgs)\n    rel_emb = text_emb_model.encode(list(rel2id.keys()), is_query=False).cpu()\n\n    kg_data = Data(\n        edge_index=train_edges,\n        edge_type=train_etypes,\n        num_nodes=num_node,\n        target_edge_index=train_target_edges,\n        target_edge_type=train_target_etypes,\n        num_relations=num_relations * 2,\n        rel_emb=rel_emb,\n    )\n\n    # build graphs of relations\n    kg_data = build_relation_graph(kg_data)\n\n    torch.save((self.collate([kg_data])), self.processed_paths[0])\n\n    # Save text embeddings model configuration\n    with open(self.processed_dir + \"/text_emb_model_cfgs.json\", \"w\") as f:\n        json.dump(OmegaConf.to_container(self.text_emb_model_cfgs), f, indent=4)\n</code></pre>"},{"location":"api/datasets/#gfmrag.datasets.QADataset","title":"<code>QADataset</code>","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>A dataset class for Question-Answering tasks built on top of a Knowledge Graph.</p> <p>This dataset inherits from torch_geometric's InMemoryDataset and processes raw QA data into a format suitable for graph-based QA models. It handles both training and test splits.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset should be saved.</p> required <code>data_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>text_emb_model_cfgs</code> <code>DictConfig</code> <p>Configuration for the text embedding model used to encode questions.</p> required <code>force_rebuild</code> <code>bool</code> <p>If True, forces the dataset to be reprocessed even if it exists. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the dataset.</p> <code>kg</code> <code>KGDataset</code> <p>The underlying knowledge graph dataset.</p> <code>rel_emb_dim</code> <code>int</code> <p>Dimension of relation embeddings.</p> <code>ent2id</code> <code>dict</code> <p>Mapping from entity names to IDs.</p> <code>rel2id</code> <code>dict</code> <p>Mapping from relation names to IDs.</p> <code>doc</code> <code>dict</code> <p>Corpus of documents.</p> <code>doc2entities</code> <code>dict</code> <p>Mapping from documents to contained entities.</p> <code>raw_train_data</code> <code>list</code> <p>Raw training data samples.</p> <code>raw_test_data</code> <code>list</code> <p>Raw test data samples.</p> <code>ent2docs</code> <code>Tensor</code> <p>Sparse tensor mapping entities to documents.</p> <code>id2doc</code> <code>dict</code> <p>Mapping from document IDs to document names.</p> Notes <p>The processed dataset contains: - Question embeddings - Question entity masks - Supporting entity masks - Supporting document masks - Sample IDs</p> <p>The dataset processes raw JSON files and creates PyTorch tensors for efficient training.</p> Source code in <code>gfmrag/datasets/qa_dataset.py</code> Python<pre><code>class QADataset(InMemoryDataset):\n    \"\"\"A dataset class for Question-Answering tasks built on top of a Knowledge Graph.\n\n    This dataset inherits from torch_geometric's InMemoryDataset and processes raw QA data\n    into a format suitable for graph-based QA models. It handles both training and test splits.\n\n    Args:\n        root (str): Root directory where the dataset should be saved.\n        data_name (str): Name of the dataset.\n        text_emb_model_cfgs (DictConfig): Configuration for the text embedding model used to encode questions.\n        force_rebuild (bool, optional): If True, forces the dataset to be reprocessed even if it exists. Defaults to False.\n\n    Attributes:\n        name (str): Name of the dataset.\n        kg (KGDataset): The underlying knowledge graph dataset.\n        rel_emb_dim (int): Dimension of relation embeddings.\n        ent2id (dict): Mapping from entity names to IDs.\n        rel2id (dict): Mapping from relation names to IDs.\n        doc (dict): Corpus of documents.\n        doc2entities (dict): Mapping from documents to contained entities.\n        raw_train_data (list): Raw training data samples.\n        raw_test_data (list): Raw test data samples.\n        ent2docs (torch.Tensor): Sparse tensor mapping entities to documents.\n        id2doc (dict): Mapping from document IDs to document names.\n\n    Notes:\n        The processed dataset contains:\n        - Question embeddings\n        - Question entity masks\n        - Supporting entity masks\n        - Supporting document masks\n        - Sample IDs\n\n        The dataset processes raw JSON files and creates PyTorch tensors for efficient training.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        data_name: str,\n        text_emb_model_cfgs: DictConfig,\n        force_rebuild: bool = False,\n    ):\n        self.name = data_name\n        self.force_rebuild = force_rebuild\n        self.text_emb_model_cfgs = text_emb_model_cfgs\n        # Get fingerprint of the model configuration\n        self.fingerprint = hashlib.md5(\n            json.dumps(\n                OmegaConf.to_container(text_emb_model_cfgs, resolve=True)\n            ).encode()\n        ).hexdigest()\n        self.kg = KGDataset(root, data_name, text_emb_model_cfgs, force_rebuild)[0]\n        self.rel_emb_dim = self.kg.rel_emb.shape[-1]\n        super().__init__(root, None, None)\n        self.data = torch.load(self.processed_paths[0], weights_only=False)\n        self.load_property()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.name}()\"\n\n    @property\n    def raw_file_names(self) -&gt; list:\n        return [\"train.json\", \"test.json\"]\n\n    @property\n    def raw_dir(self) -&gt; str:\n        return os.path.join(str(self.root), str(self.name), \"processed\", \"stage1\")\n\n    @property\n    def processed_dir(self) -&gt; str:\n        return os.path.join(\n            str(self.root),\n            str(self.name),\n            \"processed\",\n            \"stage2\",\n            self.fingerprint,\n        )\n\n    @property\n    def processed_file_names(self) -&gt; str:\n        return \"qa_data.pt\"\n\n    def load_property(self) -&gt; None:\n        \"\"\"\n        Load necessary properties from the KG dataset.\n        \"\"\"\n        with open(os.path.join(self.processed_dir, \"ent2id.json\")) as fin:\n            self.ent2id = json.load(fin)\n        with open(os.path.join(self.processed_dir, \"rel2id.json\")) as fin:\n            self.rel2id = json.load(fin)\n        with open(\n            os.path.join(str(self.root), str(self.name), \"raw\", \"dataset_corpus.json\")\n        ) as fin:\n            self.doc = json.load(fin)\n        with open(os.path.join(self.raw_dir, \"document2entities.json\")) as fin:\n            self.doc2entities = json.load(fin)\n        if os.path.exists(os.path.join(self.raw_dir, \"train.json\")):\n            with open(os.path.join(self.raw_dir, \"train.json\")) as fin:\n                self.raw_train_data = json.load(fin)\n        else:\n            self.raw_train_data = []\n        if os.path.exists(os.path.join(self.raw_dir, \"test.json\")):\n            with open(os.path.join(self.raw_dir, \"test.json\")) as fin:\n                self.raw_test_data = json.load(fin)\n        else:\n            self.raw_test_data = []\n\n        self.ent2docs = torch.load(\n            os.path.join(self.processed_dir, \"ent2doc.pt\"), weights_only=False\n        )  # (n_nodes, n_docs)\n        self.id2doc = {i: doc for i, doc in enumerate(self.doc2entities)}\n\n    def _process(self) -&gt; None:\n        if is_main_process():\n            logger.info(f\"Processing QA dataset {self.name} at rank {get_rank()}\")\n            f = osp.join(self.processed_dir, \"pre_transform.pt\")\n            if osp.exists(f) and torch.load(f, weights_only=False) != _repr(\n                self.pre_transform\n            ):\n                warnings.warn(\n                    f\"The `pre_transform` argument differs from the one used in \"\n                    f\"the pre-processed version of this dataset. If you want to \"\n                    f\"make use of another pre-processing technique, make sure to \"\n                    f\"delete '{self.processed_dir}' first\",\n                    stacklevel=1,\n                )\n\n            f = osp.join(self.processed_dir, \"pre_filter.pt\")\n            if osp.exists(f) and torch.load(f, weights_only=False) != _repr(\n                self.pre_filter\n            ):\n                warnings.warn(\n                    f\"The `pre_filter` argument differs from the one used in \"\n                    f\"the pre-processed version of this dataset. If you want to \"\n                    f\"make use of another pre-fitering technique, make sure to \"\n                    f\"delete '{self.processed_dir}' first\",\n                    stacklevel=1,\n                )\n\n            if self.force_rebuild or not files_exist(self.processed_paths):\n                if self.log and \"pytest\" not in sys.modules:\n                    print(\"Processing...\", file=sys.stderr)\n\n                makedirs(self.processed_dir)\n                self.process()\n\n                path = osp.join(self.processed_dir, \"pre_transform.pt\")\n                torch.save(_repr(self.pre_transform), path)\n                path = osp.join(self.processed_dir, \"pre_filter.pt\")\n                torch.save(_repr(self.pre_filter), path)\n\n                if self.log and \"pytest\" not in sys.modules:\n                    print(\"Done!\", file=sys.stderr)\n        else:\n            logger.info(\n                f\"Rank [{get_rank()}]: Waiting for main process to finish processing QA dataset {self.name}\"\n            )\n        synchronize()\n\n    def process(self) -&gt; None:\n        \"\"\"Process and prepare the question-answering dataset.\n\n        This method processes raw data files to create a structured dataset for question answering\n        tasks. It performs the following main operations:\n\n        1. Loads entity and relation mappings from processed files\n        2. Creates entity-document mapping tensors\n        3. Processes question samples to generate:\n            - Question embeddings\n            - Question entity masks\n            - Supporting entity masks\n            - Supporting document masks\n\n        The processed dataset is saved as torch splits containing:\n\n        - Question embeddings\n        - Various mask tensors for entities and documents\n        - Sample IDs\n\n        Files created:\n\n        - ent2doc.pt: Sparse tensor mapping entities to documents\n        - qa_data.pt: Processed QA dataset\n        - text_emb_model_cfgs.json: Text embedding model configuration\n\n        The method also saves the text embedding model configuration.\n\n        Returns:\n            None\n        \"\"\"\n        with open(os.path.join(self.processed_dir, \"ent2id.json\")) as fin:\n            self.ent2id = json.load(fin)\n        with open(os.path.join(self.processed_dir, \"rel2id.json\")) as fin:\n            self.rel2id = json.load(fin)\n        with open(os.path.join(self.raw_dir, \"document2entities.json\")) as fin:\n            self.doc2entities = json.load(fin)\n\n        num_nodes = self.kg.num_nodes\n        doc2id = {doc: i for i, doc in enumerate(self.doc2entities)}\n        # Convert document to entities to entity to document\n        n_docs = len(self.doc2entities)\n        # Create a sparse tensor for entity to document\n        doc2ent = torch.zeros((n_docs, num_nodes))\n        for doc, entities in self.doc2entities.items():\n            entity_ids = [self.ent2id[ent] for ent in entities if ent in self.ent2id]\n            doc2ent[doc2id[doc], entity_ids] = 1\n        ent2doc = doc2ent.T.to_sparse()  # (n_nodes, n_docs)\n        torch.save(ent2doc, os.path.join(self.processed_dir, \"ent2doc.pt\"))\n\n        sample_id = []\n        questions = []\n        question_entities_masks = []  # Convert question entities to mask with number of nodes\n        supporting_entities_masks = []\n        supporting_docs_masks = []\n        num_samples = []\n\n        for path in self.raw_paths:\n            if not os.path.exists(path):\n                num_samples.append(0)\n                continue  # Skip if the file does not exist\n            num_sample = 0\n            with open(path) as fin:\n                data = json.load(fin)\n                for index, item in enumerate(data):\n                    question_entities = [\n                        self.ent2id[x]\n                        for x in item[\"question_entities\"]\n                        if x in self.ent2id\n                    ]\n\n                    supporting_entities = [\n                        self.ent2id[x]\n                        for x in item[\"supporting_entities\"]\n                        if x in self.ent2id\n                    ]\n\n                    supporting_docs = [\n                        doc2id[doc] for doc in item[\"supporting_facts\"] if doc in doc2id\n                    ]\n\n                    # Skip samples if any of the entities or documens are empty\n                    if any(\n                        len(x) == 0\n                        for x in [\n                            question_entities,\n                            supporting_entities,\n                            supporting_docs,\n                        ]\n                    ):\n                        continue\n                    num_sample += 1\n                    sample_id.append(index)\n                    question = item[\"question\"]\n                    questions.append(question)\n\n                    question_entities_masks.append(\n                        entities_to_mask(question_entities, num_nodes)\n                    )\n\n                    supporting_entities_masks.append(\n                        entities_to_mask(supporting_entities, num_nodes)\n                    )\n\n                    supporting_docs_masks.append(\n                        entities_to_mask(supporting_docs, n_docs)\n                    )\n                num_samples.append(num_sample)\n\n        # Generate question embeddings\n        logger.info(\"Generating question embeddings\")\n        text_emb_model: BaseTextEmbModel = instantiate(self.text_emb_model_cfgs)\n        question_embeddings = text_emb_model.encode(\n            questions,\n            is_query=True,\n        ).cpu()\n        question_entities_masks = torch.stack(question_entities_masks)\n        supporting_entities_masks = torch.stack(supporting_entities_masks)\n        supporting_docs_masks = torch.stack(supporting_docs_masks)\n        sample_id = torch.tensor(sample_id, dtype=torch.long)\n\n        dataset = datasets.Dataset.from_dict(\n            {\n                \"question_embeddings\": question_embeddings,\n                \"question_entities_masks\": question_entities_masks,\n                \"supporting_entities_masks\": supporting_entities_masks,\n                \"supporting_docs_masks\": supporting_docs_masks,\n                \"sample_id\": sample_id,\n            }\n        ).with_format(\"torch\")\n        offset = 0\n        splits = []\n        for num_sample in num_samples:\n            split = torch_data.Subset(dataset, range(offset, offset + num_sample))\n            splits.append(split)\n            offset += num_sample\n        torch.save(splits, self.processed_paths[0])\n\n        # Save text embeddings model configuration\n        with open(self.processed_dir + \"/text_emb_model_cfgs.json\", \"w\") as f:\n            json.dump(OmegaConf.to_container(self.text_emb_model_cfgs), f, indent=4)\n</code></pre>"},{"location":"api/datasets/#gfmrag.datasets.QADataset.load_property","title":"<code>load_property()</code>","text":"<p>Load necessary properties from the KG dataset.</p> Source code in <code>gfmrag/datasets/qa_dataset.py</code> Python<pre><code>def load_property(self) -&gt; None:\n    \"\"\"\n    Load necessary properties from the KG dataset.\n    \"\"\"\n    with open(os.path.join(self.processed_dir, \"ent2id.json\")) as fin:\n        self.ent2id = json.load(fin)\n    with open(os.path.join(self.processed_dir, \"rel2id.json\")) as fin:\n        self.rel2id = json.load(fin)\n    with open(\n        os.path.join(str(self.root), str(self.name), \"raw\", \"dataset_corpus.json\")\n    ) as fin:\n        self.doc = json.load(fin)\n    with open(os.path.join(self.raw_dir, \"document2entities.json\")) as fin:\n        self.doc2entities = json.load(fin)\n    if os.path.exists(os.path.join(self.raw_dir, \"train.json\")):\n        with open(os.path.join(self.raw_dir, \"train.json\")) as fin:\n            self.raw_train_data = json.load(fin)\n    else:\n        self.raw_train_data = []\n    if os.path.exists(os.path.join(self.raw_dir, \"test.json\")):\n        with open(os.path.join(self.raw_dir, \"test.json\")) as fin:\n            self.raw_test_data = json.load(fin)\n    else:\n        self.raw_test_data = []\n\n    self.ent2docs = torch.load(\n        os.path.join(self.processed_dir, \"ent2doc.pt\"), weights_only=False\n    )  # (n_nodes, n_docs)\n    self.id2doc = {i: doc for i, doc in enumerate(self.doc2entities)}\n</code></pre>"},{"location":"api/datasets/#gfmrag.datasets.QADataset.process","title":"<code>process()</code>","text":"<p>Process and prepare the question-answering dataset.</p> <p>This method processes raw data files to create a structured dataset for question answering tasks. It performs the following main operations:</p> <ol> <li>Loads entity and relation mappings from processed files</li> <li>Creates entity-document mapping tensors</li> <li>Processes question samples to generate:<ul> <li>Question embeddings</li> <li>Question entity masks</li> <li>Supporting entity masks</li> <li>Supporting document masks</li> </ul> </li> </ol> <p>The processed dataset is saved as torch splits containing:</p> <ul> <li>Question embeddings</li> <li>Various mask tensors for entities and documents</li> <li>Sample IDs</li> </ul> <p>Files created:</p> <ul> <li>ent2doc.pt: Sparse tensor mapping entities to documents</li> <li>qa_data.pt: Processed QA dataset</li> <li>text_emb_model_cfgs.json: Text embedding model configuration</li> </ul> <p>The method also saves the text embedding model configuration.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/datasets/qa_dataset.py</code> Python<pre><code>def process(self) -&gt; None:\n    \"\"\"Process and prepare the question-answering dataset.\n\n    This method processes raw data files to create a structured dataset for question answering\n    tasks. It performs the following main operations:\n\n    1. Loads entity and relation mappings from processed files\n    2. Creates entity-document mapping tensors\n    3. Processes question samples to generate:\n        - Question embeddings\n        - Question entity masks\n        - Supporting entity masks\n        - Supporting document masks\n\n    The processed dataset is saved as torch splits containing:\n\n    - Question embeddings\n    - Various mask tensors for entities and documents\n    - Sample IDs\n\n    Files created:\n\n    - ent2doc.pt: Sparse tensor mapping entities to documents\n    - qa_data.pt: Processed QA dataset\n    - text_emb_model_cfgs.json: Text embedding model configuration\n\n    The method also saves the text embedding model configuration.\n\n    Returns:\n        None\n    \"\"\"\n    with open(os.path.join(self.processed_dir, \"ent2id.json\")) as fin:\n        self.ent2id = json.load(fin)\n    with open(os.path.join(self.processed_dir, \"rel2id.json\")) as fin:\n        self.rel2id = json.load(fin)\n    with open(os.path.join(self.raw_dir, \"document2entities.json\")) as fin:\n        self.doc2entities = json.load(fin)\n\n    num_nodes = self.kg.num_nodes\n    doc2id = {doc: i for i, doc in enumerate(self.doc2entities)}\n    # Convert document to entities to entity to document\n    n_docs = len(self.doc2entities)\n    # Create a sparse tensor for entity to document\n    doc2ent = torch.zeros((n_docs, num_nodes))\n    for doc, entities in self.doc2entities.items():\n        entity_ids = [self.ent2id[ent] for ent in entities if ent in self.ent2id]\n        doc2ent[doc2id[doc], entity_ids] = 1\n    ent2doc = doc2ent.T.to_sparse()  # (n_nodes, n_docs)\n    torch.save(ent2doc, os.path.join(self.processed_dir, \"ent2doc.pt\"))\n\n    sample_id = []\n    questions = []\n    question_entities_masks = []  # Convert question entities to mask with number of nodes\n    supporting_entities_masks = []\n    supporting_docs_masks = []\n    num_samples = []\n\n    for path in self.raw_paths:\n        if not os.path.exists(path):\n            num_samples.append(0)\n            continue  # Skip if the file does not exist\n        num_sample = 0\n        with open(path) as fin:\n            data = json.load(fin)\n            for index, item in enumerate(data):\n                question_entities = [\n                    self.ent2id[x]\n                    for x in item[\"question_entities\"]\n                    if x in self.ent2id\n                ]\n\n                supporting_entities = [\n                    self.ent2id[x]\n                    for x in item[\"supporting_entities\"]\n                    if x in self.ent2id\n                ]\n\n                supporting_docs = [\n                    doc2id[doc] for doc in item[\"supporting_facts\"] if doc in doc2id\n                ]\n\n                # Skip samples if any of the entities or documens are empty\n                if any(\n                    len(x) == 0\n                    for x in [\n                        question_entities,\n                        supporting_entities,\n                        supporting_docs,\n                    ]\n                ):\n                    continue\n                num_sample += 1\n                sample_id.append(index)\n                question = item[\"question\"]\n                questions.append(question)\n\n                question_entities_masks.append(\n                    entities_to_mask(question_entities, num_nodes)\n                )\n\n                supporting_entities_masks.append(\n                    entities_to_mask(supporting_entities, num_nodes)\n                )\n\n                supporting_docs_masks.append(\n                    entities_to_mask(supporting_docs, n_docs)\n                )\n            num_samples.append(num_sample)\n\n    # Generate question embeddings\n    logger.info(\"Generating question embeddings\")\n    text_emb_model: BaseTextEmbModel = instantiate(self.text_emb_model_cfgs)\n    question_embeddings = text_emb_model.encode(\n        questions,\n        is_query=True,\n    ).cpu()\n    question_entities_masks = torch.stack(question_entities_masks)\n    supporting_entities_masks = torch.stack(supporting_entities_masks)\n    supporting_docs_masks = torch.stack(supporting_docs_masks)\n    sample_id = torch.tensor(sample_id, dtype=torch.long)\n\n    dataset = datasets.Dataset.from_dict(\n        {\n            \"question_embeddings\": question_embeddings,\n            \"question_entities_masks\": question_entities_masks,\n            \"supporting_entities_masks\": supporting_entities_masks,\n            \"supporting_docs_masks\": supporting_docs_masks,\n            \"sample_id\": sample_id,\n        }\n    ).with_format(\"torch\")\n    offset = 0\n    splits = []\n    for num_sample in num_samples:\n        split = torch_data.Subset(dataset, range(offset, offset + num_sample))\n        splits.append(split)\n        offset += num_sample\n    torch.save(splits, self.processed_paths[0])\n\n    # Save text embeddings model configuration\n    with open(self.processed_dir + \"/text_emb_model_cfgs.json\", \"w\") as f:\n        json.dump(OmegaConf.to_container(self.text_emb_model_cfgs), f, indent=4)\n</code></pre>"},{"location":"api/doc_ranker/","title":"Document Ranker","text":""},{"location":"api/doc_ranker/#gfmrag.doc_rankers","title":"<code>gfmrag.doc_rankers</code>","text":""},{"location":"api/doc_ranker/#gfmrag.doc_rankers.BaseDocRanker","title":"<code>BaseDocRanker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for document ranker</p> <p>Parameters:</p> Name Type Description Default <code>ent2doc</code> <code>Tensor</code> <p>Mapping from entity to document</p> required Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>class BaseDocRanker(ABC):\n    \"\"\"\n    Abstract class for document ranker\n\n    Args:\n        ent2doc (torch.Tensor): Mapping from entity to document\n    \"\"\"\n\n    def __init__(self, ent2doc: torch.Tensor) -&gt; None:\n        self.ent2doc = ent2doc\n\n    @abstractmethod\n    def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n        pass\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.IDFWeightedRanker","title":"<code>IDFWeightedRanker</code>","text":"<p>               Bases: <code>BaseDocRanker</code></p> <p>Rank documents based on entity prediction with IDF weighting</p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>class IDFWeightedRanker(BaseDocRanker):\n    \"\"\"\n    Rank documents based on entity prediction with IDF weighting\n    \"\"\"\n\n    def __init__(self, ent2doc: torch.Tensor) -&gt; None:\n        super().__init__(ent2doc)\n        frequency = torch.sparse.sum(ent2doc, dim=-1).to_dense()\n        self.idf_weight = 1 / frequency\n        self.idf_weight[frequency == 0] = 0\n\n    def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Rank documents based on entity prediction with IDF weighting\n\n        Args:\n            ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n        Returns:\n            torch.Tensor: Document ranks, shape (batch_size, n_docs)\n        \"\"\"\n        doc_pred = torch.sparse.mm(\n            ent_pred * self.idf_weight.unsqueeze(0), self.ent2doc\n        )\n        return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.IDFWeightedRanker.__call__","title":"<code>__call__(ent_pred)</code>","text":"<p>Rank documents based on entity prediction with IDF weighting</p> <p>Parameters:</p> Name Type Description Default <code>ent_pred</code> <code>Tensor</code> <p>Entity prediction, shape (batch_size, n_entities)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Document ranks, shape (batch_size, n_docs)</p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Rank documents based on entity prediction with IDF weighting\n\n    Args:\n        ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n    Returns:\n        torch.Tensor: Document ranks, shape (batch_size, n_docs)\n    \"\"\"\n    doc_pred = torch.sparse.mm(\n        ent_pred * self.idf_weight.unsqueeze(0), self.ent2doc\n    )\n    return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.IDFWeightedTopKRanker","title":"<code>IDFWeightedTopKRanker</code>","text":"<p>               Bases: <code>BaseDocRanker</code></p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>class IDFWeightedTopKRanker(BaseDocRanker):\n    def __init__(self, ent2doc: torch.Tensor, top_k: int) -&gt; None:\n        super().__init__(ent2doc)\n        self.top_k = top_k\n        frequency = torch.sparse.sum(ent2doc, dim=-1).to_dense()\n        self.idf_weight = 1 / frequency\n        self.idf_weight[frequency == 0] = 0\n\n    def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Rank documents based on top-k entity prediction\n\n        Args:\n            ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n        Returns:\n            torch.Tensor: Document ranks, shape (batch_size, n_docs)\n        \"\"\"\n        top_k_ent_pred = torch.topk(ent_pred, self.top_k, dim=-1)\n        idf_weight = torch.gather(\n            self.idf_weight.expand(ent_pred.shape[0], -1), 1, top_k_ent_pred.indices\n        )\n        masked_ent_pred = torch.zeros_like(ent_pred, device=ent_pred.device)\n        masked_ent_pred.scatter_(1, top_k_ent_pred.indices, idf_weight)\n        doc_pred = torch.sparse.mm(masked_ent_pred, self.ent2doc)\n        return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.IDFWeightedTopKRanker.__call__","title":"<code>__call__(ent_pred)</code>","text":"<p>Rank documents based on top-k entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>ent_pred</code> <code>Tensor</code> <p>Entity prediction, shape (batch_size, n_entities)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Document ranks, shape (batch_size, n_docs)</p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Rank documents based on top-k entity prediction\n\n    Args:\n        ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n    Returns:\n        torch.Tensor: Document ranks, shape (batch_size, n_docs)\n    \"\"\"\n    top_k_ent_pred = torch.topk(ent_pred, self.top_k, dim=-1)\n    idf_weight = torch.gather(\n        self.idf_weight.expand(ent_pred.shape[0], -1), 1, top_k_ent_pred.indices\n    )\n    masked_ent_pred = torch.zeros_like(ent_pred, device=ent_pred.device)\n    masked_ent_pred.scatter_(1, top_k_ent_pred.indices, idf_weight)\n    doc_pred = torch.sparse.mm(masked_ent_pred, self.ent2doc)\n    return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.SimpleRanker","title":"<code>SimpleRanker</code>","text":"<p>               Bases: <code>BaseDocRanker</code></p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>class SimpleRanker(BaseDocRanker):\n    def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Rank documents based on entity prediction\n\n        Args:\n            ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n        Returns:\n            torch.Tensor: Document ranks, shape (batch_size, n_docs)\n        \"\"\"\n        doc_pred = torch.sparse.mm(ent_pred, self.ent2doc)\n        return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.SimpleRanker.__call__","title":"<code>__call__(ent_pred)</code>","text":"<p>Rank documents based on entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>ent_pred</code> <code>Tensor</code> <p>Entity prediction, shape (batch_size, n_entities)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Document ranks, shape (batch_size, n_docs)</p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Rank documents based on entity prediction\n\n    Args:\n        ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n    Returns:\n        torch.Tensor: Document ranks, shape (batch_size, n_docs)\n    \"\"\"\n    doc_pred = torch.sparse.mm(ent_pred, self.ent2doc)\n    return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.TopKRanker","title":"<code>TopKRanker</code>","text":"<p>               Bases: <code>BaseDocRanker</code></p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>class TopKRanker(BaseDocRanker):\n    def __init__(self, ent2doc: torch.Tensor, top_k: int) -&gt; None:\n        super().__init__(ent2doc)\n        self.top_k = top_k\n\n    def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Rank documents based on top-k entity prediction\n\n        Args:\n            ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n        Returns:\n            torch.Tensor: Document ranks, shape (batch_size, n_docs)\n        \"\"\"\n        top_k_ent_pred = torch.topk(ent_pred, self.top_k, dim=-1)\n        masked_ent_pred = torch.zeros_like(ent_pred, device=ent_pred.device)\n        masked_ent_pred.scatter_(1, top_k_ent_pred.indices, 1)\n        doc_pred = torch.sparse.mm(masked_ent_pred, self.ent2doc)\n        return doc_pred\n</code></pre>"},{"location":"api/doc_ranker/#gfmrag.doc_rankers.TopKRanker.__call__","title":"<code>__call__(ent_pred)</code>","text":"<p>Rank documents based on top-k entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>ent_pred</code> <code>Tensor</code> <p>Entity prediction, shape (batch_size, n_entities)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Document ranks, shape (batch_size, n_docs)</p> Source code in <code>gfmrag/doc_rankers.py</code> Python<pre><code>def __call__(self, ent_pred: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Rank documents based on top-k entity prediction\n\n    Args:\n        ent_pred (torch.Tensor): Entity prediction, shape (batch_size, n_entities)\n\n    Returns:\n        torch.Tensor: Document ranks, shape (batch_size, n_docs)\n    \"\"\"\n    top_k_ent_pred = torch.topk(ent_pred, self.top_k, dim=-1)\n    masked_ent_pred = torch.zeros_like(ent_pred, device=ent_pred.device)\n    masked_ent_pred.scatter_(1, top_k_ent_pred.indices, 1)\n    doc_pred = torch.sparse.mm(masked_ent_pred, self.ent2doc)\n    return doc_pred\n</code></pre>"},{"location":"api/evaluator/","title":"Evaluator","text":""},{"location":"api/evaluator/#gfmrag.evaluation","title":"<code>gfmrag.evaluation</code>","text":""},{"location":"api/evaluator/#gfmrag.evaluation.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base evaluator class for evaluation tasks.</p> <p>This abstract base class provides a foundation for implementing evaluators that assess model predictions. It handles loading prediction data from a JSON lines file where each line contains a single JSON object.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_file</code> <code>str</code> <p>Path to the JSON lines prediction file to evaluate. Each line should contain a valid JSON object.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>List[dict]</code> <p>List of prediction data loaded from the JSON lines file.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; evaluator = MyEvaluator(\"predictions.jsonl\")\n&gt;&gt;&gt; results = evaluator.evaluate()\n</code></pre> Note <p>Subclasses must implement the <code>evaluate()</code> method to define evaluation logic.</p> Source code in <code>gfmrag/evaluation/base_evaluator.py</code> Python<pre><code>class BaseEvaluator(ABC):\n    \"\"\"Base evaluator class for evaluation tasks.\n\n    This abstract base class provides a foundation for implementing evaluators\n    that assess model predictions. It handles loading prediction data from a JSON\n    lines file where each line contains a single JSON object.\n\n    Args:\n        prediction_file (str): Path to the JSON lines prediction file to evaluate.\n            Each line should contain a valid JSON object.\n\n    Attributes:\n        data (List[dict]): List of prediction data loaded from the JSON lines file.\n\n    Examples:\n        &gt;&gt;&gt; evaluator = MyEvaluator(\"predictions.jsonl\")\n        &gt;&gt;&gt; results = evaluator.evaluate()\n\n    Note:\n        Subclasses must implement the `evaluate()` method to define evaluation logic.\n    \"\"\"\n\n    def __init__(self, prediction_file: str) -&gt; None:\n        super().__init__()\n        with open(prediction_file) as f:\n            self.data = [json.loads(line) for line in f]\n\n    @abstractmethod\n    def evaluate(self) -&gt; dict:\n        pass\n</code></pre>"},{"location":"api/evaluator/#gfmrag.evaluation.HotpotQAEvaluator","title":"<code>HotpotQAEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>HotpotQAEvaluator</p> Source code in <code>gfmrag/evaluation/hotpot_qa_evaluator.py</code> Python<pre><code>class HotpotQAEvaluator(BaseEvaluator):\n    \"\"\"\n    HotpotQAEvaluator\n    \"\"\"\n\n    def evaluate(self) -&gt; dict:\n        metrics = {\"em\": 0.0, \"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n\n        for pred in self.data:\n            if \"Answer: \" in pred[\"response\"]:\n                pre_ans = pred[\"response\"].split(\"Answer:\")[1].strip()\n            else:\n                pre_ans = pred[\"response\"]\n            em, f1, prec, recall = update_answer(metrics, pre_ans, pred[\"answer\"])\n\n        n = len(self.data)\n        for k in metrics.keys():\n            metrics[k] /= n\n        return metrics\n</code></pre>"},{"location":"api/evaluator/#gfmrag.evaluation.MusiqueEvaluator","title":"<code>MusiqueEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>MusiqueEvaluator</p> Source code in <code>gfmrag/evaluation/musique_evaluator.py</code> Python<pre><code>class MusiqueEvaluator(BaseEvaluator):\n    \"\"\"\n    MusiqueEvaluator\n    \"\"\"\n\n    def evaluate(self) -&gt; dict:\n        metrics = {\"em\": 0.0, \"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n\n        for pred in self.data:\n            if \"Answer: \" in pred[\"response\"]:\n                pre_ans = pred[\"response\"].split(\"Answer:\")[1].strip()\n            else:\n                pre_ans = pred[\"response\"]\n            gold_answers = [pred[\"answer\"]] + pred[\"answer_aliases\"]\n            em = metric_max_over_ground_truths(compute_exact, pre_ans, gold_answers)\n            (\n                f1,\n                precision,\n                recall,\n            ) = metric_max_f1_over_ground_truths(compute_f1, pre_ans, gold_answers)\n            metrics[\"em\"] += float(em)\n            metrics[\"f1\"] += f1\n            metrics[\"precision\"] += precision\n            metrics[\"recall\"] += recall\n\n        n = len(self.data)\n        for k in metrics.keys():\n            metrics[k] /= n\n        return metrics\n</code></pre>"},{"location":"api/evaluator/#gfmrag.evaluation.TwoWikiQAEvaluator","title":"<code>TwoWikiQAEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>TwoWikiQAEvaluator</p> Source code in <code>gfmrag/evaluation/two_wiki_qa_evaluator.py</code> Python<pre><code>class TwoWikiQAEvaluator(BaseEvaluator):\n    \"\"\"\n    TwoWikiQAEvaluator\n    \"\"\"\n\n    def evaluate(self) -&gt; dict:\n        metrics = {\"em\": 0.0, \"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n\n        for pred in self.data:\n            if \"Answer: \" in pred[\"response\"]:\n                pre_ans = pred[\"response\"].split(\"Answer:\")[1].strip()\n            else:\n                pre_ans = pred[\"response\"]\n            gold_answers = [pred[\"answer\"]] + pred[\"answer_aliases\"]\n            em, f1, prec, recall = update_answer(metrics, pre_ans, gold_answers)\n\n        n = len(self.data)\n        for k in metrics.keys():\n            metrics[k] /= n\n        return metrics\n</code></pre>"},{"location":"api/gfmrag_retriever/","title":"GFM Retriever","text":""},{"location":"api/gfmrag_retriever/#gfmrag.GFMRetriever","title":"<code>gfmrag.GFMRetriever</code>","text":"<p>Graph Foundation Model (GFM) Retriever for document retrieval.</p> <p>This class implements a document retrieval system that combines named entity recognition, entity linking, graph neural networks, and document ranking to retrieve relevant documents based on a query.</p> <p>Attributes:</p> Name Type Description <code>qa_data</code> <code>QADataset</code> <p>Dataset containing the knowledge graph, documents and mappings</p> <code>graph</code> <code>Tensor</code> <p>Knowledge graph structure</p> <code>text_emb_model</code> <code>BaseTextEmbModel</code> <p>Model for text embedding</p> <code>ner_model</code> <code>BaseNERModel</code> <p>Named Entity Recognition model</p> <code>el_model</code> <code>BaseELModel</code> <p>Entity Linking model</p> <code>graph_retriever</code> <code>GNNRetriever</code> <p>Graph Neural Network based retriever</p> <code>doc_ranker</code> <code>BaseDocRanker</code> <p>Document ranking model</p> <code>doc_retriever</code> <code>DocumentRetriever</code> <p>Document retrieval utility</p> <code>device</code> <code>device</code> <p>Device to run computations on</p> <code>num_nodes</code> <code>int</code> <p>Number of nodes in the knowledge graph</p> <code>entities_weight</code> <code>Tensor | None</code> <p>Optional weights for entities</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; retriever = GFMRetriever.from_config(cfg)\n&gt;&gt;&gt; docs = retriever.retrieve(\"Who is the president of France?\", top_k=5)\n</code></pre> Source code in <code>gfmrag/gfmrag_retriever.py</code> Python<pre><code>class GFMRetriever:\n    \"\"\"Graph Foundation Model (GFM) Retriever for document retrieval.\n\n    This class implements a document retrieval system that combines named entity recognition,\n    entity linking, graph neural networks, and document ranking to retrieve relevant documents\n    based on a query.\n\n    Attributes:\n        qa_data (QADataset): Dataset containing the knowledge graph, documents and mappings\n        graph (torch.Tensor): Knowledge graph structure\n        text_emb_model (BaseTextEmbModel): Model for text embedding\n        ner_model (BaseNERModel): Named Entity Recognition model\n        el_model (BaseELModel): Entity Linking model\n        graph_retriever (GNNRetriever): Graph Neural Network based retriever\n        doc_ranker (BaseDocRanker): Document ranking model\n        doc_retriever (DocumentRetriever): Document retrieval utility\n        device (torch.device): Device to run computations on\n        num_nodes (int): Number of nodes in the knowledge graph\n        entities_weight (torch.Tensor | None): Optional weights for entities\n\n    Examples:\n        &gt;&gt;&gt; retriever = GFMRetriever.from_config(cfg)\n        &gt;&gt;&gt; docs = retriever.retrieve(\"Who is the president of France?\", top_k=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        qa_data: QADataset,\n        text_emb_model: BaseTextEmbModel,\n        ner_model: BaseNERModel,\n        el_model: BaseELModel,\n        graph_retriever: GNNRetriever,\n        doc_ranker: BaseDocRanker,\n        doc_retriever: utils.DocumentRetriever,\n        entities_weight: torch.Tensor | None,\n        device: torch.device,\n    ) -&gt; None:\n        self.qa_data = qa_data\n        self.graph = qa_data.kg\n        self.text_emb_model = text_emb_model\n        self.ner_model = ner_model\n        self.el_model = el_model\n        self.graph_retriever = graph_retriever\n        self.doc_ranker = doc_ranker\n        self.doc_retriever = doc_retriever\n        self.device = device\n        self.num_nodes = self.graph.num_nodes\n        self.entities_weight = entities_weight\n\n    @torch.no_grad()\n    def retrieve(self, query: str, top_k: int) -&gt; list[dict]:\n        \"\"\"\n        Retrieve documents from the corpus based on the given query.\n\n        1. Prepares the query input for the graph retriever\n        2. Executes the graph retriever forward pass to get entity predictions\n        3. Ranks documents based on entity predictions\n        4. Retrieves the top-k supporting documents\n\n        Args:\n            query (str): input query\n            top_k (int): number of documents to retrieve\n\n        Returns:\n            list[dict]: A list of retrieved documents, where each document is represented as a dictionary\n                        containing document metadata and content\n        \"\"\"\n\n        # Prepare input for deep graph retriever\n        graph_retriever_input = self.prepare_input_for_graph_retriever(query)\n        graph_retriever_input = query_utils.cuda(\n            graph_retriever_input, device=self.device\n        )\n\n        # Graph retriever forward pass\n        ent_pred = self.graph_retriever(\n            self.graph, graph_retriever_input, entities_weight=self.entities_weight\n        )\n        doc_pred = self.doc_ranker(ent_pred)[0]  # Ent2docs mapping, batch size is 1\n\n        # Retrieve the supporting documents\n        retrieved_docs = self.doc_retriever(doc_pred.cpu(), top_k=top_k)\n\n        return retrieved_docs\n\n    def prepare_input_for_graph_retriever(self, query: str) -&gt; dict:\n        \"\"\"\n        Prepare input for the graph retriever model by processing the query through entity detection, linking and embedding generation. The function performs the following steps:\n\n        1. Detects entities in the query using NER model\n        2. Links detected entities to knowledge graph entities\n        3. Converts entities to node masks\n        4. Generates question embeddings\n        5. Combines embeddings and masks into input format\n\n        Args:\n            query (str): Input query text to process\n\n        Returns:\n            dict: Dictionary containing processed inputs with keys:\n\n                - question_embeddings: Embedded representation of the query\n                - question_entities_masks: Binary mask tensor indicating entity nodes (shape: 1 x num_nodes)\n\n        Notes:\n            - If no entities are detected in query, the full query is used for entity linking\n            - Only linked entities that exist in qa_data.ent2id are included in masks\n            - Entity masks and embeddings are formatted for graph retriever model input\n        \"\"\"\n\n        # Prepare input for deep graph retriever\n        mentioned_entities = self.ner_model(query)\n        if len(mentioned_entities) == 0:\n            logger.warning(\n                \"No mentioned entities found in the query. Use the query as is for entity linking.\"\n            )\n            mentioned_entities = [query]\n        linked_entities = self.el_model(mentioned_entities, topk=1)\n        entity_ids = [\n            self.qa_data.ent2id[ent[0][\"entity\"]]\n            for ent in linked_entities.values()\n            if ent[0][\"entity\"] in self.qa_data.ent2id\n        ]\n        question_entities_masks = (\n            entities_to_mask(entity_ids, self.num_nodes).unsqueeze(0).to(self.device)\n        )  # 1 x num_nodes\n        question_embedding = self.text_emb_model.encode(\n            [query],\n            is_query=True,\n            show_progress_bar=False,\n        )\n        graph_retriever_input = {\n            \"question_embeddings\": question_embedding,\n            \"question_entities_masks\": question_entities_masks,\n        }\n        return graph_retriever_input\n\n    @staticmethod\n    def from_config(cfg: DictConfig) -&gt; \"GFMRetriever\":\n        \"\"\"\n        Constructs a GFMRetriever instance from a configuration dictionary.\n\n        This factory method initializes all necessary components for the GFM retrieval system including:\n        - Graph retrieval model\n        - Question-answering dataset\n        - Named Entity Recognition (NER) model\n        - Entity Linking (EL) model\n        - Document ranking and retrieval components\n        - Text embedding model\n\n        Args:\n            cfg (DictConfig): Configuration dictionary containing settings for:\n\n                - graph_retriever: Model path and NER/EL model configurations\n                - dataset: Dataset parameters\n                - Optional entity weight initialization flag\n\n        Returns:\n            GFMRetriever: Fully initialized retriever instance with all components loaded and\n                          moved to appropriate device (CPU/GPU)\n\n        Note:\n            The configuration must contain valid paths and parameters for all required models\n            and dataset components. Models are automatically moved to available device (CPU/GPU).\n        \"\"\"\n        graph_retriever, model_config = utils.load_model_from_pretrained(\n            cfg.graph_retriever.model_path\n        )\n        graph_retriever.eval()\n        qa_data = QADataset(\n            **cfg.dataset,\n            text_emb_model_cfgs=OmegaConf.create(model_config[\"text_emb_model_config\"]),\n        )\n        device = utils.get_device()\n        graph_retriever = graph_retriever.to(device)\n\n        qa_data.kg = qa_data.kg.to(device)\n        ent2docs = qa_data.ent2docs.to(device)\n\n        ner_model = instantiate(cfg.graph_retriever.ner_model)\n        el_model = instantiate(cfg.graph_retriever.el_model)\n\n        el_model.index(list(qa_data.ent2id.keys()))\n\n        # Create doc ranker\n        doc_ranker = instantiate(cfg.graph_retriever.doc_ranker, ent2doc=ent2docs)\n        doc_retriever = utils.DocumentRetriever(qa_data.doc, qa_data.id2doc)\n\n        text_emb_model = instantiate(\n            OmegaConf.create(model_config[\"text_emb_model_config\"])\n        )\n\n        entities_weight = None\n        if cfg.graph_retriever.init_entities_weight:\n            entities_weight = utils.get_entities_weight(ent2docs)\n\n        return GFMRetriever(\n            qa_data=qa_data,\n            text_emb_model=text_emb_model,\n            ner_model=ner_model,\n            el_model=el_model,\n            graph_retriever=graph_retriever,\n            doc_ranker=doc_ranker,\n            doc_retriever=doc_retriever,\n            entities_weight=entities_weight,\n            device=device,\n        )\n</code></pre>"},{"location":"api/gfmrag_retriever/#gfmrag.GFMRetriever.from_config","title":"<code>from_config(cfg)</code>  <code>staticmethod</code>","text":"<p>Constructs a GFMRetriever instance from a configuration dictionary.</p> <p>This factory method initializes all necessary components for the GFM retrieval system including: - Graph retrieval model - Question-answering dataset - Named Entity Recognition (NER) model - Entity Linking (EL) model - Document ranking and retrieval components - Text embedding model</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration dictionary containing settings for:</p> <ul> <li>graph_retriever: Model path and NER/EL model configurations</li> <li>dataset: Dataset parameters</li> <li>Optional entity weight initialization flag</li> </ul> required <p>Returns:</p> Name Type Description <code>GFMRetriever</code> <code>GFMRetriever</code> <p>Fully initialized retriever instance with all components loaded and           moved to appropriate device (CPU/GPU)</p> Note <p>The configuration must contain valid paths and parameters for all required models and dataset components. Models are automatically moved to available device (CPU/GPU).</p> Source code in <code>gfmrag/gfmrag_retriever.py</code> Python<pre><code>@staticmethod\ndef from_config(cfg: DictConfig) -&gt; \"GFMRetriever\":\n    \"\"\"\n    Constructs a GFMRetriever instance from a configuration dictionary.\n\n    This factory method initializes all necessary components for the GFM retrieval system including:\n    - Graph retrieval model\n    - Question-answering dataset\n    - Named Entity Recognition (NER) model\n    - Entity Linking (EL) model\n    - Document ranking and retrieval components\n    - Text embedding model\n\n    Args:\n        cfg (DictConfig): Configuration dictionary containing settings for:\n\n            - graph_retriever: Model path and NER/EL model configurations\n            - dataset: Dataset parameters\n            - Optional entity weight initialization flag\n\n    Returns:\n        GFMRetriever: Fully initialized retriever instance with all components loaded and\n                      moved to appropriate device (CPU/GPU)\n\n    Note:\n        The configuration must contain valid paths and parameters for all required models\n        and dataset components. Models are automatically moved to available device (CPU/GPU).\n    \"\"\"\n    graph_retriever, model_config = utils.load_model_from_pretrained(\n        cfg.graph_retriever.model_path\n    )\n    graph_retriever.eval()\n    qa_data = QADataset(\n        **cfg.dataset,\n        text_emb_model_cfgs=OmegaConf.create(model_config[\"text_emb_model_config\"]),\n    )\n    device = utils.get_device()\n    graph_retriever = graph_retriever.to(device)\n\n    qa_data.kg = qa_data.kg.to(device)\n    ent2docs = qa_data.ent2docs.to(device)\n\n    ner_model = instantiate(cfg.graph_retriever.ner_model)\n    el_model = instantiate(cfg.graph_retriever.el_model)\n\n    el_model.index(list(qa_data.ent2id.keys()))\n\n    # Create doc ranker\n    doc_ranker = instantiate(cfg.graph_retriever.doc_ranker, ent2doc=ent2docs)\n    doc_retriever = utils.DocumentRetriever(qa_data.doc, qa_data.id2doc)\n\n    text_emb_model = instantiate(\n        OmegaConf.create(model_config[\"text_emb_model_config\"])\n    )\n\n    entities_weight = None\n    if cfg.graph_retriever.init_entities_weight:\n        entities_weight = utils.get_entities_weight(ent2docs)\n\n    return GFMRetriever(\n        qa_data=qa_data,\n        text_emb_model=text_emb_model,\n        ner_model=ner_model,\n        el_model=el_model,\n        graph_retriever=graph_retriever,\n        doc_ranker=doc_ranker,\n        doc_retriever=doc_retriever,\n        entities_weight=entities_weight,\n        device=device,\n    )\n</code></pre>"},{"location":"api/gfmrag_retriever/#gfmrag.GFMRetriever.prepare_input_for_graph_retriever","title":"<code>prepare_input_for_graph_retriever(query)</code>","text":"<p>Prepare input for the graph retriever model by processing the query through entity detection, linking and embedding generation. The function performs the following steps:</p> <ol> <li>Detects entities in the query using NER model</li> <li>Links detected entities to knowledge graph entities</li> <li>Converts entities to node masks</li> <li>Generates question embeddings</li> <li>Combines embeddings and masks into input format</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Input query text to process</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing processed inputs with keys:</p> <ul> <li>question_embeddings: Embedded representation of the query</li> <li>question_entities_masks: Binary mask tensor indicating entity nodes (shape: 1 x num_nodes)</li> </ul> Notes <ul> <li>If no entities are detected in query, the full query is used for entity linking</li> <li>Only linked entities that exist in qa_data.ent2id are included in masks</li> <li>Entity masks and embeddings are formatted for graph retriever model input</li> </ul> Source code in <code>gfmrag/gfmrag_retriever.py</code> Python<pre><code>def prepare_input_for_graph_retriever(self, query: str) -&gt; dict:\n    \"\"\"\n    Prepare input for the graph retriever model by processing the query through entity detection, linking and embedding generation. The function performs the following steps:\n\n    1. Detects entities in the query using NER model\n    2. Links detected entities to knowledge graph entities\n    3. Converts entities to node masks\n    4. Generates question embeddings\n    5. Combines embeddings and masks into input format\n\n    Args:\n        query (str): Input query text to process\n\n    Returns:\n        dict: Dictionary containing processed inputs with keys:\n\n            - question_embeddings: Embedded representation of the query\n            - question_entities_masks: Binary mask tensor indicating entity nodes (shape: 1 x num_nodes)\n\n    Notes:\n        - If no entities are detected in query, the full query is used for entity linking\n        - Only linked entities that exist in qa_data.ent2id are included in masks\n        - Entity masks and embeddings are formatted for graph retriever model input\n    \"\"\"\n\n    # Prepare input for deep graph retriever\n    mentioned_entities = self.ner_model(query)\n    if len(mentioned_entities) == 0:\n        logger.warning(\n            \"No mentioned entities found in the query. Use the query as is for entity linking.\"\n        )\n        mentioned_entities = [query]\n    linked_entities = self.el_model(mentioned_entities, topk=1)\n    entity_ids = [\n        self.qa_data.ent2id[ent[0][\"entity\"]]\n        for ent in linked_entities.values()\n        if ent[0][\"entity\"] in self.qa_data.ent2id\n    ]\n    question_entities_masks = (\n        entities_to_mask(entity_ids, self.num_nodes).unsqueeze(0).to(self.device)\n    )  # 1 x num_nodes\n    question_embedding = self.text_emb_model.encode(\n        [query],\n        is_query=True,\n        show_progress_bar=False,\n    )\n    graph_retriever_input = {\n        \"question_embeddings\": question_embedding,\n        \"question_entities_masks\": question_entities_masks,\n    }\n    return graph_retriever_input\n</code></pre>"},{"location":"api/gfmrag_retriever/#gfmrag.GFMRetriever.retrieve","title":"<code>retrieve(query, top_k)</code>","text":"<p>Retrieve documents from the corpus based on the given query.</p> <ol> <li>Prepares the query input for the graph retriever</li> <li>Executes the graph retriever forward pass to get entity predictions</li> <li>Ranks documents based on entity predictions</li> <li>Retrieves the top-k supporting documents</li> </ol> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>input query</p> required <code>top_k</code> <code>int</code> <p>number of documents to retrieve</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of retrieved documents, where each document is represented as a dictionary         containing document metadata and content</p> Source code in <code>gfmrag/gfmrag_retriever.py</code> Python<pre><code>@torch.no_grad()\ndef retrieve(self, query: str, top_k: int) -&gt; list[dict]:\n    \"\"\"\n    Retrieve documents from the corpus based on the given query.\n\n    1. Prepares the query input for the graph retriever\n    2. Executes the graph retriever forward pass to get entity predictions\n    3. Ranks documents based on entity predictions\n    4. Retrieves the top-k supporting documents\n\n    Args:\n        query (str): input query\n        top_k (int): number of documents to retrieve\n\n    Returns:\n        list[dict]: A list of retrieved documents, where each document is represented as a dictionary\n                    containing document metadata and content\n    \"\"\"\n\n    # Prepare input for deep graph retriever\n    graph_retriever_input = self.prepare_input_for_graph_retriever(query)\n    graph_retriever_input = query_utils.cuda(\n        graph_retriever_input, device=self.device\n    )\n\n    # Graph retriever forward pass\n    ent_pred = self.graph_retriever(\n        self.graph, graph_retriever_input, entities_weight=self.entities_weight\n    )\n    doc_pred = self.doc_ranker(ent_pred)[0]  # Ent2docs mapping, batch size is 1\n\n    # Retrieve the supporting documents\n    retrieved_docs = self.doc_retriever(doc_pred.cpu(), top_k=top_k)\n\n    return retrieved_docs\n</code></pre>"},{"location":"api/kg_indexer/","title":"KG-index Constructor","text":""},{"location":"api/kg_indexer/#gfmrag.KGIndexer","title":"<code>gfmrag.KGIndexer</code>","text":"<p>A class for indexing and processing datasets by creating knowledge graph indices and preparing QA data.</p> <p>Attributes:</p> Name Type Description <code>DELIMITER</code> <code>str</code> <p>Delimiter used for separating elements in knowledge graph triples, default is <code>\",\"</code>.</p> <code>kg_constructor</code> <code>BaseKGConstructor</code> <p>Constructor for building knowledge graphs</p> <code>qa_constructor</code> <code>BaseQAConstructor</code> <p>Constructor for preparing QA datasets</p> Source code in <code>gfmrag/kg_indexer.py</code> Python<pre><code>class KGIndexer:\n    \"\"\"\n    A class for indexing and processing datasets by creating knowledge graph indices and preparing QA data.\n\n    Attributes:\n        DELIMITER (str): Delimiter used for separating elements in knowledge graph triples, default is `\",\"`.\n        kg_constructor (BaseKGConstructor): Constructor for building knowledge graphs\n        qa_constructor (BaseQAConstructor): Constructor for preparing QA datasets\n    \"\"\"\n\n    DELIMITER = KG_DELIMITER\n\n    def __init__(\n        self, kg_constructor: BaseKGConstructor, qa_constructor: BaseQAConstructor\n    ) -&gt; None:\n        \"\"\"\n        Initializes the KGIndexer with the given knowledge graph and QA constructors.\n\n        Args:\n            kg_constructor (BaseKGConstructor): An instance of a knowledge graph constructor.\n            qa_constructor (BaseQAConstructor): An instance of a QA constructor.\n\n        Returns:\n            None\n        \"\"\"\n        self.kg_constructor = kg_constructor\n        self.qa_constructor = qa_constructor\n\n    def index_data(self, dataset_cfg: DictConfig) -&gt; None:\n        \"\"\"Index and process dataset by creating knowledge graph (KG) indices and preparing QA data.\n\n        This method performs two main tasks:\n            1. Creates and saves knowledge graph related files (kg.txt and document2entities.json)\n            2. Identify the query entities and supporting entities in training and testing data if available in the raw data directory\n\n        Files created:\n            - kg.txt: Contains knowledge graph triples\n            - document2entities.json: Maps documents to their entities\n            - train.json: Processed training data (if raw exists)\n            - test.json: Processed test data (if raw exists)\n\n            Directory structure:\n            ```\n                root/\n                \u2514\u2500\u2500 data_name/\n                    \u251c\u2500\u2500 raw/\n                    |   \u251c\u2500\u2500 dataset_corpus.json\n                    \u2502   \u251c\u2500\u2500 train.json (optional)\n                    \u2502   \u2514\u2500\u2500 test.json (optional)\n                    \u2514\u2500\u2500 processed/\n                        \u2514\u2500\u2500 stage1/\n                            \u251c\u2500\u2500 kg.txt\n                            \u251c\u2500\u2500 document2entities.json\n                            \u251c\u2500\u2500 train.json\n                            \u2514\u2500\u2500 test.json\n            ```\n\n        Args:\n            dataset_cfg (DictConfig):\n                - root (str): Root directory of the dataset\n                - data_name (str): Name of the dataset\n\n        Returns:\n            None\n        \"\"\"\n\n        root = dataset_cfg.root\n        data_name = dataset_cfg.data_name\n        raw_data_dir = os.path.join(root, data_name, \"raw\")\n        prosessed_data_dir = os.path.join(root, data_name, \"processed\", \"stage1\")\n\n        if not os.path.exists(prosessed_data_dir):\n            os.makedirs(prosessed_data_dir)\n\n        # Create KG index for each dataset\n        if not os.path.exists(os.path.join(prosessed_data_dir, \"kg.txt\")):\n            logger.info(\"Stage1 KG construction\")\n            kg = self.kg_constructor.create_kg(root, data_name)\n            with open(os.path.join(prosessed_data_dir, \"kg.txt\"), \"w\") as f:\n                for triple in kg:\n                    f.write(self.DELIMITER.join(triple) + \"\\n\")\n        if not os.path.exists(\n            os.path.join(prosessed_data_dir, \"document2entities.json\")\n        ):\n            logger.info(\"Stage1 Get document2entities\")\n            doc2entities = self.kg_constructor.get_document2entities(root, data_name)\n            with open(\n                os.path.join(prosessed_data_dir, \"document2entities.json\"), \"w\"\n            ) as f:\n                json.dump(doc2entities, f, indent=4)\n\n        # Try to prepare training and testing data from dataset\n        if os.path.exists(\n            os.path.join(raw_data_dir, \"train.json\")\n        ) and not os.path.exists(os.path.join(prosessed_data_dir, \"train.json\")):\n            logger.info(f\"Preparing {os.path.join(raw_data_dir, 'train.json')}\")\n            train_data = self.qa_constructor.prepare_data(root, data_name, \"train.json\")\n            with open(os.path.join(prosessed_data_dir, \"train.json\"), \"w\") as f:\n                json.dump(train_data, f, indent=4)\n\n        if os.path.exists(\n            os.path.join(raw_data_dir, \"test.json\")\n        ) and not os.path.exists(os.path.join(prosessed_data_dir, \"test.json\")):\n            logger.info(f\"Preparing {os.path.join(raw_data_dir, 'test.json')}\")\n            test_data = self.qa_constructor.prepare_data(root, data_name, \"test.json\")\n            with open(os.path.join(prosessed_data_dir, \"test.json\"), \"w\") as f:\n                json.dump(test_data, f, indent=4)\n</code></pre>"},{"location":"api/kg_indexer/#gfmrag.KGIndexer.__init__","title":"<code>__init__(kg_constructor, qa_constructor)</code>","text":"<p>Initializes the KGIndexer with the given knowledge graph and QA constructors.</p> <p>Parameters:</p> Name Type Description Default <code>kg_constructor</code> <code>BaseKGConstructor</code> <p>An instance of a knowledge graph constructor.</p> required <code>qa_constructor</code> <code>BaseQAConstructor</code> <p>An instance of a QA constructor.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/kg_indexer.py</code> Python<pre><code>def __init__(\n    self, kg_constructor: BaseKGConstructor, qa_constructor: BaseQAConstructor\n) -&gt; None:\n    \"\"\"\n    Initializes the KGIndexer with the given knowledge graph and QA constructors.\n\n    Args:\n        kg_constructor (BaseKGConstructor): An instance of a knowledge graph constructor.\n        qa_constructor (BaseQAConstructor): An instance of a QA constructor.\n\n    Returns:\n        None\n    \"\"\"\n    self.kg_constructor = kg_constructor\n    self.qa_constructor = qa_constructor\n</code></pre>"},{"location":"api/kg_indexer/#gfmrag.KGIndexer.index_data","title":"<code>index_data(dataset_cfg)</code>","text":"<p>Index and process dataset by creating knowledge graph (KG) indices and preparing QA data.</p> This method performs two main tasks <ol> <li>Creates and saves knowledge graph related files (kg.txt and document2entities.json)</li> <li>Identify the query entities and supporting entities in training and testing data if available in the raw data directory</li> </ol> Files created <ul> <li>kg.txt: Contains knowledge graph triples</li> <li>document2entities.json: Maps documents to their entities</li> <li>train.json: Processed training data (if raw exists)</li> <li>test.json: Processed test data (if raw exists)</li> </ul> <p>Directory structure: Text Only<pre><code>    root/\n    \u2514\u2500\u2500 data_name/\n        \u251c\u2500\u2500 raw/\n        |   \u251c\u2500\u2500 dataset_corpus.json\n        \u2502   \u251c\u2500\u2500 train.json (optional)\n        \u2502   \u2514\u2500\u2500 test.json (optional)\n        \u2514\u2500\u2500 processed/\n            \u2514\u2500\u2500 stage1/\n                \u251c\u2500\u2500 kg.txt\n                \u251c\u2500\u2500 document2entities.json\n                \u251c\u2500\u2500 train.json\n                \u2514\u2500\u2500 test.json\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataset_cfg</code> <code>DictConfig</code> <ul> <li>root (str): Root directory of the dataset</li> <li>data_name (str): Name of the dataset</li> </ul> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/kg_indexer.py</code> Python<pre><code>def index_data(self, dataset_cfg: DictConfig) -&gt; None:\n    \"\"\"Index and process dataset by creating knowledge graph (KG) indices and preparing QA data.\n\n    This method performs two main tasks:\n        1. Creates and saves knowledge graph related files (kg.txt and document2entities.json)\n        2. Identify the query entities and supporting entities in training and testing data if available in the raw data directory\n\n    Files created:\n        - kg.txt: Contains knowledge graph triples\n        - document2entities.json: Maps documents to their entities\n        - train.json: Processed training data (if raw exists)\n        - test.json: Processed test data (if raw exists)\n\n        Directory structure:\n        ```\n            root/\n            \u2514\u2500\u2500 data_name/\n                \u251c\u2500\u2500 raw/\n                |   \u251c\u2500\u2500 dataset_corpus.json\n                \u2502   \u251c\u2500\u2500 train.json (optional)\n                \u2502   \u2514\u2500\u2500 test.json (optional)\n                \u2514\u2500\u2500 processed/\n                    \u2514\u2500\u2500 stage1/\n                        \u251c\u2500\u2500 kg.txt\n                        \u251c\u2500\u2500 document2entities.json\n                        \u251c\u2500\u2500 train.json\n                        \u2514\u2500\u2500 test.json\n        ```\n\n    Args:\n        dataset_cfg (DictConfig):\n            - root (str): Root directory of the dataset\n            - data_name (str): Name of the dataset\n\n    Returns:\n        None\n    \"\"\"\n\n    root = dataset_cfg.root\n    data_name = dataset_cfg.data_name\n    raw_data_dir = os.path.join(root, data_name, \"raw\")\n    prosessed_data_dir = os.path.join(root, data_name, \"processed\", \"stage1\")\n\n    if not os.path.exists(prosessed_data_dir):\n        os.makedirs(prosessed_data_dir)\n\n    # Create KG index for each dataset\n    if not os.path.exists(os.path.join(prosessed_data_dir, \"kg.txt\")):\n        logger.info(\"Stage1 KG construction\")\n        kg = self.kg_constructor.create_kg(root, data_name)\n        with open(os.path.join(prosessed_data_dir, \"kg.txt\"), \"w\") as f:\n            for triple in kg:\n                f.write(self.DELIMITER.join(triple) + \"\\n\")\n    if not os.path.exists(\n        os.path.join(prosessed_data_dir, \"document2entities.json\")\n    ):\n        logger.info(\"Stage1 Get document2entities\")\n        doc2entities = self.kg_constructor.get_document2entities(root, data_name)\n        with open(\n            os.path.join(prosessed_data_dir, \"document2entities.json\"), \"w\"\n        ) as f:\n            json.dump(doc2entities, f, indent=4)\n\n    # Try to prepare training and testing data from dataset\n    if os.path.exists(\n        os.path.join(raw_data_dir, \"train.json\")\n    ) and not os.path.exists(os.path.join(prosessed_data_dir, \"train.json\")):\n        logger.info(f\"Preparing {os.path.join(raw_data_dir, 'train.json')}\")\n        train_data = self.qa_constructor.prepare_data(root, data_name, \"train.json\")\n        with open(os.path.join(prosessed_data_dir, \"train.json\"), \"w\") as f:\n            json.dump(train_data, f, indent=4)\n\n    if os.path.exists(\n        os.path.join(raw_data_dir, \"test.json\")\n    ) and not os.path.exists(os.path.join(prosessed_data_dir, \"test.json\")):\n        logger.info(f\"Preparing {os.path.join(raw_data_dir, 'test.json')}\")\n        test_data = self.qa_constructor.prepare_data(root, data_name, \"test.json\")\n        with open(os.path.join(prosessed_data_dir, \"test.json\"), \"w\") as f:\n            json.dump(test_data, f, indent=4)\n</code></pre>"},{"location":"api/llms/","title":"Large Language Models","text":""},{"location":"api/llms/#gfmrag.llms","title":"<code>gfmrag.llms</code>","text":""},{"location":"api/llms/#gfmrag.llms.BaseLanguageModel","title":"<code>BaseLanguageModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base lanuage model. Define how to generate sentence by using a LM</p> Source code in <code>gfmrag/llms/base_language_model.py</code> Python<pre><code>class BaseLanguageModel(ABC):\n    \"\"\"\n    Base lanuage model. Define how to generate sentence by using a LM\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, model_name_or_path: str):\n        pass\n\n    @abstractmethod\n    def token_len(self, text: str) -&gt; int:\n        \"\"\"\n        Return tokenized length of text\n\n        Args:\n            text (str): input text\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_sentence(\n        self, llm_input: str | list, system_input: str = \"\"\n    ) -&gt; str | Exception:\n        \"\"\"\n        Generate sentence by using a LM\n\n        Args:\n            lm_input (LMInput): input for LM\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.BaseLanguageModel.generate_sentence","title":"<code>generate_sentence(llm_input, system_input='')</code>  <code>abstractmethod</code>","text":"<p>Generate sentence by using a LM</p> <p>Parameters:</p> Name Type Description Default <code>lm_input</code> <code>LMInput</code> <p>input for LM</p> required Source code in <code>gfmrag/llms/base_language_model.py</code> Python<pre><code>@abstractmethod\ndef generate_sentence(\n    self, llm_input: str | list, system_input: str = \"\"\n) -&gt; str | Exception:\n    \"\"\"\n    Generate sentence by using a LM\n\n    Args:\n        lm_input (LMInput): input for LM\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.BaseLanguageModel.token_len","title":"<code>token_len(text)</code>  <code>abstractmethod</code>","text":"<p>Return tokenized length of text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text</p> required Source code in <code>gfmrag/llms/base_language_model.py</code> Python<pre><code>@abstractmethod\ndef token_len(self, text: str) -&gt; int:\n    \"\"\"\n    Return tokenized length of text\n\n    Args:\n        text (str): input text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.ChatGPT","title":"<code>ChatGPT</code>","text":"<p>               Bases: <code>BaseLanguageModel</code></p> <p>A class that interacts with OpenAI's ChatGPT models through their API.</p> <p>This class provides functionality to generate text using ChatGPT models while handling token limits, retries, and various input formats.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>The name or path of the ChatGPT model to use</p> required <code>retry</code> <code>int</code> <p>Number of retries for failed API calls. Defaults to 5</p> <code>5</code> <p>Attributes:</p> Name Type Description <code>retry</code> <code>int</code> <p>Maximum number of retry attempts for failed API calls</p> <code>model_name</code> <code>str</code> <p>Name of the ChatGPT model being used</p> <code>maximun_token</code> <code>int</code> <p>Maximum token limit for the specified model</p> <code>client</code> <code>OpenAI</code> <p>OpenAI client instance for API interactions</p> <p>Methods:</p> Name Description <code>token_len</code> <p>Calculate the number of tokens in a given text</p> <code>generate_sentence</code> <p>Generate response using the ChatGPT model</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the specified model is not found when calculating tokens</p> <code>Exception</code> <p>If generation fails after maximum retries</p> Source code in <code>gfmrag/llms/chatgpt.py</code> Python<pre><code>class ChatGPT(BaseLanguageModel):\n    \"\"\"A class that interacts with OpenAI's ChatGPT models through their API.\n\n    This class provides functionality to generate text using ChatGPT models while handling\n    token limits, retries, and various input formats.\n\n    Args:\n        model_name_or_path (str): The name or path of the ChatGPT model to use\n        retry (int, optional): Number of retries for failed API calls. Defaults to 5\n\n    Attributes:\n        retry (int): Maximum number of retry attempts for failed API calls\n        model_name (str): Name of the ChatGPT model being used\n        maximun_token (int): Maximum token limit for the specified model\n        client (OpenAI): OpenAI client instance for API interactions\n\n    Methods:\n        token_len(text): Calculate the number of tokens in a given text\n        generate_sentence(llm_input, system_input): Generate response using the ChatGPT model\n\n    Raises:\n        KeyError: If the specified model is not found when calculating tokens\n        Exception: If generation fails after maximum retries\n    \"\"\"\n\n    def __init__(self, model_name_or_path: str, retry: int = 5):\n        self.retry = retry\n        self.model_name = model_name_or_path\n        self.maximun_token = get_token_limit(self.model_name)\n\n        client = OpenAI(\n            api_key=os.environ[\n                \"OPENAI_API_KEY\"\n            ],  # this is also the default, it can be omitted\n        )\n        self.client = client\n\n    def token_len(self, text: str) -&gt; int:\n        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n        try:\n            encoding = tiktoken.encoding_for_model(self.model_name)\n            num_tokens = len(encoding.encode(text))\n        except KeyError as e:\n            raise KeyError(f\"Warning: model {self.model_name} not found.\") from e\n        return num_tokens\n\n    def generate_sentence(\n        self, llm_input: str | list, system_input: str = \"\"\n    ) -&gt; str | Exception:\n        \"\"\"Generate a response using the ChatGPT API.\n\n        This method sends a request to the ChatGPT API and returns the generated response.\n        It handles both single string inputs and message lists, with retry logic for failed attempts.\n\n        Args:\n            llm_input (Union[str, list]): Either a string containing the user's input or a list of message dictionaries\n                in the format [{\"role\": \"role_type\", \"content\": \"message_content\"}, ...]\n            system_input (str, optional): System message to be prepended to the conversation. Defaults to \"\".\n\n        Returns:\n            Union[str, Exception]: The generated response text if successful, or the Exception if all retries fail.\n                The response is stripped of leading/trailing whitespace.\n\n        Raises:\n            Exception: If all retry attempts fail, returns the last encountered exception.\n\n        Notes:\n            - Automatically truncates inputs that exceed the maximum token limit\n            - Uses exponential backoff with 30 second delays between retries\n            - Sets temperature to 0.0 for deterministic outputs\n            - Timeout is set to 60 seconds per API call\n        \"\"\"\n\n        # If the input is a list, it is assumed that the input is a list of messages\n        if isinstance(llm_input, list):\n            message = llm_input\n        else:\n            message = []\n            if system_input:\n                message.append({\"role\": \"system\", \"content\": system_input})\n            message.append({\"role\": \"user\", \"content\": llm_input})\n        cur_retry = 0\n        num_retry = self.retry\n        # Check if the input is too long\n        message_string = \"\\n\".join([m[\"content\"] for m in message])\n        input_length = self.token_len(message_string)\n        if input_length &gt; self.maximun_token:\n            print(\n                f\"Input lengt {input_length} is too long. The maximum token is {self.maximun_token}.\\n Right tuncate the input to {self.maximun_token} tokens.\"\n            )\n            llm_input = llm_input[: self.maximun_token]\n        error = Exception(\"Failed to generate sentence\")\n        while cur_retry &lt;= num_retry:\n            try:\n                response = self.client.chat.completions.create(\n                    model=self.model_name, messages=message, timeout=60, temperature=0.0\n                )\n                result = response.choices[0].message.content.strip()  # type: ignore\n                return result\n            except Exception as e:\n                logger.error(\"Message: \", llm_input)\n                logger.error(\"Number of token: \", self.token_len(message_string))\n                logger.error(e)\n                time.sleep(30)\n                cur_retry += 1\n                error = e\n                continue\n        return error\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.ChatGPT.generate_sentence","title":"<code>generate_sentence(llm_input, system_input='')</code>","text":"<p>Generate a response using the ChatGPT API.</p> <p>This method sends a request to the ChatGPT API and returns the generated response. It handles both single string inputs and message lists, with retry logic for failed attempts.</p> <p>Parameters:</p> Name Type Description Default <code>llm_input</code> <code>Union[str, list]</code> <p>Either a string containing the user's input or a list of message dictionaries in the format [{\"role\": \"role_type\", \"content\": \"message_content\"}, ...]</p> required <code>system_input</code> <code>str</code> <p>System message to be prepended to the conversation. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>str | Exception</code> <p>Union[str, Exception]: The generated response text if successful, or the Exception if all retries fail. The response is stripped of leading/trailing whitespace.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If all retry attempts fail, returns the last encountered exception.</p> Notes <ul> <li>Automatically truncates inputs that exceed the maximum token limit</li> <li>Uses exponential backoff with 30 second delays between retries</li> <li>Sets temperature to 0.0 for deterministic outputs</li> <li>Timeout is set to 60 seconds per API call</li> </ul> Source code in <code>gfmrag/llms/chatgpt.py</code> Python<pre><code>def generate_sentence(\n    self, llm_input: str | list, system_input: str = \"\"\n) -&gt; str | Exception:\n    \"\"\"Generate a response using the ChatGPT API.\n\n    This method sends a request to the ChatGPT API and returns the generated response.\n    It handles both single string inputs and message lists, with retry logic for failed attempts.\n\n    Args:\n        llm_input (Union[str, list]): Either a string containing the user's input or a list of message dictionaries\n            in the format [{\"role\": \"role_type\", \"content\": \"message_content\"}, ...]\n        system_input (str, optional): System message to be prepended to the conversation. Defaults to \"\".\n\n    Returns:\n        Union[str, Exception]: The generated response text if successful, or the Exception if all retries fail.\n            The response is stripped of leading/trailing whitespace.\n\n    Raises:\n        Exception: If all retry attempts fail, returns the last encountered exception.\n\n    Notes:\n        - Automatically truncates inputs that exceed the maximum token limit\n        - Uses exponential backoff with 30 second delays between retries\n        - Sets temperature to 0.0 for deterministic outputs\n        - Timeout is set to 60 seconds per API call\n    \"\"\"\n\n    # If the input is a list, it is assumed that the input is a list of messages\n    if isinstance(llm_input, list):\n        message = llm_input\n    else:\n        message = []\n        if system_input:\n            message.append({\"role\": \"system\", \"content\": system_input})\n        message.append({\"role\": \"user\", \"content\": llm_input})\n    cur_retry = 0\n    num_retry = self.retry\n    # Check if the input is too long\n    message_string = \"\\n\".join([m[\"content\"] for m in message])\n    input_length = self.token_len(message_string)\n    if input_length &gt; self.maximun_token:\n        print(\n            f\"Input lengt {input_length} is too long. The maximum token is {self.maximun_token}.\\n Right tuncate the input to {self.maximun_token} tokens.\"\n        )\n        llm_input = llm_input[: self.maximun_token]\n    error = Exception(\"Failed to generate sentence\")\n    while cur_retry &lt;= num_retry:\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name, messages=message, timeout=60, temperature=0.0\n            )\n            result = response.choices[0].message.content.strip()  # type: ignore\n            return result\n        except Exception as e:\n            logger.error(\"Message: \", llm_input)\n            logger.error(\"Number of token: \", self.token_len(message_string))\n            logger.error(e)\n            time.sleep(30)\n            cur_retry += 1\n            error = e\n            continue\n    return error\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.ChatGPT.token_len","title":"<code>token_len(text)</code>","text":"<p>Returns the number of tokens used by a list of messages.</p> Source code in <code>gfmrag/llms/chatgpt.py</code> Python<pre><code>def token_len(self, text: str) -&gt; int:\n    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(self.model_name)\n        num_tokens = len(encoding.encode(text))\n    except KeyError as e:\n        raise KeyError(f\"Warning: model {self.model_name} not found.\") from e\n    return num_tokens\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.HfCausalModel","title":"<code>HfCausalModel</code>","text":"<p>               Bases: <code>BaseLanguageModel</code></p> <p>A class for handling Hugging Face causal language models with various configurations.</p> <p>This class provides functionality to load and use Hugging Face's causal language models with different precision types, quantization options, and attention implementations.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <p>str The name or path of the pre-trained model to load</p> required <code>maximun_token</code> <p>int, optional Maximum number of tokens for the model input, by default 4096</p> <code>4096</code> <code>max_new_tokens</code> <p>int, optional Maximum number of new tokens to generate, by default 1024</p> <code>1024</code> <code>dtype</code> <p>str, optional Data type for model computation ('fp32', 'fp16', or 'bf16'), by default 'bf16'</p> <code>'bf16'</code> <code>quant</code> <p>str or None, optional Quantization option (None, '4bit', or '8bit'), by default None</p> <code>None</code> <code>attn_implementation</code> <p>str, optional Attention implementation method ('eager', 'sdpa', or 'flash_attention_2'), by default 'flash_attention_2'</p> <code>'flash_attention_2'</code> <p>Methods:</p> Name Description <code>token_len</code> <p>str) -&gt; int Returns the number of tokens in the input text</p> <code>generate_sentence</code> <p>Union[str, list], system_input: str = \"\") -&gt; Union[str, Exception] Generates text based on the input prompt or message list</p> Source code in <code>gfmrag/llms/base_hf_causal_model.py</code> Python<pre><code>class HfCausalModel(BaseLanguageModel):\n    \"\"\"A class for handling Hugging Face causal language models with various configurations.\n\n    This class provides functionality to load and use Hugging Face's causal language models\n    with different precision types, quantization options, and attention implementations.\n\n    Args:\n        model_name_or_path : str\n            The name or path of the pre-trained model to load\n        maximun_token : int, optional\n            Maximum number of tokens for the model input, by default 4096\n        max_new_tokens : int, optional\n            Maximum number of new tokens to generate, by default 1024\n        dtype : str, optional\n            Data type for model computation ('fp32', 'fp16', or 'bf16'), by default 'bf16'\n        quant : str or None, optional\n            Quantization option (None, '4bit', or '8bit'), by default None\n        attn_implementation : str, optional\n            Attention implementation method ('eager', 'sdpa', or 'flash_attention_2'),\n            by default 'flash_attention_2'\n\n    Methods:\n        token_len(text: str) -&gt; int\n            Returns the number of tokens in the input text\n        generate_sentence(llm_input: Union[str, list], system_input: str = \"\") -&gt; Union[str, Exception]\n            Generates text based on the input prompt or message list\n    \"\"\"\n\n    DTYPE = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n    QUANT = [None, \"4bit\", \"8bit\"]\n    ATTEN_IMPLEMENTATION = [\"eager\", \"sdpa\", \"flash_attention_2\"]\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        maximun_token: int = 4096,\n        max_new_tokens: int = 1024,\n        dtype: str = \"bf16\",\n        quant: None | str = None,\n        attn_implementation: str = \"flash_attention_2\",\n    ):\n        assert quant in self.QUANT, f\"quant should be one of {self.QUANT}\"\n        assert attn_implementation in self.ATTEN_IMPLEMENTATION, (\n            f\"attn_implementation should be one of {self.ATTEN_IMPLEMENTATION}\"\n        )\n        assert dtype in self.DTYPE, f\"dtype should be one of {self.DTYPE}\"\n        self.maximun_token = maximun_token\n        self.max_new_tokens = max_new_tokens\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            model_name_or_path, token=HF_TOKEN, trust_remote_code=True\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name_or_path,\n            device_map=\"auto\",\n            token=HF_TOKEN,\n            torch_dtype=self.DTYPE.get(dtype, None),\n            load_in_8bit=quant == \"8bit\",\n            load_in_4bit=quant == \"4bit\",\n            trust_remote_code=True,\n            attn_implementation=attn_implementation,\n        )\n        self.maximun_token = self.tokenizer.model_max_length\n        self.generator = pipeline(\n            \"text-generation\", model=model, tokenizer=self.tokenizer\n        )\n\n    def token_len(self, text: str) -&gt; int:\n        return len(self.tokenizer.tokenize(text))\n\n    @torch.inference_mode()\n    def generate_sentence(\n        self, llm_input: str | list, system_input: str = \"\"\n    ) -&gt; str | Exception:\n        \"\"\"\n        Generate sentence by using a Language Model.\n\n        This method processes input (either a string or a list of messages) and generates text using the configured language model.\n        If a system prompt is provided along with a string input, it will be included in the message structure.\n\n        Args:\n            llm_input (Union[str, list]): Input for the language model. Can be either a string containing the prompt,\n                or a list of message dictionaries with 'role' and 'content' keys.\n            system_input (str, optional): System prompt to be prepended to the input. Only used when llm_input is a string.\n                Defaults to empty string.\n\n            Union[str, Exception]: Generated text output from the language model if successful,\n                or the Exception object if generation fails.\n\n        Examples:\n            &gt;&gt;&gt; # Using string input with system prompt\n            &gt;&gt;&gt; model.generate_sentence(\"Tell me a joke\", system_input=\"Be funny\")\n\n            &gt;&gt;&gt; # Using message list input\n            &gt;&gt;&gt; messages = [\n            ...     {\"role\": \"system\", \"content\": \"Be helpful\"},\n            ...     {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n            ... ]\n            &gt;&gt;&gt; model.generate_sentence(messages)\n        \"\"\"\n        # If the input is a list, it is assumed that the input is a list of messages\n        if isinstance(llm_input, list):\n            message = llm_input\n        else:\n            message = []\n            if system_input:\n                message.append({\"role\": \"system\", \"content\": system_input})\n            message.append({\"role\": \"user\", \"content\": llm_input})\n        try:\n            outputs = self.generator(\n                message,\n                return_full_text=False,\n                max_new_tokens=self.max_new_tokens,\n                handle_long_generation=\"hole\",\n            )\n            return outputs[0][\"generated_text\"].strip()  # type: ignore\n        except Exception as e:\n            return e\n</code></pre>"},{"location":"api/llms/#gfmrag.llms.HfCausalModel.generate_sentence","title":"<code>generate_sentence(llm_input, system_input='')</code>","text":"<p>Generate sentence by using a Language Model.</p> <p>This method processes input (either a string or a list of messages) and generates text using the configured language model. If a system prompt is provided along with a string input, it will be included in the message structure.</p> <p>Parameters:</p> Name Type Description Default <code>llm_input</code> <code>Union[str, list]</code> <p>Input for the language model. Can be either a string containing the prompt, or a list of message dictionaries with 'role' and 'content' keys.</p> required <code>system_input</code> <code>str</code> <p>System prompt to be prepended to the input. Only used when llm_input is a string. Defaults to empty string.</p> <code>''</code> <code>Union[str,</code> <code>Exception]</code> <p>Generated text output from the language model if successful, or the Exception object if generation fails.</p> required <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; # Using string input with system prompt\n&gt;&gt;&gt; model.generate_sentence(\"Tell me a joke\", system_input=\"Be funny\")\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Using message list input\n&gt;&gt;&gt; messages = [\n...     {\"role\": \"system\", \"content\": \"Be helpful\"},\n...     {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n... ]\n&gt;&gt;&gt; model.generate_sentence(messages)\n</code></pre> Source code in <code>gfmrag/llms/base_hf_causal_model.py</code> Python<pre><code>@torch.inference_mode()\ndef generate_sentence(\n    self, llm_input: str | list, system_input: str = \"\"\n) -&gt; str | Exception:\n    \"\"\"\n    Generate sentence by using a Language Model.\n\n    This method processes input (either a string or a list of messages) and generates text using the configured language model.\n    If a system prompt is provided along with a string input, it will be included in the message structure.\n\n    Args:\n        llm_input (Union[str, list]): Input for the language model. Can be either a string containing the prompt,\n            or a list of message dictionaries with 'role' and 'content' keys.\n        system_input (str, optional): System prompt to be prepended to the input. Only used when llm_input is a string.\n            Defaults to empty string.\n\n        Union[str, Exception]: Generated text output from the language model if successful,\n            or the Exception object if generation fails.\n\n    Examples:\n        &gt;&gt;&gt; # Using string input with system prompt\n        &gt;&gt;&gt; model.generate_sentence(\"Tell me a joke\", system_input=\"Be funny\")\n\n        &gt;&gt;&gt; # Using message list input\n        &gt;&gt;&gt; messages = [\n        ...     {\"role\": \"system\", \"content\": \"Be helpful\"},\n        ...     {\"role\": \"user\", \"content\": \"Tell me a joke\"}\n        ... ]\n        &gt;&gt;&gt; model.generate_sentence(messages)\n    \"\"\"\n    # If the input is a list, it is assumed that the input is a list of messages\n    if isinstance(llm_input, list):\n        message = llm_input\n    else:\n        message = []\n        if system_input:\n            message.append({\"role\": \"system\", \"content\": system_input})\n        message.append({\"role\": \"user\", \"content\": llm_input})\n    try:\n        outputs = self.generator(\n            message,\n            return_full_text=False,\n            max_new_tokens=self.max_new_tokens,\n            handle_long_generation=\"hole\",\n        )\n        return outputs[0][\"generated_text\"].strip()  # type: ignore\n    except Exception as e:\n        return e\n</code></pre>"},{"location":"api/losses/","title":"Loss Functions","text":""},{"location":"api/losses/#gfmrag.losses","title":"<code>gfmrag.losses</code>","text":""},{"location":"api/losses/#gfmrag.losses.BCELoss","title":"<code>BCELoss</code>","text":"<p>               Bases: <code>BaseLoss</code></p> <p>Binary Cross Entropy loss function with adversarial temperature.</p> Source code in <code>gfmrag/losses.py</code> Python<pre><code>class BCELoss(BaseLoss):\n    \"\"\"\n    Binary Cross Entropy loss function with adversarial temperature.\n    \"\"\"\n\n    def __init__(\n        self, adversarial_temperature: float = 0, *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Initialize the loss function.\n\n        Args:\n            adversarial_temperature (float, optional): Temperature parameter for adversarial loss scaling. Defaults to 0.\n            *args (Any): Variable length argument list.\n            **kwargs (Any): Arbitrary keyword arguments.\n\n        Returns:\n            None\n        \"\"\"\n        self.adversarial_temperature = adversarial_temperature\n\n    def __call__(\n        self, pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Calculate the weighted binary cross-entropy loss with adversarial temperature.\n\n        This method implements a custom loss function that applies different weights to positive\n        and negative samples. For negative samples, it can optionally use adversarial temperature\n        to compute softmax-based weights.\n\n        Args:\n            pred (torch.Tensor): The predicted logits tensor\n            target (torch.Tensor): The target tensor with binary labels (0 or 1)\n            *args (Any): Variable length argument list\n            **kwargs (Any): Arbitrary keyword arguments\n\n        Returns:\n            Any: The computed loss value\n\n        The loss calculation involves:\n\n        1. Computing binary cross entropy loss\n        2. Identifying positive and negative samples\n        3. Applying weights to negative samples based on adversarial_temperature\n        4. Computing weighted average of the losses\n        \"\"\"\n        loss = F.binary_cross_entropy_with_logits(pred, target, reduction=\"none\")\n        is_positive = target &gt; 0.5\n        is_negative = target &lt;= 0.5\n        num_positive = is_positive.sum(dim=-1)\n        num_negative = is_negative.sum(dim=-1)\n\n        neg_weight = torch.zeros_like(pred)\n        neg_weight[is_positive] = (1 / num_positive.float()).repeat_interleave(\n            num_positive\n        )\n\n        if self.adversarial_temperature &gt; 0:\n            with torch.no_grad():\n                logit = pred[is_negative] / self.adversarial_temperature\n                neg_weight[is_negative] = variadic_softmax(logit, num_negative)\n                # neg_weight[:, 1:] = F.softmax(pred[:, 1:] / cfg.task.adversarial_temperature, dim=-1)\n        else:\n            neg_weight[is_negative] = (1 / num_negative.float()).repeat_interleave(\n                num_negative\n            )\n        loss = (loss * neg_weight).sum(dim=-1) / neg_weight.sum(dim=-1)\n        loss = loss.mean()\n        return loss\n</code></pre>"},{"location":"api/losses/#gfmrag.losses.BCELoss.__call__","title":"<code>__call__(pred, target, *args, **kwargs)</code>","text":"<p>Calculate the weighted binary cross-entropy loss with adversarial temperature.</p> <p>This method implements a custom loss function that applies different weights to positive and negative samples. For negative samples, it can optionally use adversarial temperature to compute softmax-based weights.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>The predicted logits tensor</p> required <code>target</code> <code>Tensor</code> <p>The target tensor with binary labels (0 or 1)</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The computed loss value</p> <p>The loss calculation involves:</p> <ol> <li>Computing binary cross entropy loss</li> <li>Identifying positive and negative samples</li> <li>Applying weights to negative samples based on adversarial_temperature</li> <li>Computing weighted average of the losses</li> </ol> Source code in <code>gfmrag/losses.py</code> Python<pre><code>def __call__(\n    self, pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any\n) -&gt; Any:\n    \"\"\"Calculate the weighted binary cross-entropy loss with adversarial temperature.\n\n    This method implements a custom loss function that applies different weights to positive\n    and negative samples. For negative samples, it can optionally use adversarial temperature\n    to compute softmax-based weights.\n\n    Args:\n        pred (torch.Tensor): The predicted logits tensor\n        target (torch.Tensor): The target tensor with binary labels (0 or 1)\n        *args (Any): Variable length argument list\n        **kwargs (Any): Arbitrary keyword arguments\n\n    Returns:\n        Any: The computed loss value\n\n    The loss calculation involves:\n\n    1. Computing binary cross entropy loss\n    2. Identifying positive and negative samples\n    3. Applying weights to negative samples based on adversarial_temperature\n    4. Computing weighted average of the losses\n    \"\"\"\n    loss = F.binary_cross_entropy_with_logits(pred, target, reduction=\"none\")\n    is_positive = target &gt; 0.5\n    is_negative = target &lt;= 0.5\n    num_positive = is_positive.sum(dim=-1)\n    num_negative = is_negative.sum(dim=-1)\n\n    neg_weight = torch.zeros_like(pred)\n    neg_weight[is_positive] = (1 / num_positive.float()).repeat_interleave(\n        num_positive\n    )\n\n    if self.adversarial_temperature &gt; 0:\n        with torch.no_grad():\n            logit = pred[is_negative] / self.adversarial_temperature\n            neg_weight[is_negative] = variadic_softmax(logit, num_negative)\n            # neg_weight[:, 1:] = F.softmax(pred[:, 1:] / cfg.task.adversarial_temperature, dim=-1)\n    else:\n        neg_weight[is_negative] = (1 / num_negative.float()).repeat_interleave(\n            num_negative\n        )\n    loss = (loss * neg_weight).sum(dim=-1) / neg_weight.sum(dim=-1)\n    loss = loss.mean()\n    return loss\n</code></pre>"},{"location":"api/losses/#gfmrag.losses.BCELoss.__init__","title":"<code>__init__(adversarial_temperature=0, *args, **kwargs)</code>","text":"<p>Initialize the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>adversarial_temperature</code> <code>float</code> <p>Temperature parameter for adversarial loss scaling. Defaults to 0.</p> <code>0</code> <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/losses.py</code> Python<pre><code>def __init__(\n    self, adversarial_temperature: float = 0, *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"Initialize the loss function.\n\n    Args:\n        adversarial_temperature (float, optional): Temperature parameter for adversarial loss scaling. Defaults to 0.\n        *args (Any): Variable length argument list.\n        **kwargs (Any): Arbitrary keyword arguments.\n\n    Returns:\n        None\n    \"\"\"\n    self.adversarial_temperature = adversarial_temperature\n</code></pre>"},{"location":"api/losses/#gfmrag.losses.BaseLoss","title":"<code>BaseLoss</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base abstract class for all loss functions.</p> <p>This class serves as a template for implementing custom loss functions. All loss functions should inherit from this class and implement the required abstract methods.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Any, **kwargs: Any) -&gt; None Initialize the loss function with given parameters.</p> <code>__call__</code> <p>torch.Tensor, target: torch.Tensor, args: Any, *kwargs: Any) -&gt; Any Calculate the loss between predicted and target values.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <p>torch.Tensor The predicted values from the model</p> required <code>target</code> <p>torch.Tensor The ground truth values</p> required <code>*args</code> <p>Any Variable length argument list</p> <code>()</code> <code>**kwargs</code> <p>Any Arbitrary keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The computed loss value</p> Source code in <code>gfmrag/losses.py</code> Python<pre><code>class BaseLoss(ABC):\n    \"\"\"Base abstract class for all loss functions.\n\n    This class serves as a template for implementing custom loss functions. All loss\n    functions should inherit from this class and implement the required abstract methods.\n\n    Methods:\n        __init__(*args: Any, **kwargs: Any) -&gt; None\n            Initialize the loss function with given parameters.\n\n        __call__(pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any) -&gt; Any\n            Calculate the loss between predicted and target values.\n\n    Args:\n        pred : torch.Tensor\n            The predicted values from the model\n        target : torch.Tensor\n            The ground truth values\n        *args : Any\n            Variable length argument list\n        **kwargs : Any\n            Arbitrary keyword arguments\n\n    Returns:\n        Any: The computed loss value\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    @abstractmethod\n    def __call__(\n        self, pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; Any:\n        pass\n</code></pre>"},{"location":"api/losses/#gfmrag.losses.ListCELoss","title":"<code>ListCELoss</code>","text":"<p>               Bases: <code>BaseLoss</code></p> <p>Ranking loss for multi-label target lists.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted logits tensor of shape (B x N) where B is batch size and N is number of possible labels.</p> required <code>target</code> <code>Tensor</code> <p>Binary target tensor of shape (B x N) where 1 indicates positive labels and 0 indicates negative labels.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments (unused).</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <p>torch.Tensor: Scalar tensor containing the mean loss value.</p> Notes <ul> <li>Empty targets (all zeros) are automatically skipped in loss computation</li> <li>Small epsilon values (1e-5) are added to prevent numerical instability</li> <li>The loss is normalized by the number of positive labels per sample</li> </ul> Source code in <code>gfmrag/losses.py</code> Python<pre><code>class ListCELoss(BaseLoss):\n    \"\"\"Ranking loss for multi-label target lists.\n\n\n    Args:\n        pred (torch.Tensor): Predicted logits tensor of shape (B x N) where B is batch size\n            and N is number of possible labels.\n        target (torch.Tensor): Binary target tensor of shape (B x N) where 1 indicates positive\n            labels and 0 indicates negative labels.\n        *args: Additional positional arguments (unused).\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        torch.Tensor: Scalar tensor containing the mean loss value.\n\n    Notes:\n        - Empty targets (all zeros) are automatically skipped in loss computation\n        - Small epsilon values (1e-5) are added to prevent numerical instability\n        - The loss is normalized by the number of positive labels per sample\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        pass\n\n    def __call__(\n        self, pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Compute the ranking loss\n\n        This loss function first normalizes the predictions using sigmoid and sum, then calculates\n        a negative log likelihood loss weighted by the target values. Empty targets are skipped.\n\n        Args:\n            pred (torch.Tensor): Prediction tensor of shape (B x N) containing logits\n            target (torch.Tensor): Target tensor of shape (B x N) containing ground truth values\n            *args: Variable length argument list\n            **kwargs: Arbitrary keyword arguments\n\n        Returns:\n            torch.Tensor: Scalar loss value averaged over non-empty batch elements\n\n        Note:\n            - B represents batch size\n            - N represents the number of elements per sample\n            - A small epsilon (1e-5) is added for numerical stability\n        \"\"\"\n        target_sum = target.sum(dim=-1)\n        non_zero_target_mask = target_sum != 0  # Skip empty target\n        target_sum = target_sum[non_zero_target_mask]\n        pred = pred[non_zero_target_mask]\n        target = target[non_zero_target_mask]\n        pred_prob = torch.sigmoid(pred)  # B x N\n        pred_prob_sum = pred_prob.sum(dim=-1, keepdim=True)  # B x 1\n        loss = -torch.log((pred_prob / (pred_prob_sum + 1e-5)) + 1e-5) * target\n        loss = loss.sum(dim=-1) / target_sum\n        loss = loss.mean()\n        return loss\n</code></pre>"},{"location":"api/losses/#gfmrag.losses.ListCELoss.__call__","title":"<code>__call__(pred, target, *args, **kwargs)</code>","text":"<p>Compute the ranking loss</p> <p>This loss function first normalizes the predictions using sigmoid and sum, then calculates a negative log likelihood loss weighted by the target values. Empty targets are skipped.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Prediction tensor of shape (B x N) containing logits</p> required <code>target</code> <code>Tensor</code> <p>Target tensor of shape (B x N) containing ground truth values</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>torch.Tensor: Scalar loss value averaged over non-empty batch elements</p> Note <ul> <li>B represents batch size</li> <li>N represents the number of elements per sample</li> <li>A small epsilon (1e-5) is added for numerical stability</li> </ul> Source code in <code>gfmrag/losses.py</code> Python<pre><code>def __call__(\n    self, pred: torch.Tensor, target: torch.Tensor, *args: Any, **kwargs: Any\n) -&gt; Any:\n    \"\"\"Compute the ranking loss\n\n    This loss function first normalizes the predictions using sigmoid and sum, then calculates\n    a negative log likelihood loss weighted by the target values. Empty targets are skipped.\n\n    Args:\n        pred (torch.Tensor): Prediction tensor of shape (B x N) containing logits\n        target (torch.Tensor): Target tensor of shape (B x N) containing ground truth values\n        *args: Variable length argument list\n        **kwargs: Arbitrary keyword arguments\n\n    Returns:\n        torch.Tensor: Scalar loss value averaged over non-empty batch elements\n\n    Note:\n        - B represents batch size\n        - N represents the number of elements per sample\n        - A small epsilon (1e-5) is added for numerical stability\n    \"\"\"\n    target_sum = target.sum(dim=-1)\n    non_zero_target_mask = target_sum != 0  # Skip empty target\n    target_sum = target_sum[non_zero_target_mask]\n    pred = pred[non_zero_target_mask]\n    target = target[non_zero_target_mask]\n    pred_prob = torch.sigmoid(pred)  # B x N\n    pred_prob_sum = pred_prob.sum(dim=-1, keepdim=True)  # B x 1\n    loss = -torch.log((pred_prob / (pred_prob_sum + 1e-5)) + 1e-5) * target\n    loss = loss.sum(dim=-1) / target_sum\n    loss = loss.mean()\n    return loss\n</code></pre>"},{"location":"api/models/","title":"GNN Models","text":""},{"location":"api/models/#gfmrag.models","title":"<code>gfmrag.models</code>","text":""},{"location":"api/models/#gfmrag.models.GNNRetriever","title":"<code>GNNRetriever</code>","text":"<p>               Bases: <code>QueryGNN</code></p> <p>A Query-dependent Graph Neural Network-based retriever that processes questions and entities for information retrieval.</p> <p>This class extends QueryGNN to implement a GNN-based retrieval system that processes question embeddings and entity information to retrieve relevant information from a graph.</p> <p>Attributes:</p> Name Type Description <code>question_mlp</code> <code>Linear</code> <p>Linear layer for transforming question embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>entity_model</code> <code>QueryNBFNet</code> <p>The underlying query-dependent GNN for reasoning on graph.</p> required <code>rel_emb_dim</code> <code>int</code> <p>Dimension of relation embeddings.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Methods:</p> Name Description <code>forward</code> <p>Processes the input graph and question embeddings to generate retrieval scores.</p> <p>Args:     graph (Data): The input graph structure.     batch (dict[str, torch.Tensor]): Batch of input data containing question embeddings and masks.     entities_weight (torch.Tensor, optional): Optional weights for entities.</p> <p>Returns:     torch.Tensor: Output scores for retrieval.</p> <code>visualize</code> <p>Generates visualization data for the model's reasoning process.</p> <p>Args:     graph (Data): The input graph structure.     sample (dict[str, torch.Tensor]): Single sample data containing question embeddings and masks.     entities_weight (torch.Tensor, optional): Optional weights for entities.</p> <p>Returns:     dict[int, torch.Tensor]: Visualization data for each reasoning step.</p> Note <p>The visualization method currently only supports batch size of 1.</p> Source code in <code>gfmrag/models.py</code> Python<pre><code>class GNNRetriever(QueryGNN):\n    \"\"\"A Query-dependent Graph Neural Network-based retriever that processes questions and entities for information retrieval.\n\n    This class extends QueryGNN to implement a GNN-based retrieval system that processes question\n    embeddings and entity information to retrieve relevant information from a graph.\n\n    Attributes:\n        question_mlp (nn.Linear): Linear layer for transforming question embeddings.\n\n    Args:\n        entity_model (QueryNBFNet): The underlying query-dependent GNN for reasoning on graph.\n        rel_emb_dim (int): Dimension of relation embeddings.\n        *args (Any): Variable length argument list.\n        **kwargs (Any): Arbitrary keyword arguments.\n\n    Methods:\n        forward(graph, batch, entities_weight=None):\n            Processes the input graph and question embeddings to generate retrieval scores.\n\n            Args:\n                graph (Data): The input graph structure.\n                batch (dict[str, torch.Tensor]): Batch of input data containing question embeddings and masks.\n                entities_weight (torch.Tensor, optional): Optional weights for entities.\n\n            Returns:\n                torch.Tensor: Output scores for retrieval.\n\n        visualize(graph, sample, entities_weight=None):\n            Generates visualization data for the model's reasoning process.\n\n            Args:\n                graph (Data): The input graph structure.\n                sample (dict[str, torch.Tensor]): Single sample data containing question embeddings and masks.\n                entities_weight (torch.Tensor, optional): Optional weights for entities.\n\n            Returns:\n                dict[int, torch.Tensor]: Visualization data for each reasoning step.\n\n    Note:\n        The visualization method currently only supports batch size of 1.\n    \"\"\"\n\n    \"\"\"Wrap the GNN model for retrieval.\"\"\"\n\n    def __init__(\n        self, entity_model: QueryNBFNet, rel_emb_dim: int, *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Initialize the RelGFM model.\n\n        Args:\n            entity_model (QueryNBFNet): Model for entity embedding and message passing\n            rel_emb_dim (int): Dimension of relation embeddings\n            *args: Variable length argument list\n            **kwargs: Arbitrary keyword arguments\n\n        Returns:\n            None\n\n        Note:\n            This constructor initializes the base class with entity_model and rel_emb_dim,\n            and creates a linear layer to project question embeddings to entity dimension.\n        \"\"\"\n\n        super().__init__(entity_model, rel_emb_dim)\n        self.question_mlp = nn.Linear(self.rel_emb_dim, self.entity_model.dims[0])\n\n    def forward(\n        self,\n        graph: Data,\n        batch: dict[str, torch.Tensor],\n        entities_weight: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the model.\n\n        This method processes a graph and question embeddings to produce entity-level reasoning output.\n\n        Args:\n            graph (Data): A PyTorch Geometric Data object containing the graph structure and features.\n            batch (dict[str, torch.Tensor]): A dictionary containing:\n                - question_embeddings: Tensor of question embeddings\n                - question_entities_masks: Tensor of masks for question entities\n            entities_weight (torch.Tensor | None, optional): Optional weight tensor for entities. Defaults to None.\n\n        Returns:\n            torch.Tensor: The output tensor representing entity-level reasoning results.\n\n        Notes:\n            The forward pass includes:\n            1. Processing question embeddings through MLP\n            2. Expanding relation representations\n            3. Applying optional entity weights\n            4. Computing entity-question interaction\n            5. Running entity-level reasoning model\n        \"\"\"\n\n        question_emb = batch[\"question_embeddings\"]\n        question_entities_mask = batch[\"question_entities_masks\"]\n\n        question_embedding = self.question_mlp(question_emb)  # shape: (bs, emb_dim)\n        batch_size = question_embedding.size(0)\n        relation_representations = (\n            self.rel_mlp(graph.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n        )\n\n        # initialize the input with the fuzzy set and question embs\n        if entities_weight is not None:\n            question_entities_mask = question_entities_mask * entities_weight.unsqueeze(\n                0\n            )\n\n        input = torch.einsum(\n            \"bn, bd -&gt; bnd\", question_entities_mask, question_embedding\n        )\n\n        # GNN model: run the entity-level reasoner to get a scalar distribution over nodes\n        output = self.entity_model(\n            graph, input, relation_representations, question_embedding\n        )\n\n        return output\n\n    def visualize(\n        self,\n        graph: Data,\n        sample: dict[str, torch.Tensor],\n        entities_weight: torch.Tensor | None = None,\n    ) -&gt; dict[int, torch.Tensor]:\n        \"\"\"Visualizes attention weights and intermediate states for the model.\n\n        This function generates visualization data for understanding how the model processes\n        inputs and generates entity predictions. It is designed for debugging and analysis purposes.\n\n        Args:\n            graph (Data): The input knowledge graph structure containing entity and relation information\n            sample (dict[str, torch.Tensor]): Dictionary containing:\n                - question_embeddings: Tensor of question text embeddings\n                - question_entities_masks: Binary mask tensor indicating question entities\n            entities_weight (torch.Tensor | None, optional): Optional tensor of entity weights to apply.\n                Defaults to None.\n\n        Returns:\n            dict[int, torch.Tensor]: Dictionary mapping layer indices to attention weight tensors,\n                allowing visualization of attention patterns at different model depths.\n\n        Note:\n            Currently only supports batch size of 1 for visualization purposes.\n\n        Raises:\n            AssertionError: If batch size is not 1\n        \"\"\"\n\n        question_emb = sample[\"question_embeddings\"]\n        question_entities_mask = sample[\"question_entities_masks\"]\n        question_embedding = self.question_mlp(question_emb)  # shape: (bs, emb_dim)\n        batch_size = question_embedding.size(0)\n\n        assert batch_size == 1, \"Currently only supports batch size 1 for visualization\"\n\n        relation_representations = (\n            self.rel_mlp(graph.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n        )\n\n        # initialize the input with the fuzzy set and question embs\n        if entities_weight is not None:\n            question_entities_mask = question_entities_mask * entities_weight.unsqueeze(\n                0\n            )\n\n        input = torch.einsum(\n            \"bn, bd -&gt; bnd\", question_entities_mask, question_embedding\n        )\n        return self.entity_model.visualize(\n            graph,\n            sample,\n            input,\n            relation_representations,\n            question_embedding,  # type: ignore\n        )\n</code></pre>"},{"location":"api/models/#gfmrag.models.GNNRetriever.__init__","title":"<code>__init__(entity_model, rel_emb_dim, *args, **kwargs)</code>","text":"<p>Initialize the RelGFM model.</p> <p>Parameters:</p> Name Type Description Default <code>entity_model</code> <code>QueryNBFNet</code> <p>Model for entity embedding and message passing</p> required <code>rel_emb_dim</code> <code>int</code> <p>Dimension of relation embeddings</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Note <p>This constructor initializes the base class with entity_model and rel_emb_dim, and creates a linear layer to project question embeddings to entity dimension.</p> Source code in <code>gfmrag/models.py</code> Python<pre><code>def __init__(\n    self, entity_model: QueryNBFNet, rel_emb_dim: int, *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize the RelGFM model.\n\n    Args:\n        entity_model (QueryNBFNet): Model for entity embedding and message passing\n        rel_emb_dim (int): Dimension of relation embeddings\n        *args: Variable length argument list\n        **kwargs: Arbitrary keyword arguments\n\n    Returns:\n        None\n\n    Note:\n        This constructor initializes the base class with entity_model and rel_emb_dim,\n        and creates a linear layer to project question embeddings to entity dimension.\n    \"\"\"\n\n    super().__init__(entity_model, rel_emb_dim)\n    self.question_mlp = nn.Linear(self.rel_emb_dim, self.entity_model.dims[0])\n</code></pre>"},{"location":"api/models/#gfmrag.models.GNNRetriever.forward","title":"<code>forward(graph, batch, entities_weight=None)</code>","text":"<p>Forward pass of the model.</p> <p>This method processes a graph and question embeddings to produce entity-level reasoning output.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Data</code> <p>A PyTorch Geometric Data object containing the graph structure and features.</p> required <code>batch</code> <code>dict[str, Tensor]</code> <p>A dictionary containing: - question_embeddings: Tensor of question embeddings - question_entities_masks: Tensor of masks for question entities</p> required <code>entities_weight</code> <code>Tensor | None</code> <p>Optional weight tensor for entities. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor representing entity-level reasoning results.</p> Notes <p>The forward pass includes: 1. Processing question embeddings through MLP 2. Expanding relation representations 3. Applying optional entity weights 4. Computing entity-question interaction 5. Running entity-level reasoning model</p> Source code in <code>gfmrag/models.py</code> Python<pre><code>def forward(\n    self,\n    graph: Data,\n    batch: dict[str, torch.Tensor],\n    entities_weight: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the model.\n\n    This method processes a graph and question embeddings to produce entity-level reasoning output.\n\n    Args:\n        graph (Data): A PyTorch Geometric Data object containing the graph structure and features.\n        batch (dict[str, torch.Tensor]): A dictionary containing:\n            - question_embeddings: Tensor of question embeddings\n            - question_entities_masks: Tensor of masks for question entities\n        entities_weight (torch.Tensor | None, optional): Optional weight tensor for entities. Defaults to None.\n\n    Returns:\n        torch.Tensor: The output tensor representing entity-level reasoning results.\n\n    Notes:\n        The forward pass includes:\n        1. Processing question embeddings through MLP\n        2. Expanding relation representations\n        3. Applying optional entity weights\n        4. Computing entity-question interaction\n        5. Running entity-level reasoning model\n    \"\"\"\n\n    question_emb = batch[\"question_embeddings\"]\n    question_entities_mask = batch[\"question_entities_masks\"]\n\n    question_embedding = self.question_mlp(question_emb)  # shape: (bs, emb_dim)\n    batch_size = question_embedding.size(0)\n    relation_representations = (\n        self.rel_mlp(graph.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n    )\n\n    # initialize the input with the fuzzy set and question embs\n    if entities_weight is not None:\n        question_entities_mask = question_entities_mask * entities_weight.unsqueeze(\n            0\n        )\n\n    input = torch.einsum(\n        \"bn, bd -&gt; bnd\", question_entities_mask, question_embedding\n    )\n\n    # GNN model: run the entity-level reasoner to get a scalar distribution over nodes\n    output = self.entity_model(\n        graph, input, relation_representations, question_embedding\n    )\n\n    return output\n</code></pre>"},{"location":"api/models/#gfmrag.models.GNNRetriever.visualize","title":"<code>visualize(graph, sample, entities_weight=None)</code>","text":"<p>Visualizes attention weights and intermediate states for the model.</p> <p>This function generates visualization data for understanding how the model processes inputs and generates entity predictions. It is designed for debugging and analysis purposes.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Data</code> <p>The input knowledge graph structure containing entity and relation information</p> required <code>sample</code> <code>dict[str, Tensor]</code> <p>Dictionary containing: - question_embeddings: Tensor of question text embeddings - question_entities_masks: Binary mask tensor indicating question entities</p> required <code>entities_weight</code> <code>Tensor | None</code> <p>Optional tensor of entity weights to apply. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, Tensor]</code> <p>dict[int, torch.Tensor]: Dictionary mapping layer indices to attention weight tensors, allowing visualization of attention patterns at different model depths.</p> Note <p>Currently only supports batch size of 1 for visualization purposes.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If batch size is not 1</p> Source code in <code>gfmrag/models.py</code> Python<pre><code>def visualize(\n    self,\n    graph: Data,\n    sample: dict[str, torch.Tensor],\n    entities_weight: torch.Tensor | None = None,\n) -&gt; dict[int, torch.Tensor]:\n    \"\"\"Visualizes attention weights and intermediate states for the model.\n\n    This function generates visualization data for understanding how the model processes\n    inputs and generates entity predictions. It is designed for debugging and analysis purposes.\n\n    Args:\n        graph (Data): The input knowledge graph structure containing entity and relation information\n        sample (dict[str, torch.Tensor]): Dictionary containing:\n            - question_embeddings: Tensor of question text embeddings\n            - question_entities_masks: Binary mask tensor indicating question entities\n        entities_weight (torch.Tensor | None, optional): Optional tensor of entity weights to apply.\n            Defaults to None.\n\n    Returns:\n        dict[int, torch.Tensor]: Dictionary mapping layer indices to attention weight tensors,\n            allowing visualization of attention patterns at different model depths.\n\n    Note:\n        Currently only supports batch size of 1 for visualization purposes.\n\n    Raises:\n        AssertionError: If batch size is not 1\n    \"\"\"\n\n    question_emb = sample[\"question_embeddings\"]\n    question_entities_mask = sample[\"question_entities_masks\"]\n    question_embedding = self.question_mlp(question_emb)  # shape: (bs, emb_dim)\n    batch_size = question_embedding.size(0)\n\n    assert batch_size == 1, \"Currently only supports batch size 1 for visualization\"\n\n    relation_representations = (\n        self.rel_mlp(graph.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n    )\n\n    # initialize the input with the fuzzy set and question embs\n    if entities_weight is not None:\n        question_entities_mask = question_entities_mask * entities_weight.unsqueeze(\n            0\n        )\n\n    input = torch.einsum(\n        \"bn, bd -&gt; bnd\", question_entities_mask, question_embedding\n    )\n    return self.entity_model.visualize(\n        graph,\n        sample,\n        input,\n        relation_representations,\n        question_embedding,  # type: ignore\n    )\n</code></pre>"},{"location":"api/models/#gfmrag.models.QueryGNN","title":"<code>QueryGNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural network module for query embedding in graph neural networks.</p> <p>This class implements a query embedding model that combines relation embeddings with an entity-based graph neural network for knowledge graph completion tasks.</p> <p>Parameters:</p> Name Type Description Default <code>entity_model</code> <code>EntityNBFNet</code> <p>The entity-based neural network model for reasoning on graph structure.</p> required <code>rel_emb_dim</code> <code>int</code> <p>Dimension of the relation embeddings.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>rel_emb_dim</code> <code>int</code> <p>Dimension of relation embeddings.</p> <code>entity_model</code> <code>EntityNBFNet</code> <p>The entity model instance.</p> <code>rel_mlp</code> <code>Linear</code> <p>Linear transformation layer for relation embeddings.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Data, batch: torch.Tensor) -&gt; torch.Tensor: Forward pass of the query GNN model.</p> <p>Args:     data (Data): Graph data object containing the knowledge graph structure and features.     batch (torch.Tensor): Batch of triples with shape (batch_size, 1+num_negatives, 3),                         where each triple contains (head, tail, relation) indices.</p> <p>Returns:     torch.Tensor: Scoring tensor for the input triples.</p> Source code in <code>gfmrag/models.py</code> Python<pre><code>class QueryGNN(nn.Module):\n    \"\"\"A neural network module for query embedding in graph neural networks.\n\n    This class implements a query embedding model that combines relation embeddings with an entity-based graph neural network\n    for knowledge graph completion tasks.\n\n    Args:\n        entity_model (EntityNBFNet): The entity-based neural network model for reasoning on graph structure.\n        rel_emb_dim (int): Dimension of the relation embeddings.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Attributes:\n        rel_emb_dim (int): Dimension of relation embeddings.\n        entity_model (EntityNBFNet): The entity model instance.\n        rel_mlp (nn.Linear): Linear transformation layer for relation embeddings.\n\n    Methods:\n        forward(data: Data, batch: torch.Tensor) -&gt; torch.Tensor:\n            Forward pass of the query GNN model.\n\n            Args:\n                data (Data): Graph data object containing the knowledge graph structure and features.\n                batch (torch.Tensor): Batch of triples with shape (batch_size, 1+num_negatives, 3),\n                                    where each triple contains (head, tail, relation) indices.\n\n            Returns:\n                torch.Tensor: Scoring tensor for the input triples.\n    \"\"\"\n\n    def __init__(\n        self, entity_model: EntityNBFNet, rel_emb_dim: int, *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Initialize the model.\n\n        Args:\n            entity_model (EntityNBFNet): The entity model component\n            rel_emb_dim (int): Dimension of relation embeddings\n            *args (Any): Variable length argument list\n            **kwargs (Any): Arbitrary keyword arguments\n\n        \"\"\"\n\n        super().__init__()\n        self.rel_emb_dim = rel_emb_dim\n        self.entity_model = entity_model\n        self.rel_mlp = nn.Linear(rel_emb_dim, self.entity_model.dims[0])\n\n    def forward(self, data: Data, batch: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            data (Data): Graph data object containing entity embeddings and graph structure.\n            batch (torch.Tensor): Batch of triple indices with shape (batch_size, 1+num_negatives, 3),\n                                where each triple contains (head_idx, tail_idx, relation_idx).\n\n        Returns:\n            torch.Tensor: Scores for the triples in the batch.\n\n        Notes:\n            - Relations are assumed to be the same across all positive and negative triples\n            - Easy edges are removed before processing to encourage learning of non-trivial paths\n            - The batch tensor contains both positive and negative samples where the first sample\n              is positive and the rest are negative samples\n        \"\"\"\n        # batch shape: (bs, 1+num_negs, 3)\n        # relations are the same all positive and negative triples, so we can extract only one from the first triple among 1+nug_negs\n        batch_size = len(batch)\n        relation_representations = (\n            self.rel_mlp(data.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n        )\n        h_index, t_index, r_index = batch.unbind(-1)\n        # to make NBFNet iteration learn non-trivial paths\n        data = self.entity_model.remove_easy_edges(data, h_index, t_index, r_index)\n        score = self.entity_model(data, relation_representations, batch)\n\n        return score\n</code></pre>"},{"location":"api/models/#gfmrag.models.QueryGNN.__init__","title":"<code>__init__(entity_model, rel_emb_dim, *args, **kwargs)</code>","text":"<p>Initialize the model.</p> <p>Parameters:</p> Name Type Description Default <code>entity_model</code> <code>EntityNBFNet</code> <p>The entity model component</p> required <code>rel_emb_dim</code> <code>int</code> <p>Dimension of relation embeddings</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments</p> <code>{}</code> Source code in <code>gfmrag/models.py</code> Python<pre><code>def __init__(\n    self, entity_model: EntityNBFNet, rel_emb_dim: int, *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"Initialize the model.\n\n    Args:\n        entity_model (EntityNBFNet): The entity model component\n        rel_emb_dim (int): Dimension of relation embeddings\n        *args (Any): Variable length argument list\n        **kwargs (Any): Arbitrary keyword arguments\n\n    \"\"\"\n\n    super().__init__()\n    self.rel_emb_dim = rel_emb_dim\n    self.entity_model = entity_model\n    self.rel_mlp = nn.Linear(rel_emb_dim, self.entity_model.dims[0])\n</code></pre>"},{"location":"api/models/#gfmrag.models.QueryGNN.forward","title":"<code>forward(data, batch)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Graph data object containing entity embeddings and graph structure.</p> required <code>batch</code> <code>Tensor</code> <p>Batch of triple indices with shape (batch_size, 1+num_negatives, 3),                 where each triple contains (head_idx, tail_idx, relation_idx).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Scores for the triples in the batch.</p> Notes <ul> <li>Relations are assumed to be the same across all positive and negative triples</li> <li>Easy edges are removed before processing to encourage learning of non-trivial paths</li> <li>The batch tensor contains both positive and negative samples where the first sample   is positive and the rest are negative samples</li> </ul> Source code in <code>gfmrag/models.py</code> Python<pre><code>def forward(self, data: Data, batch: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        data (Data): Graph data object containing entity embeddings and graph structure.\n        batch (torch.Tensor): Batch of triple indices with shape (batch_size, 1+num_negatives, 3),\n                            where each triple contains (head_idx, tail_idx, relation_idx).\n\n    Returns:\n        torch.Tensor: Scores for the triples in the batch.\n\n    Notes:\n        - Relations are assumed to be the same across all positive and negative triples\n        - Easy edges are removed before processing to encourage learning of non-trivial paths\n        - The batch tensor contains both positive and negative samples where the first sample\n          is positive and the rest are negative samples\n    \"\"\"\n    # batch shape: (bs, 1+num_negs, 3)\n    # relations are the same all positive and negative triples, so we can extract only one from the first triple among 1+nug_negs\n    batch_size = len(batch)\n    relation_representations = (\n        self.rel_mlp(data.rel_emb).unsqueeze(0).expand(batch_size, -1, -1)\n    )\n    h_index, t_index, r_index = batch.unbind(-1)\n    # to make NBFNet iteration learn non-trivial paths\n    data = self.entity_model.remove_easy_edges(data, h_index, t_index, r_index)\n    score = self.entity_model(data, relation_representations, batch)\n\n    return score\n</code></pre>"},{"location":"api/models/#gfmrag.ultra.models.EntityNBFNet","title":"<code>gfmrag.ultra.models.EntityNBFNet</code>","text":"<p>               Bases: <code>BaseNBFNet</code></p> <p>Neural Bellman-Ford Network for Entity Prediction.</p> <p>This class extends BaseNBFNet to perform entity prediction in knowledge graphs using a neural version of the Bellman-Ford algorithm. It learns entity representations through message passing over the graph structure.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of input node/relation features</p> required <code>hidden_dims</code> <code>list</code> <p>List of hidden dimensions for each layer</p> required <code>num_relation</code> <code>int</code> <p>Number of relation types. Defaults to 1 (dummy value)</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments passed to BaseNBFNet</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>ModuleList</code> <p>List of GeneralizedRelationalConv layers</p> <code>mlp</code> <code>Sequential</code> <p>Multi-layer perceptron for final prediction</p> <code>query</code> <code>Tensor</code> <p>Relation type embeddings used as queries</p> <p>Methods:</p> Name Description <code>bellmanford</code> <p>Performs neural Bellman-Ford message passing iterations.</p> <p>Args:     data: Graph data object containing edge information     h_index (torch.Tensor): Indices of head entities     r_index (torch.Tensor): Indices of relations     separate_grad (bool): Whether to use separate gradients for visualization</p> <p>Returns:     dict: Contains node features and edge weights after message passing</p> <code>forward</code> <p>Forward pass for entity prediction.</p> <p>Args:     data: Graph data object     relation_representations (torch.Tensor): Embeddings of relations     batch: Batch of (head, tail, relation) triples</p> <p>Returns:     torch.Tensor: Prediction scores for tail entities</p> Source code in <code>gfmrag/ultra/models.py</code> Python<pre><code>class EntityNBFNet(BaseNBFNet):\n    \"\"\"Neural Bellman-Ford Network for Entity Prediction.\n\n    This class extends BaseNBFNet to perform entity prediction in knowledge graphs using a neural\n    version of the Bellman-Ford algorithm. It learns entity representations through message passing\n    over the graph structure.\n\n    Args:\n        input_dim (int): Dimension of input node/relation features\n        hidden_dims (list): List of hidden dimensions for each layer\n        num_relation (int, optional): Number of relation types. Defaults to 1 (dummy value)\n        **kwargs: Additional arguments passed to BaseNBFNet\n\n    Attributes:\n        layers (nn.ModuleList): List of GeneralizedRelationalConv layers\n        mlp (nn.Sequential): Multi-layer perceptron for final prediction\n        query (torch.Tensor): Relation type embeddings used as queries\n\n    Methods:\n        bellmanford(data, h_index, r_index, separate_grad=False):\n            Performs neural Bellman-Ford message passing iterations.\n\n            Args:\n                data: Graph data object containing edge information\n                h_index (torch.Tensor): Indices of head entities\n                r_index (torch.Tensor): Indices of relations\n                separate_grad (bool): Whether to use separate gradients for visualization\n\n            Returns:\n                dict: Contains node features and edge weights after message passing\n\n        forward(data, relation_representations, batch):\n            Forward pass for entity prediction.\n\n            Args:\n                data: Graph data object\n                relation_representations (torch.Tensor): Embeddings of relations\n                batch: Batch of (head, tail, relation) triples\n\n            Returns:\n                torch.Tensor: Prediction scores for tail entities\n    \"\"\"\n    def __init__(self, input_dim, hidden_dims, num_relation=1, **kwargs):\n        # dummy num_relation = 1 as we won't use it in the NBFNet layer\n        super().__init__(input_dim, hidden_dims, num_relation, **kwargs)\n\n        self.layers = nn.ModuleList()\n        for i in range(len(self.dims) - 1):\n            self.layers.append(\n                layers.GeneralizedRelationalConv(\n                    self.dims[i],\n                    self.dims[i + 1],\n                    num_relation,\n                    self.dims[0],\n                    self.message_func,\n                    self.aggregate_func,\n                    self.layer_norm,\n                    self.activation,\n                    dependent=False,\n                    project_relations=True,\n                )\n            )\n\n        feature_dim = (\n            sum(hidden_dims) if self.concat_hidden else hidden_dims[-1]\n        ) + input_dim\n        self.mlp = nn.Sequential()\n        mlp = []\n        for i in range(self.num_mlp_layers - 1):\n            mlp.append(nn.Linear(feature_dim, feature_dim))\n            mlp.append(nn.ReLU())\n        mlp.append(nn.Linear(feature_dim, 1))\n        self.mlp = nn.Sequential(*mlp)\n\n    def bellmanford(self, data, h_index, r_index, separate_grad=False):\n        batch_size = len(r_index)\n\n        # initialize queries (relation types of the given triples)\n        query = self.query[torch.arange(batch_size, device=r_index.device), r_index]\n        index = h_index.unsqueeze(-1).expand_as(query)\n\n        # initial (boundary) condition - initialize all node states as zeros\n        boundary = torch.zeros(\n            batch_size, data.num_nodes, self.dims[0], device=h_index.device\n        )\n        # by the scatter operation we put query (relation) embeddings as init features of source (index) nodes\n        boundary.scatter_add_(1, index.unsqueeze(1), query.unsqueeze(1))\n\n        size = (data.num_nodes, data.num_nodes)\n        edge_weight = torch.ones(data.num_edges, device=h_index.device)\n\n        hiddens = []\n        edge_weights = []\n        layer_input = boundary\n\n        for layer in self.layers:\n            # for visualization\n            if separate_grad:\n                edge_weight = edge_weight.clone().requires_grad_()\n\n            # Bellman-Ford iteration, we send the original boundary condition in addition to the updated node states\n            hidden = layer(\n                layer_input,\n                query,\n                boundary,\n                data.edge_index,\n                data.edge_type,\n                size,\n                edge_weight,\n            )\n            if self.short_cut and hidden.shape == layer_input.shape:\n                # residual connection here\n                hidden = hidden + layer_input\n            hiddens.append(hidden)\n            edge_weights.append(edge_weight)\n            layer_input = hidden\n\n        # original query (relation type) embeddings\n        node_query = query.unsqueeze(1).expand(\n            -1, data.num_nodes, -1\n        )  # (batch_size, num_nodes, input_dim)\n        if self.concat_hidden:\n            output = torch.cat(hiddens + [node_query], dim=-1)\n        else:\n            output = torch.cat([hiddens[-1], node_query], dim=-1)\n\n        return {\n            \"node_feature\": output,\n            \"edge_weights\": edge_weights,\n        }\n\n    def forward(self, data, relation_representations, batch):\n        h_index, t_index, r_index = batch.unbind(-1)\n\n        # initial query representations are those from the relation graph\n        self.query = relation_representations\n\n        # initialize relations in each NBFNet layer (with uinque projection internally)\n        for layer in self.layers:\n            layer.relation = relation_representations\n\n        # if self.training:\n            # Edge dropout in the training mode\n            # here we want to remove immediate edges (head, relation, tail) from the edge_index and edge_types\n            # to make NBFNet iteration learn non-trivial paths\n            # data = self.remove_easy_edges(data, h_index, t_index, r_index)\n\n        shape = h_index.shape\n        # turn all triples in a batch into a tail prediction mode\n        h_index, t_index, r_index = self.negative_sample_to_tail(\n            h_index, t_index, r_index, num_direct_rel=data.num_relations // 2\n        )\n        assert (h_index[:, [0]] == h_index).all()\n        assert (r_index[:, [0]] == r_index).all()\n\n        # message passing and updated node representations\n        output = self.bellmanford(\n            data, h_index[:, 0], r_index[:, 0]\n        )  # (num_nodes, batch_size, feature_dim\uff09\n        feature = output[\"node_feature\"]\n        index = t_index.unsqueeze(-1).expand(-1, -1, feature.shape[-1])\n        # extract representations of tail entities from the updated node states\n        feature = feature.gather(\n            1, index\n        )  # (batch_size, num_negative + 1, feature_dim)\n\n        # probability logit for each tail node in the batch\n        # (batch_size, num_negative + 1, dim) -&gt; (batch_size, num_negative + 1)\n        score = self.mlp(feature).squeeze(-1)\n        return score.view(shape)\n</code></pre>"},{"location":"api/models/#gfmrag.ultra.models.QueryNBFNet","title":"<code>gfmrag.ultra.models.QueryNBFNet</code>","text":"<p>               Bases: <code>EntityNBFNet</code></p> <p>The entity-level reasoner for UltraQuery-like complex query answering pipelines.</p> <p>This class extends EntityNBFNet to handle query-specific reasoning in knowledge graphs. Key differences from EntityNBFNet include:</p> <ol> <li>Initial node features are provided during forward pass rather than read from triples batch</li> <li>Query comes from outer loop</li> <li>Returns distribution over all nodes (assuming t_index covers all nodes)</li> </ol> <p>Attributes:</p> Name Type Description <code>layers</code> <p>List of neural network layers for message passing</p> <code>short_cut</code> <p>Boolean flag for using residual connections</p> <code>concat_hidden</code> <p>Boolean flag for concatenating hidden states</p> <code>mlp</code> <p>Multi-layer perceptron for final scoring</p> <code>num_beam</code> <p>Beam size for path search</p> <code>path_topk</code> <p>Number of top paths to return</p> <p>Methods:</p> Name Description <code>bellmanford</code> <p>Performs Bellman-Ford message passing iterations. Args:     data: Graph data object containing edge information     node_features: Initial node representations     query: Query representation     separate_grad: Whether to track gradients separately for edges Returns:     dict: Contains node features and edge weights</p> <code>forward</code> <p>Main forward pass of the model. Args:     data: Graph data object     node_features: Initial node features     relation_representations: Representations for relations     query: Query representation Returns:     torch.Tensor: Scores for each node</p> <code>visualize</code> <p>Visualizes reasoning paths for given entities. Args:     data: Graph data object     sample: Dictionary containing entity masks     node_features: Initial node features     relation_representations: Representations for relations     query: Query representation Returns:     dict: Contains paths and weights for target entities</p> Source code in <code>gfmrag/ultra/models.py</code> Python<pre><code>class QueryNBFNet(EntityNBFNet):\n    \"\"\"\n    The entity-level reasoner for UltraQuery-like complex query answering pipelines.\n\n    This class extends EntityNBFNet to handle query-specific reasoning in knowledge graphs.\n    Key differences from EntityNBFNet include:\n\n    1. Initial node features are provided during forward pass rather than read from triples batch\n    2. Query comes from outer loop\n    3. Returns distribution over all nodes (assuming t_index covers all nodes)\n\n    Attributes:\n        layers: List of neural network layers for message passing\n        short_cut: Boolean flag for using residual connections\n        concat_hidden: Boolean flag for concatenating hidden states\n        mlp: Multi-layer perceptron for final scoring\n        num_beam: Beam size for path search\n        path_topk: Number of top paths to return\n\n    Methods:\n        bellmanford(data, node_features, query, separate_grad=False):\n            Performs Bellman-Ford message passing iterations.\n            Args:\n                data: Graph data object containing edge information\n                node_features: Initial node representations\n                query: Query representation\n                separate_grad: Whether to track gradients separately for edges\n            Returns:\n                dict: Contains node features and edge weights\n\n        forward(data, node_features, relation_representations, query):\n            Main forward pass of the model.\n            Args:\n                data: Graph data object\n                node_features: Initial node features\n                relation_representations: Representations for relations\n                query: Query representation\n            Returns:\n                torch.Tensor: Scores for each node\n\n        visualize(data, sample, node_features, relation_representations, query):\n            Visualizes reasoning paths for given entities.\n            Args:\n                data: Graph data object\n                sample: Dictionary containing entity masks\n                node_features: Initial node features\n                relation_representations: Representations for relations\n                query: Query representation\n            Returns:\n                dict: Contains paths and weights for target entities\n    \"\"\"\n\n    def bellmanford(self, data, node_features, query, separate_grad=False):\n        size = (data.num_nodes, data.num_nodes)\n        edge_weight = torch.ones(data.num_edges, device=query.device)\n\n        hiddens = []\n        edge_weights = []\n        layer_input = node_features\n\n        for layer in self.layers:\n            # for visualization\n            if separate_grad:\n                edge_weight = edge_weight.clone().requires_grad_()\n\n            # Bellman-Ford iteration, we send the original boundary condition in addition to the updated node states\n            hidden = layer(\n                layer_input,\n                query,\n                node_features,\n                data.edge_index,\n                data.edge_type,\n                size,\n                edge_weight,\n            )\n            if self.short_cut and hidden.shape == layer_input.shape:\n                # residual connection here\n                hidden = hidden + layer_input\n            hiddens.append(hidden)\n            edge_weights.append(edge_weight)\n            layer_input = hidden\n\n        # original query (relation type) embeddings\n        node_query = query.unsqueeze(1).expand(\n            -1, data.num_nodes, -1\n        )  # (batch_size, num_nodes, input_dim)\n        if self.concat_hidden:\n            output = torch.cat(hiddens + [node_query], dim=-1)\n        else:\n            output = torch.cat([hiddens[-1], node_query], dim=-1)\n\n        return {\n            \"node_feature\": output,\n            \"edge_weights\": edge_weights,\n        }\n\n    def forward(self, data, node_features, relation_representations, query):\n        # initialize relations in each NBFNet layer (with uinque projection internally)\n        for layer in self.layers:\n            layer.relation = relation_representations\n\n        # we already did traversal_dropout in the outer loop of UltraQuery\n        # if self.training:\n        #     # Edge dropout in the training mode\n        #     # here we want to remove immediate edges (head, relation, tail) from the edge_index and edge_types\n        #     # to make NBFNet iteration learn non-trivial paths\n        #     data = self.remove_easy_edges(data, h_index, t_index, r_index)\n\n        # node features arrive in shape (bs, num_nodes, dim)\n        # NBFNet needs batch size on the first place\n        output = self.bellmanford(\n            data, node_features, query\n        )  # (num_nodes, batch_size, feature_dim\uff09\n        score = self.mlp(output[\"node_feature\"]).squeeze(-1)  # (bs, num_nodes)\n        return score\n\n    def visualize(self, data, sample, node_features, relation_representations, query):\n        for layer in self.layers:\n            layer.relation = relation_representations\n\n        output = self.bellmanford(\n            data, node_features, query, separate_grad=True\n        )  # (num_nodes, batch_size, feature_dim\uff09\n        node_feature = output[\"node_feature\"]\n        edge_weights = output[\"edge_weights\"]\n        question_entities_mask = sample[\"question_entities_masks\"]\n        target_entities_mask = sample[\"supporting_entities_masks\"]\n        query_entities_index = question_entities_mask.nonzero(as_tuple=True)[1]\n        target_entities_index = target_entities_mask.nonzero(as_tuple=True)[1]\n\n        paths_results = {}\n        for t_index in target_entities_index:\n            index = t_index.unsqueeze(0).unsqueeze(0).unsqueeze(-1).expand(-1, -1, node_feature.shape[-1])\n            feature = node_feature.gather(1, index).squeeze(0)\n            score = self.mlp(feature).squeeze(-1)\n\n            edge_grads = autograd.grad(score, edge_weights, retain_graph=True)\n            distances, back_edges = self.beam_search_distance(data, edge_grads, query_entities_index, t_index, self.num_beam)\n            paths, weights = self.topk_average_length(distances, back_edges, t_index, self.path_topk)\n            paths_results[t_index.item()] = (paths, weights)\n        return paths_results\n</code></pre>"},{"location":"api/prompt_builder/","title":"Prompt Builder","text":""},{"location":"api/prompt_builder/#gfmrag.prompt_builder","title":"<code>gfmrag.prompt_builder</code>","text":""},{"location":"api/prompt_builder/#gfmrag.prompt_builder.QAPromptBuilder","title":"<code>QAPromptBuilder</code>","text":"<p>A class for building prompts for question-answering tasks.</p> <p>This class constructs formatted prompts for Q&amp;A systems using a configuration-based approach. It supports system prompts, document contexts, questions, and optional few-shot examples.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_cfg</code> <code>DictConfig</code> <p>Configuration dictionary containing: - system_prompt: The system instruction prompt - doc_prompt: Template for formatting document context - question_prompt: Template for formatting the question - examples: List of few-shot examples (optional)</p> required <p>Methods:</p> Name Description <code>build_input_prompt</code> <p>Builds a formatted prompt list for the Q&amp;A system.</p> <p>Args:     question (str): The input question to be answered     retrieved_docs (list): List of dictionaries containing document information         with 'title' and 'content' keys     thoughts (list, optional): Additional thought process or reasoning steps</p> <p>Returns:     list: A list of dictionaries containing the formatted prompt with roles         and content for each component</p> Source code in <code>gfmrag/prompt_builder.py</code> Python<pre><code>class QAPromptBuilder:\n    \"\"\"A class for building prompts for question-answering tasks.\n\n    This class constructs formatted prompts for Q&amp;A systems using a configuration-based approach.\n    It supports system prompts, document contexts, questions, and optional few-shot examples.\n\n    Args:\n        prompt_cfg (DictConfig): Configuration dictionary containing:\n            - system_prompt: The system instruction prompt\n            - doc_prompt: Template for formatting document context\n            - question_prompt: Template for formatting the question\n            - examples: List of few-shot examples (optional)\n\n    Methods:\n        build_input_prompt(question, retrieved_docs, thoughts=None):\n            Builds a formatted prompt list for the Q&amp;A system.\n\n            Args:\n                question (str): The input question to be answered\n                retrieved_docs (list): List of dictionaries containing document information\n                    with 'title' and 'content' keys\n                thoughts (list, optional): Additional thought process or reasoning steps\n\n            Returns:\n                list: A list of dictionaries containing the formatted prompt with roles\n                    and content for each component\n    \"\"\"\n\n    def __init__(self, prompt_cfg: DictConfig) -&gt; None:\n        self.cfg = prompt_cfg\n        self.system_prompt = self.cfg.system_prompt\n        self.doc_prompt = self.cfg.doc_prompt\n        self.question_prompt = self.cfg.question_prompt\n        self.examples = self.cfg.examples\n\n    def build_input_prompt(\n        self, question: str, retrieved_docs: list, thoughts: list | None = None\n    ) -&gt; list:\n        prompt = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n        ]\n\n        doc_context = \"\\n\".join(\n            [\n                self.doc_prompt.format(title=doc[\"title\"], content=doc[\"content\"])\n                for doc in retrieved_docs\n            ]\n        )\n\n        question = self.question_prompt.format(question=question)\n        if thoughts is not None:\n            question += \" \".join(thoughts)\n\n        if len(self.examples) &gt; 0:\n            for example in self.examples:\n                prompt.extend(\n                    [\n                        {\"role\": \"user\", \"content\": example[\"input\"]},\n                        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n                    ]\n                )\n        prompt.append(\n            {\n                \"role\": \"user\",\n                \"content\": doc_context + \"\\n\" + question,\n            }\n        )\n\n        return prompt\n</code></pre>"},{"location":"api/text_emb_models/","title":"Text Embedding Models","text":""},{"location":"api/text_emb_models/#gfmrag.text_emb_models","title":"<code>gfmrag.text_emb_models</code>","text":""},{"location":"api/text_emb_models/#gfmrag.text_emb_models.BaseTextEmbModel","title":"<code>BaseTextEmbModel</code>","text":"<p>A base class for text embedding models using SentenceTransformer.</p> <p>This class provides functionality to encode text into embeddings using various SentenceTransformer models with configurable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>text_emb_model_name</code> <code>str</code> <p>Name or path of the SentenceTransformer model to use</p> required <code>normalize</code> <code>bool</code> <p>Whether to L2-normalize the embeddings. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Batch size for encoding. Defaults to 32.</p> <code>32</code> <code>query_instruct</code> <code>str | None</code> <p>Instruction/prompt to prepend to queries. Defaults to None.</p> <code>None</code> <code>passage_instruct</code> <code>str | None</code> <p>Instruction/prompt to prepend to passages. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict | None</code> <p>Additional keyword arguments for the model. Defaults to None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>text_emb_model</code> <code>SentenceTransformer</code> <p>The underlying SentenceTransformer model</p> <code>text_emb_model_name</code> <code>str</code> <p>Name of the model being used</p> <code>normalize</code> <code>bool</code> <p>Whether embeddings are L2-normalized</p> <code>batch_size</code> <code>int</code> <p>Batch size used for encoding</p> <code>query_instruct</code> <code>str | None</code> <p>Instruction text for queries</p> <code>passage_instruct</code> <code>str | None</code> <p>Instruction text for passages</p> <code>model_kwargs</code> <code>dict | None</code> <p>Additional model configuration parameters</p> <p>Methods:</p> Name Description <code>encode</code> <p>list[str], is_query: bool = False, show_progress_bar: bool = True) -&gt; torch.Tensor: Encodes a list of texts into embeddings.</p> Source code in <code>gfmrag/text_emb_models/base_model.py</code> Python<pre><code>class BaseTextEmbModel:\n    \"\"\"A base class for text embedding models using SentenceTransformer.\n\n    This class provides functionality to encode text into embeddings using various\n    SentenceTransformer models with configurable parameters.\n\n    Args:\n        text_emb_model_name (str): Name or path of the SentenceTransformer model to use\n        normalize (bool, optional): Whether to L2-normalize the embeddings. Defaults to False.\n        batch_size (int, optional): Batch size for encoding. Defaults to 32.\n        query_instruct (str | None, optional): Instruction/prompt to prepend to queries. Defaults to None.\n        passage_instruct (str | None, optional): Instruction/prompt to prepend to passages. Defaults to None.\n        model_kwargs (dict | None, optional): Additional keyword arguments for the model. Defaults to None.\n\n    Attributes:\n        text_emb_model (SentenceTransformer): The underlying SentenceTransformer model\n        text_emb_model_name (str): Name of the model being used\n        normalize (bool): Whether embeddings are L2-normalized\n        batch_size (int): Batch size used for encoding\n        query_instruct (str | None): Instruction text for queries\n        passage_instruct (str | None): Instruction text for passages\n        model_kwargs (dict | None): Additional model configuration parameters\n\n    Methods:\n        encode(text: list[str], is_query: bool = False, show_progress_bar: bool = True) -&gt; torch.Tensor:\n            Encodes a list of texts into embeddings.\n    \"\"\"\n\n    def __init__(\n        self,\n        text_emb_model_name: str,\n        normalize: bool = False,\n        batch_size: int = 32,\n        query_instruct: str | None = None,\n        passage_instruct: str | None = None,\n        model_kwargs: dict | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the BaseTextEmbModel.\n\n        Args:\n            text_emb_model_name (str): Name or path of the SentenceTransformer model to use\n            normalize (bool, optional): Whether to L2-normalize the embeddings. Defaults to False.\n            batch_size (int, optional): Batch size for encoding. Defaults to 32.\n            query_instruct (str | None, optional): Instruction/prompt to prepend to queries. Defaults to None.\n            passage_instruct (str | None, optional): Instruction/prompt to prepend to passages. Defaults to None.\n            model_kwargs (dict | None, optional): Additional keyword arguments for the model. Defaults to None.\n        \"\"\"\n        self.text_emb_model_name = text_emb_model_name\n        self.normalize = normalize\n        self.batch_size = batch_size\n        self.query_instruct = query_instruct\n        self.passage_instruct = passage_instruct\n        self.model_kwargs = model_kwargs\n\n        self.text_emb_model = SentenceTransformer(\n            self.text_emb_model_name,\n            trust_remote_code=True,\n            model_kwargs=self.model_kwargs,\n        )\n\n    def encode(\n        self, text: list[str], is_query: bool = False, show_progress_bar: bool = True\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Encodes a list of text strings into embeddings using the text embedding model.\n\n        Args:\n            text (list[str]): List of text strings to encode\n            is_query (bool, optional): Whether the text is a query (True) or passage (False).\n                Determines which instruction prompt to use. Defaults to False.\n            show_progress_bar (bool, optional): Whether to display progress bar during encoding.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: Tensor containing the encoded embeddings for the input text\n\n        Examples:\n            &gt;&gt;&gt; text_emb_model = BaseTextEmbModel(\"sentence-transformers/all-mpnet-base-v2\")\n            &gt;&gt;&gt; text = [\"Hello, world!\", \"This is a test.\"]\n            &gt;&gt;&gt; embeddings = text_emb_model.encode(text)\n        \"\"\"\n\n        return self.text_emb_model.encode(\n            text,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            normalize_embeddings=self.normalize,\n            batch_size=self.batch_size,\n            prompt=self.query_instruct if is_query else self.passage_instruct,\n            show_progress_bar=show_progress_bar,\n            convert_to_tensor=True,\n        )\n</code></pre>"},{"location":"api/text_emb_models/#gfmrag.text_emb_models.BaseTextEmbModel.__init__","title":"<code>__init__(text_emb_model_name, normalize=False, batch_size=32, query_instruct=None, passage_instruct=None, model_kwargs=None)</code>","text":"<p>Initialize the BaseTextEmbModel.</p> <p>Parameters:</p> Name Type Description Default <code>text_emb_model_name</code> <code>str</code> <p>Name or path of the SentenceTransformer model to use</p> required <code>normalize</code> <code>bool</code> <p>Whether to L2-normalize the embeddings. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Batch size for encoding. Defaults to 32.</p> <code>32</code> <code>query_instruct</code> <code>str | None</code> <p>Instruction/prompt to prepend to queries. Defaults to None.</p> <code>None</code> <code>passage_instruct</code> <code>str | None</code> <p>Instruction/prompt to prepend to passages. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict | None</code> <p>Additional keyword arguments for the model. Defaults to None.</p> <code>None</code> Source code in <code>gfmrag/text_emb_models/base_model.py</code> Python<pre><code>def __init__(\n    self,\n    text_emb_model_name: str,\n    normalize: bool = False,\n    batch_size: int = 32,\n    query_instruct: str | None = None,\n    passage_instruct: str | None = None,\n    model_kwargs: dict | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the BaseTextEmbModel.\n\n    Args:\n        text_emb_model_name (str): Name or path of the SentenceTransformer model to use\n        normalize (bool, optional): Whether to L2-normalize the embeddings. Defaults to False.\n        batch_size (int, optional): Batch size for encoding. Defaults to 32.\n        query_instruct (str | None, optional): Instruction/prompt to prepend to queries. Defaults to None.\n        passage_instruct (str | None, optional): Instruction/prompt to prepend to passages. Defaults to None.\n        model_kwargs (dict | None, optional): Additional keyword arguments for the model. Defaults to None.\n    \"\"\"\n    self.text_emb_model_name = text_emb_model_name\n    self.normalize = normalize\n    self.batch_size = batch_size\n    self.query_instruct = query_instruct\n    self.passage_instruct = passage_instruct\n    self.model_kwargs = model_kwargs\n\n    self.text_emb_model = SentenceTransformer(\n        self.text_emb_model_name,\n        trust_remote_code=True,\n        model_kwargs=self.model_kwargs,\n    )\n</code></pre>"},{"location":"api/text_emb_models/#gfmrag.text_emb_models.BaseTextEmbModel.encode","title":"<code>encode(text, is_query=False, show_progress_bar=True)</code>","text":"<p>Encodes a list of text strings into embeddings using the text embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>list[str]</code> <p>List of text strings to encode</p> required <code>is_query</code> <code>bool</code> <p>Whether the text is a query (True) or passage (False). Determines which instruction prompt to use. Defaults to False.</p> <code>False</code> <code>show_progress_bar</code> <code>bool</code> <p>Whether to display progress bar during encoding. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Tensor containing the encoded embeddings for the input text</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; text_emb_model = BaseTextEmbModel(\"sentence-transformers/all-mpnet-base-v2\")\n&gt;&gt;&gt; text = [\"Hello, world!\", \"This is a test.\"]\n&gt;&gt;&gt; embeddings = text_emb_model.encode(text)\n</code></pre> Source code in <code>gfmrag/text_emb_models/base_model.py</code> Python<pre><code>def encode(\n    self, text: list[str], is_query: bool = False, show_progress_bar: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"\n    Encodes a list of text strings into embeddings using the text embedding model.\n\n    Args:\n        text (list[str]): List of text strings to encode\n        is_query (bool, optional): Whether the text is a query (True) or passage (False).\n            Determines which instruction prompt to use. Defaults to False.\n        show_progress_bar (bool, optional): Whether to display progress bar during encoding.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: Tensor containing the encoded embeddings for the input text\n\n    Examples:\n        &gt;&gt;&gt; text_emb_model = BaseTextEmbModel(\"sentence-transformers/all-mpnet-base-v2\")\n        &gt;&gt;&gt; text = [\"Hello, world!\", \"This is a test.\"]\n        &gt;&gt;&gt; embeddings = text_emb_model.encode(text)\n    \"\"\"\n\n    return self.text_emb_model.encode(\n        text,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        normalize_embeddings=self.normalize,\n        batch_size=self.batch_size,\n        prompt=self.query_instruct if is_query else self.passage_instruct,\n        show_progress_bar=show_progress_bar,\n        convert_to_tensor=True,\n    )\n</code></pre>"},{"location":"api/text_emb_models/#gfmrag.text_emb_models.NVEmbedV2","title":"<code>NVEmbedV2</code>","text":"<p>               Bases: <code>BaseTextEmbModel</code></p> <p>A text embedding model class that extends BaseTextEmbModel specifically for Nvidia models.</p> <p>This class customizes the base embedding model by: 1. Setting a larger max sequence length of 32768 2. Setting right-side padding 3. Adding EOS tokens to input text</p> <p>Parameters:</p> Name Type Description Default <code>text_emb_model_name</code> <code>str</code> <p>Name or path of the text embedding model</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the output embeddings</p> required <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> required <code>query_instruct</code> <code>str</code> <p>Instruction prefix for query texts. Defaults to \"\".</p> <code>''</code> <code>passage_instruct</code> <code>str</code> <p>Instruction prefix for passage texts. Defaults to \"\".</p> <code>''</code> <code>model_kwargs</code> <code>dict | None</code> <p>Additional keyword arguments for model initialization. Defaults to None.</p> <code>None</code> <p>Methods:</p> Name Description <code>add_eos</code> <p>Adds EOS token to each input example</p> <code>encode</code> <p>Encodes text by first adding EOS tokens then calling parent encode method</p> <p>Attributes:</p> Name Type Description <code>text_emb_model</code> <p>The underlying text embedding model with customized max_seq_length and padding_side</p> Source code in <code>gfmrag/text_emb_models/nv_embed.py</code> Python<pre><code>class NVEmbedV2(BaseTextEmbModel):\n    \"\"\"A text embedding model class that extends BaseTextEmbModel specifically for Nvidia models.\n\n    This class customizes the base embedding model by:\n    1. Setting a larger max sequence length of 32768\n    2. Setting right-side padding\n    3. Adding EOS tokens to input text\n\n    Args:\n        text_emb_model_name (str): Name or path of the text embedding model\n        normalize (bool): Whether to normalize the output embeddings\n        batch_size (int): Batch size for processing\n        query_instruct (str, optional): Instruction prefix for query texts. Defaults to \"\".\n        passage_instruct (str, optional): Instruction prefix for passage texts. Defaults to \"\".\n        model_kwargs (dict | None, optional): Additional keyword arguments for model initialization. Defaults to None.\n\n    Methods:\n        add_eos: Adds EOS token to each input example\n        encode: Encodes text by first adding EOS tokens then calling parent encode method\n\n    Attributes:\n        text_emb_model: The underlying text embedding model with customized max_seq_length and padding_side\n    \"\"\"\n\n    def __init__(\n        self,\n        text_emb_model_name: str,\n        normalize: bool,\n        batch_size: int,\n        query_instruct: str = \"\",\n        passage_instruct: str = \"\",\n        model_kwargs: dict | None = None,\n    ) -&gt; None:\n        super().__init__(\n            text_emb_model_name,\n            normalize,\n            batch_size,\n            query_instruct,\n            passage_instruct,\n            model_kwargs,\n        )\n        self.text_emb_model.max_seq_length = 32768\n        self.text_emb_model.tokenizer.padding_side = \"right\"\n\n    def add_eos(self, input_examples: list[str]) -&gt; list[str]:\n        input_examples = [\n            input_example + self.text_emb_model.tokenizer.eos_token\n            for input_example in input_examples\n        ]\n        return input_examples\n\n    def encode(self, text: list[str], *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Encode a list of text strings into embeddings with added EOS token.\n\n        This method adds an EOS (end of sequence) token to each text string before encoding.\n\n        Args:\n            text (list[str]): List of text strings to encode\n            *args (Any): Additional positional arguments passed to parent encode method\n            **kwargs (Any): Additional keyword arguments passed to parent encode method\n\n        Returns:\n            torch.Tensor: Encoded text embeddings tensor\n\n        Examples:\n            &gt;&gt;&gt; encoder = NVEmbedder()\n            &gt;&gt;&gt; texts = [\"Hello world\", \"Another text\"]\n            &gt;&gt;&gt; embeddings = encoder.encode(texts)\n        \"\"\"\n        return super().encode(self.add_eos(text), *args, **kwargs)\n</code></pre>"},{"location":"api/text_emb_models/#gfmrag.text_emb_models.NVEmbedV2.encode","title":"<code>encode(text, *args, **kwargs)</code>","text":"<p>Encode a list of text strings into embeddings with added EOS token.</p> <p>This method adds an EOS (end of sequence) token to each text string before encoding.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>list[str]</code> <p>List of text strings to encode</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments passed to parent encode method</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to parent encode method</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Encoded text embeddings tensor</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; encoder = NVEmbedder()\n&gt;&gt;&gt; texts = [\"Hello world\", \"Another text\"]\n&gt;&gt;&gt; embeddings = encoder.encode(texts)\n</code></pre> Source code in <code>gfmrag/text_emb_models/nv_embed.py</code> Python<pre><code>def encode(self, text: list[str], *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n    \"\"\"\n    Encode a list of text strings into embeddings with added EOS token.\n\n    This method adds an EOS (end of sequence) token to each text string before encoding.\n\n    Args:\n        text (list[str]): List of text strings to encode\n        *args (Any): Additional positional arguments passed to parent encode method\n        **kwargs (Any): Additional keyword arguments passed to parent encode method\n\n    Returns:\n        torch.Tensor: Encoded text embeddings tensor\n\n    Examples:\n        &gt;&gt;&gt; encoder = NVEmbedder()\n        &gt;&gt;&gt; texts = [\"Hello world\", \"Another text\"]\n        &gt;&gt;&gt; embeddings = encoder.encode(texts)\n    \"\"\"\n    return super().encode(self.add_eos(text), *args, **kwargs)\n</code></pre>"},{"location":"api/kg_construnction/el_model/","title":"Entity Linking Model","text":""},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model","title":"<code>gfmrag.kg_construction.entity_linking_model</code>","text":""},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.BaseELModel","title":"<code>BaseELModel</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>gfmrag/kg_construction/entity_linking_model/base_model.py</code> Python<pre><code>class BaseELModel(ABC):\n    @abstractmethod\n    def __init__(self, **kwargs: Any) -&gt; None:\n        pass\n\n    @abstractmethod\n    def index(self, entity_list: list) -&gt; None:\n        \"\"\"\n        This method creates an index for the provided list of entities to enable efficient entity linking and searching capabilities.\n\n        Args:\n            entity_list (list): A list of entities to be indexed. Each entity should be a string or dictionary containing\n                               the entity text and other relevant metadata.\n\n            None: This method modifies the internal index structure but does not return anything.\n\n        Raises:\n            ValueError: If entity_list is empty or contains invalid entity formats.\n            TypeError: If entity_list is not a list type.\n\n        Examples:\n            &gt;&gt;&gt; model = EntityLinkingModel()\n            &gt;&gt;&gt; entities = [\"Paris\", \"France\", \"Eiffel Tower\"]\n            &gt;&gt;&gt; model.index(entities)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n        \"\"\"\n        Link entities in the given text to the knowledge graph.\n\n        Args:\n            ner_entity_list (list): list of named entities\n            topk (int): number of linked entities to return\n\n        Returns:\n            dict: dict of linked entities in the knowledge graph\n\n                - key (str): named entity\n                - value (list[dict]): list of linked entities\n\n                    - entity: linked entity\n                    - score: score of the entity\n                    - norm_score: normalized score of the entity\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.BaseELModel.__call__","title":"<code>__call__(ner_entity_list, topk=1)</code>  <code>abstractmethod</code>","text":"<p>Link entities in the given text to the knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>ner_entity_list</code> <code>list</code> <p>list of named entities</p> required <code>topk</code> <code>int</code> <p>number of linked entities to return</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict of linked entities in the knowledge graph</p> <ul> <li>key (str): named entity</li> <li> <p>value (list[dict]): list of linked entities</p> <ul> <li>entity: linked entity</li> <li>score: score of the entity</li> <li>norm_score: normalized score of the entity</li> </ul> </li> </ul> Source code in <code>gfmrag/kg_construction/entity_linking_model/base_model.py</code> Python<pre><code>@abstractmethod\ndef __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n    \"\"\"\n    Link entities in the given text to the knowledge graph.\n\n    Args:\n        ner_entity_list (list): list of named entities\n        topk (int): number of linked entities to return\n\n    Returns:\n        dict: dict of linked entities in the knowledge graph\n\n            - key (str): named entity\n            - value (list[dict]): list of linked entities\n\n                - entity: linked entity\n                - score: score of the entity\n                - norm_score: normalized score of the entity\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.BaseELModel.index","title":"<code>index(entity_list)</code>  <code>abstractmethod</code>","text":"<p>This method creates an index for the provided list of entities to enable efficient entity linking and searching capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>entity_list</code> <code>list</code> <p>A list of entities to be indexed. Each entity should be a string or dictionary containing                the entity text and other relevant metadata.</p> required <code>None</code> <p>This method modifies the internal index structure but does not return anything.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity_list is empty or contains invalid entity formats.</p> <code>TypeError</code> <p>If entity_list is not a list type.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; model = EntityLinkingModel()\n&gt;&gt;&gt; entities = [\"Paris\", \"France\", \"Eiffel Tower\"]\n&gt;&gt;&gt; model.index(entities)\n</code></pre> Source code in <code>gfmrag/kg_construction/entity_linking_model/base_model.py</code> Python<pre><code>@abstractmethod\ndef index(self, entity_list: list) -&gt; None:\n    \"\"\"\n    This method creates an index for the provided list of entities to enable efficient entity linking and searching capabilities.\n\n    Args:\n        entity_list (list): A list of entities to be indexed. Each entity should be a string or dictionary containing\n                           the entity text and other relevant metadata.\n\n        None: This method modifies the internal index structure but does not return anything.\n\n    Raises:\n        ValueError: If entity_list is empty or contains invalid entity formats.\n        TypeError: If entity_list is not a list type.\n\n    Examples:\n        &gt;&gt;&gt; model = EntityLinkingModel()\n        &gt;&gt;&gt; entities = [\"Paris\", \"France\", \"Eiffel Tower\"]\n        &gt;&gt;&gt; model.index(entities)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.ColbertELModel","title":"<code>ColbertELModel</code>","text":"<p>               Bases: <code>BaseELModel</code></p> <p>ColBERT-based Entity Linking Model.</p> <p>This class implements an entity linking model using ColBERT, a neural information retrieval framework. It indexes a list of entities and performs entity linking by finding the most similar entities in the index for given named entities.</p> <p>Attributes:</p> Name Type Description <code>checkpoint_path</code> <code>str</code> <p>Path to the ColBERT checkpoint file</p> <code>root</code> <code>str</code> <p>Root directory for storing indices</p> <code>doc_index_name</code> <code>str</code> <p>Name of document index</p> <code>phrase_index_name</code> <code>str</code> <p>Name of phrase index</p> <code>force</code> <code>bool</code> <p>Whether to force reindex if index exists</p> <code>entity_list</code> <code>list</code> <p>List of entities to be indexed</p> <code>phrase_searcher</code> <code>list</code> <p>ColBERT phrase searcher object</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the checkpoint file is not found at the specified path.</p> <code>AttributeError</code> <p>If entity linking is attempted before indexing.</p> Notes <p>You need to download the checkpoint by running the following command: <code>wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz &amp;&amp; tar -zxvf colbertv2.0.tar.gz -C tmp/</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; model = ColbertELModel(\"path/to/checkpoint\")\n&gt;&gt;&gt; model.index([\"entity1\", \"entity2\", \"entity3\"])\n&gt;&gt;&gt; results = model([\"query1\", \"query2\"], topk=3)\n&gt;&gt;&gt; print(results)\n{'paris city': [{'entity': 'entity1', 'score': 0.82, 'norm_score': 1.0},\n                {'entity': 'entity2', 'score': 0.35, 'norm_score': 0.43}]}\n</code></pre> Source code in <code>gfmrag/kg_construction/entity_linking_model/colbert_el_model.py</code> Python<pre><code>class ColbertELModel(BaseELModel):\n    \"\"\"ColBERT-based Entity Linking Model.\n\n    This class implements an entity linking model using ColBERT, a neural information retrieval\n    framework. It indexes a list of entities and performs entity linking by finding the most\n    similar entities in the index for given named entities.\n\n    Attributes:\n        checkpoint_path (str): Path to the ColBERT checkpoint file\n        root (str): Root directory for storing indices\n        doc_index_name (str): Name of document index\n        phrase_index_name (str): Name of phrase index\n        force (bool): Whether to force reindex if index exists\n        entity_list (list): List of entities to be indexed\n        phrase_searcher: ColBERT phrase searcher object\n\n    Raises:\n        FileNotFoundError: If the checkpoint file is not found at the specified path.\n        AttributeError: If entity linking is attempted before indexing.\n\n    Notes:\n        You need to download the checkpoint by running the following command:\n        `wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz &amp;&amp; tar -zxvf colbertv2.0.tar.gz -C tmp/`\n\n    Examples:\n        &gt;&gt;&gt; model = ColbertELModel(\"path/to/checkpoint\")\n        &gt;&gt;&gt; model.index([\"entity1\", \"entity2\", \"entity3\"])\n        &gt;&gt;&gt; results = model([\"query1\", \"query2\"], topk=3)\n        &gt;&gt;&gt; print(results)\n        {'paris city': [{'entity': 'entity1', 'score': 0.82, 'norm_score': 1.0},\n                        {'entity': 'entity2', 'score': 0.35, 'norm_score': 0.43}]}\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint_path: str,\n        root: str = \"tmp\",\n        doc_index_name: str = \"nbits_2\",\n        phrase_index_name: str = \"nbits_2\",\n        force: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ColBERT entity linking model.\n\n        This initializes a ColBERT model for entity linking using pre-trained checkpoints and indices.\n\n        Args:\n            checkpoint_path (str): Path to the ColBERT checkpoint file. Model weights will be loaded from this path. Can be downloaded [here](https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz)\n            root (str, optional): Root directory for storing indices. Defaults to \"tmp\".\n            doc_index_name (str, optional): Name of the document index. Defaults to \"nbits_2\".\n            phrase_index_name (str, optional): Name of the phrase index. Defaults to \"nbits_2\".\n            force (bool, optional): Whether to force recomputation of existing indices. Defaults to False.\n\n        Raises:\n            FileNotFoundError: If the checkpoint file does not exist at the specified path.\n\n        Returns:\n            None\n        \"\"\"\n\n        if not os.path.exists(checkpoint_path):\n            raise FileNotFoundError(\n                \"Checkpoint not found, download the checkpoint with: 'wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz &amp;&amp; tar -zxvf tmp/colbertv2.0.tar.gz -C tmp/'\"\n            )\n        self.checkpoint_path = checkpoint_path\n        self.root = root\n        self.doc_index_name = doc_index_name\n        self.phrase_index_name = phrase_index_name\n        self.force = force\n\n    def index(self, entity_list: list) -&gt; None:\n        \"\"\"\n        Index a list of entities using ColBERT for efficient similarity search.\n\n        This method processes and indexes a list of entities using the ColBERT model. It creates\n        a unique index based on the MD5 hash of the entity list and stores it in the specified\n        root directory.\n\n        Args:\n            entity_list (list): List of entity strings to be indexed.\n\n        Returns:\n            None\n\n        Notes:\n            - Creates a unique index directory based on MD5 hash of entities\n            - If force=True, will delete existing index with same fingerprint\n            - Processes entities into phrases before indexing\n            - Sets up ColBERT indexer and searcher with specified configuration\n            - Stores phrase_searcher as instance variable for later use\n        \"\"\"\n        self.entity_list = entity_list\n        # Get md5 fingerprint of the whole given entity list\n        fingerprint = hashlib.md5(\"\".join(entity_list).encode()).hexdigest()\n        exp_name = f\"Entity_index_{fingerprint}\"\n        if os.path.exists(f\"{self.root}/colbert/{fingerprint}\") and self.force:\n            shutil.rmtree(f\"{self.root}/colbert/{fingerprint}\")\n        colbert_config = {\n            \"root\": f\"{self.root}/colbert/{fingerprint}\",\n            \"doc_index_name\": self.doc_index_name,\n            \"phrase_index_name\": self.phrase_index_name,\n        }\n        phrases = [processing_phrases(p) for p in entity_list]\n        with Run().context(\n            RunConfig(nranks=1, experiment=exp_name, root=colbert_config[\"root\"])\n        ):\n            config = ColBERTConfig(\n                nbits=2,\n                root=colbert_config[\"root\"],\n            )\n            indexer = Indexer(checkpoint=self.checkpoint_path, config=config)\n            indexer.index(\n                name=self.phrase_index_name, collection=phrases, overwrite=\"reuse\"\n            )\n\n        with Run().context(\n            RunConfig(nranks=1, experiment=exp_name, root=colbert_config[\"root\"])\n        ):\n            config = ColBERTConfig(\n                root=colbert_config[\"root\"],\n            )\n            phrase_searcher = Searcher(\n                index=colbert_config[\"phrase_index_name\"], config=config, verbose=1\n            )\n        self.phrase_searcher = phrase_searcher\n\n    def __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n        \"\"\"\n        Link entities in the given text to the knowledge graph.\n\n        Args:\n            ner_entity_list (list): list of named entities\n            topk (int): number of linked entities to return\n\n        Returns:\n            dict: dict of linked entities in the knowledge graph\n\n                - key (str): named entity\n                - value (list[dict]): list of linked entities\n\n                    - entity: linked entity\n                    - score: score of the entity\n                    - norm_score: normalized score of the entity\n        \"\"\"\n\n        try:\n            self.__getattribute__(\"phrase_searcher\")\n        except AttributeError as e:\n            raise AttributeError(\"Index the entities first using index method\") from e\n\n        ner_entity_list = [processing_phrases(p) for p in ner_entity_list]\n        query_data: dict[int, str] = {\n            i: query for i, query in enumerate(ner_entity_list)\n        }\n\n        queries = Queries(path=None, data=query_data)\n        ranking = self.phrase_searcher.search_all(queries, k=topk)\n\n        linked_entity_dict: dict[str, list] = {}\n        for i in range(len(queries)):\n            query = queries[i]\n            rank = ranking.data[i]\n            linked_entity_dict[query] = []\n            max_score = rank[0][2]\n\n            for phrase_id, _rank, score in rank:\n                linked_entity_dict[query].append(\n                    {\n                        \"entity\": self.entity_list[phrase_id],\n                        \"score\": score,\n                        \"norm_score\": score / max_score,\n                    }\n                )\n\n        return linked_entity_dict\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.ColbertELModel.__call__","title":"<code>__call__(ner_entity_list, topk=1)</code>","text":"<p>Link entities in the given text to the knowledge graph.</p> <p>Parameters:</p> Name Type Description Default <code>ner_entity_list</code> <code>list</code> <p>list of named entities</p> required <code>topk</code> <code>int</code> <p>number of linked entities to return</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict of linked entities in the knowledge graph</p> <ul> <li>key (str): named entity</li> <li> <p>value (list[dict]): list of linked entities</p> <ul> <li>entity: linked entity</li> <li>score: score of the entity</li> <li>norm_score: normalized score of the entity</li> </ul> </li> </ul> Source code in <code>gfmrag/kg_construction/entity_linking_model/colbert_el_model.py</code> Python<pre><code>def __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n    \"\"\"\n    Link entities in the given text to the knowledge graph.\n\n    Args:\n        ner_entity_list (list): list of named entities\n        topk (int): number of linked entities to return\n\n    Returns:\n        dict: dict of linked entities in the knowledge graph\n\n            - key (str): named entity\n            - value (list[dict]): list of linked entities\n\n                - entity: linked entity\n                - score: score of the entity\n                - norm_score: normalized score of the entity\n    \"\"\"\n\n    try:\n        self.__getattribute__(\"phrase_searcher\")\n    except AttributeError as e:\n        raise AttributeError(\"Index the entities first using index method\") from e\n\n    ner_entity_list = [processing_phrases(p) for p in ner_entity_list]\n    query_data: dict[int, str] = {\n        i: query for i, query in enumerate(ner_entity_list)\n    }\n\n    queries = Queries(path=None, data=query_data)\n    ranking = self.phrase_searcher.search_all(queries, k=topk)\n\n    linked_entity_dict: dict[str, list] = {}\n    for i in range(len(queries)):\n        query = queries[i]\n        rank = ranking.data[i]\n        linked_entity_dict[query] = []\n        max_score = rank[0][2]\n\n        for phrase_id, _rank, score in rank:\n            linked_entity_dict[query].append(\n                {\n                    \"entity\": self.entity_list[phrase_id],\n                    \"score\": score,\n                    \"norm_score\": score / max_score,\n                }\n            )\n\n    return linked_entity_dict\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.ColbertELModel.__init__","title":"<code>__init__(checkpoint_path, root='tmp', doc_index_name='nbits_2', phrase_index_name='nbits_2', force=False)</code>","text":"<p>Initialize the ColBERT entity linking model.</p> <p>This initializes a ColBERT model for entity linking using pre-trained checkpoints and indices.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to the ColBERT checkpoint file. Model weights will be loaded from this path. Can be downloaded here</p> required <code>root</code> <code>str</code> <p>Root directory for storing indices. Defaults to \"tmp\".</p> <code>'tmp'</code> <code>doc_index_name</code> <code>str</code> <p>Name of the document index. Defaults to \"nbits_2\".</p> <code>'nbits_2'</code> <code>phrase_index_name</code> <code>str</code> <p>Name of the phrase index. Defaults to \"nbits_2\".</p> <code>'nbits_2'</code> <code>force</code> <code>bool</code> <p>Whether to force recomputation of existing indices. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the checkpoint file does not exist at the specified path.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/kg_construction/entity_linking_model/colbert_el_model.py</code> Python<pre><code>def __init__(\n    self,\n    checkpoint_path: str,\n    root: str = \"tmp\",\n    doc_index_name: str = \"nbits_2\",\n    phrase_index_name: str = \"nbits_2\",\n    force: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the ColBERT entity linking model.\n\n    This initializes a ColBERT model for entity linking using pre-trained checkpoints and indices.\n\n    Args:\n        checkpoint_path (str): Path to the ColBERT checkpoint file. Model weights will be loaded from this path. Can be downloaded [here](https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz)\n        root (str, optional): Root directory for storing indices. Defaults to \"tmp\".\n        doc_index_name (str, optional): Name of the document index. Defaults to \"nbits_2\".\n        phrase_index_name (str, optional): Name of the phrase index. Defaults to \"nbits_2\".\n        force (bool, optional): Whether to force recomputation of existing indices. Defaults to False.\n\n    Raises:\n        FileNotFoundError: If the checkpoint file does not exist at the specified path.\n\n    Returns:\n        None\n    \"\"\"\n\n    if not os.path.exists(checkpoint_path):\n        raise FileNotFoundError(\n            \"Checkpoint not found, download the checkpoint with: 'wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz &amp;&amp; tar -zxvf tmp/colbertv2.0.tar.gz -C tmp/'\"\n        )\n    self.checkpoint_path = checkpoint_path\n    self.root = root\n    self.doc_index_name = doc_index_name\n    self.phrase_index_name = phrase_index_name\n    self.force = force\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.ColbertELModel.index","title":"<code>index(entity_list)</code>","text":"<p>Index a list of entities using ColBERT for efficient similarity search.</p> <p>This method processes and indexes a list of entities using the ColBERT model. It creates a unique index based on the MD5 hash of the entity list and stores it in the specified root directory.</p> <p>Parameters:</p> Name Type Description Default <code>entity_list</code> <code>list</code> <p>List of entity strings to be indexed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Creates a unique index directory based on MD5 hash of entities</li> <li>If force=True, will delete existing index with same fingerprint</li> <li>Processes entities into phrases before indexing</li> <li>Sets up ColBERT indexer and searcher with specified configuration</li> <li>Stores phrase_searcher as instance variable for later use</li> </ul> Source code in <code>gfmrag/kg_construction/entity_linking_model/colbert_el_model.py</code> Python<pre><code>def index(self, entity_list: list) -&gt; None:\n    \"\"\"\n    Index a list of entities using ColBERT for efficient similarity search.\n\n    This method processes and indexes a list of entities using the ColBERT model. It creates\n    a unique index based on the MD5 hash of the entity list and stores it in the specified\n    root directory.\n\n    Args:\n        entity_list (list): List of entity strings to be indexed.\n\n    Returns:\n        None\n\n    Notes:\n        - Creates a unique index directory based on MD5 hash of entities\n        - If force=True, will delete existing index with same fingerprint\n        - Processes entities into phrases before indexing\n        - Sets up ColBERT indexer and searcher with specified configuration\n        - Stores phrase_searcher as instance variable for later use\n    \"\"\"\n    self.entity_list = entity_list\n    # Get md5 fingerprint of the whole given entity list\n    fingerprint = hashlib.md5(\"\".join(entity_list).encode()).hexdigest()\n    exp_name = f\"Entity_index_{fingerprint}\"\n    if os.path.exists(f\"{self.root}/colbert/{fingerprint}\") and self.force:\n        shutil.rmtree(f\"{self.root}/colbert/{fingerprint}\")\n    colbert_config = {\n        \"root\": f\"{self.root}/colbert/{fingerprint}\",\n        \"doc_index_name\": self.doc_index_name,\n        \"phrase_index_name\": self.phrase_index_name,\n    }\n    phrases = [processing_phrases(p) for p in entity_list]\n    with Run().context(\n        RunConfig(nranks=1, experiment=exp_name, root=colbert_config[\"root\"])\n    ):\n        config = ColBERTConfig(\n            nbits=2,\n            root=colbert_config[\"root\"],\n        )\n        indexer = Indexer(checkpoint=self.checkpoint_path, config=config)\n        indexer.index(\n            name=self.phrase_index_name, collection=phrases, overwrite=\"reuse\"\n        )\n\n    with Run().context(\n        RunConfig(nranks=1, experiment=exp_name, root=colbert_config[\"root\"])\n    ):\n        config = ColBERTConfig(\n            root=colbert_config[\"root\"],\n        )\n        phrase_searcher = Searcher(\n            index=colbert_config[\"phrase_index_name\"], config=config, verbose=1\n        )\n    self.phrase_searcher = phrase_searcher\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.DPRELModel","title":"<code>DPRELModel</code>","text":"<p>               Bases: <code>BaseELModel</code></p> <p>Entity Linking Model based on Dense Passage Retrieval (DPR).</p> <p>This class implements an entity linking model using DPR architecture and SentenceTransformer for encoding entities and computing similarity scores between mentions and candidate entities.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the SentenceTransformer model to use</p> required <code>root</code> <code>str</code> <p>Root directory for caching embeddings. Defaults to \"tmp\".</p> <code>'tmp'</code> <code>use_cache</code> <code>bool</code> <p>Whether to cache and reuse entity embeddings. Defaults to True.</p> <code>True</code> <code>normalize</code> <code>bool</code> <p>Whether to L2-normalize embeddings. Defaults to True.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Batch size for encoding. Defaults to 32.</p> <code>32</code> <code>query_instruct</code> <code>str</code> <p>Instruction/prompt prefix for query encoding. Defaults to \"\".</p> <code>''</code> <code>passage_instruct</code> <code>str</code> <p>Instruction/prompt prefix for passage encoding. Defaults to \"\".</p> <code>''</code> <code>model_kwargs</code> <code>dict</code> <p>Additional kwargs to pass to SentenceTransformer. Defaults to None.</p> <code>None</code> <p>Methods:</p> Name Description <code>index</code> <p>Indexes a list of entities by computing and caching their embeddings</p> <code>__call__</code> <p>Links named entities to indexed entities and returns top-k matches</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; model = DPRELModel('sentence-transformers/all-mpnet-base-v2')\n&gt;&gt;&gt; model.index(['Paris', 'London', 'Berlin'])\n&gt;&gt;&gt; results = model(['paris city'], topk=2)\n&gt;&gt;&gt; print(results)\n{'paris city': [{'entity': 'Paris', 'score': 0.82, 'norm_score': 1.0},\n                {'entity': 'London', 'score': 0.35, 'norm_score': 0.43}]}\n</code></pre> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>class DPRELModel(BaseELModel):\n    \"\"\"\n    Entity Linking Model based on Dense Passage Retrieval (DPR).\n\n    This class implements an entity linking model using DPR architecture and SentenceTransformer\n    for encoding entities and computing similarity scores between mentions and candidate entities.\n\n    Args:\n        model_name (str): Name or path of the SentenceTransformer model to use\n        root (str, optional): Root directory for caching embeddings. Defaults to \"tmp\".\n        use_cache (bool, optional): Whether to cache and reuse entity embeddings. Defaults to True.\n        normalize (bool, optional): Whether to L2-normalize embeddings. Defaults to True.\n        batch_size (int, optional): Batch size for encoding. Defaults to 32.\n        query_instruct (str, optional): Instruction/prompt prefix for query encoding. Defaults to \"\".\n        passage_instruct (str, optional): Instruction/prompt prefix for passage encoding. Defaults to \"\".\n        model_kwargs (dict, optional): Additional kwargs to pass to SentenceTransformer. Defaults to None.\n\n    Methods:\n        index(entity_list): Indexes a list of entities by computing and caching their embeddings\n        __call__(ner_entity_list, topk): Links named entities to indexed entities and returns top-k matches\n\n    Examples:\n        &gt;&gt;&gt; model = DPRELModel('sentence-transformers/all-mpnet-base-v2')\n        &gt;&gt;&gt; model.index(['Paris', 'London', 'Berlin'])\n        &gt;&gt;&gt; results = model(['paris city'], topk=2)\n        &gt;&gt;&gt; print(results)\n        {'paris city': [{'entity': 'Paris', 'score': 0.82, 'norm_score': 1.0},\n                        {'entity': 'London', 'score': 0.35, 'norm_score': 0.43}]}\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        root: str = \"tmp\",\n        use_cache: bool = True,\n        normalize: bool = True,\n        batch_size: int = 32,\n        query_instruct: str = \"\",\n        passage_instruct: str = \"\",\n        model_kwargs: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize DPR Entity Linking Model.\n\n        Args:\n            model_name (str): Name or path of the pre-trained model to load.\n            root (str, optional): Root directory for cache storage. Defaults to \"tmp\".\n            use_cache (bool, optional): Whether to use cache for embeddings. Defaults to True.\n            normalize (bool, optional): Whether to normalize the embeddings. Defaults to True.\n            batch_size (int, optional): Batch size for encoding. Defaults to 32.\n            query_instruct (str, optional): Instruction prefix for query encoding. Defaults to \"\".\n            passage_instruct (str, optional): Instruction prefix for passage encoding. Defaults to \"\".\n            model_kwargs (dict | None, optional): Additional arguments to pass to the model. Defaults to None.\n        \"\"\"\n\n        self.model_name = model_name\n        self.use_cache = use_cache\n        self.normalize = normalize\n        self.batch_size = batch_size\n        self.root = os.path.join(root, f\"{self.model_name.replace('/', '_')}_dpr_cache\")\n        if self.use_cache and not os.path.exists(self.root):\n            os.makedirs(self.root)\n        self.model = SentenceTransformer(\n            model_name, trust_remote_code=True, model_kwargs=model_kwargs\n        )\n        self.query_instruct = query_instruct\n        self.passage_instruct = passage_instruct\n\n    def index(self, entity_list: list) -&gt; None:\n        \"\"\"\n        Index a list of entities by encoding them into embeddings and optionally caching the results.\n\n        This method processes a list of entity strings, converting them into dense vector representations\n        using a pre-trained model. To avoid redundant computation, it implements a caching mechanism\n        based on the MD5 hash of the input entity list.\n\n        Args:\n            entity_list (list): A list of strings representing entities to be indexed.\n\n        Returns:\n            None\n\n        Notes:\n            - The method stores the embeddings in self.entity_embeddings\n            - If caching is enabled and a cache file exists for the given entity list,\n              embeddings are loaded from cache instead of being recomputed\n            - Cache files are stored using the MD5 hash of the concatenated entity list as filename\n            - Embeddings are computed on GPU if available, otherwise on CPU\n        \"\"\"\n        self.entity_list = entity_list\n        # Get md5 fingerprint of the whole given entity list\n        fingerprint = hashlib.md5(\"\".join(entity_list).encode()).hexdigest()\n        cache_file = f\"{self.root}/{fingerprint}.pt\"\n        if os.path.exists(cache_file):\n            self.entity_embeddings = torch.load(\n                cache_file, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n            )\n        else:\n            self.entity_embeddings = self.model.encode(\n                entity_list,\n                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                convert_to_tensor=True,\n                show_progress_bar=True,\n                prompt=self.passage_instruct,\n                normalize_embeddings=self.normalize,\n                batch_size=self.batch_size,\n            )\n            if self.use_cache:\n                torch.save(self.entity_embeddings, cache_file)\n\n    def __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n        \"\"\"\n        Performs entity linking by matching input entities with pre-encoded entity embeddings.\n\n        This method takes a list of named entities (e.g., from NER), computes their embeddings,\n        and finds the closest matching entities from the pre-encoded knowledge base using\n        cosine similarity.\n\n        Args:\n            ner_entity_list (list): List of named entities to link\n            topk (int, optional): Number of top matches to return for each entity. Defaults to 1.\n\n        Returns:\n            dict: Dictionary mapping each input entity to its linked candidates. For each candidate:\n                - entity (str): The matched entity name from the knowledge base\n                - score (float): Raw similarity score\n                - norm_score (float): Normalized similarity score (relative to top match)\n        \"\"\"\n        ner_entity_embeddings = self.model.encode(\n            ner_entity_list,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            convert_to_tensor=True,\n            prompt=self.query_instruct,\n            normalize_embeddings=self.normalize,\n            batch_size=self.batch_size,\n        )\n        scores = ner_entity_embeddings @ self.entity_embeddings.T\n        top_k_scores, top_k_values = torch.topk(scores, topk, dim=-1)\n        linked_entity_dict: dict[str, list] = {}\n        for i in range(len(ner_entity_list)):\n            linked_entity_dict[ner_entity_list[i]] = []\n\n            sorted_score = top_k_scores[i]\n            sorted_indices = top_k_values[i]\n            max_score = sorted_score[0].item()\n\n            for score, top_k_index in zip(sorted_score, sorted_indices):\n                linked_entity_dict[ner_entity_list[i]].append(\n                    {\n                        \"entity\": self.entity_list[top_k_index],\n                        \"score\": score.item(),\n                        \"norm_score\": score.item() / max_score,\n                    }\n                )\n        return linked_entity_dict\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.DPRELModel.__call__","title":"<code>__call__(ner_entity_list, topk=1)</code>","text":"<p>Performs entity linking by matching input entities with pre-encoded entity embeddings.</p> <p>This method takes a list of named entities (e.g., from NER), computes their embeddings, and finds the closest matching entities from the pre-encoded knowledge base using cosine similarity.</p> <p>Parameters:</p> Name Type Description Default <code>ner_entity_list</code> <code>list</code> <p>List of named entities to link</p> required <code>topk</code> <code>int</code> <p>Number of top matches to return for each entity. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping each input entity to its linked candidates. For each candidate: - entity (str): The matched entity name from the knowledge base - score (float): Raw similarity score - norm_score (float): Normalized similarity score (relative to top match)</p> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def __call__(self, ner_entity_list: list, topk: int = 1) -&gt; dict:\n    \"\"\"\n    Performs entity linking by matching input entities with pre-encoded entity embeddings.\n\n    This method takes a list of named entities (e.g., from NER), computes their embeddings,\n    and finds the closest matching entities from the pre-encoded knowledge base using\n    cosine similarity.\n\n    Args:\n        ner_entity_list (list): List of named entities to link\n        topk (int, optional): Number of top matches to return for each entity. Defaults to 1.\n\n    Returns:\n        dict: Dictionary mapping each input entity to its linked candidates. For each candidate:\n            - entity (str): The matched entity name from the knowledge base\n            - score (float): Raw similarity score\n            - norm_score (float): Normalized similarity score (relative to top match)\n    \"\"\"\n    ner_entity_embeddings = self.model.encode(\n        ner_entity_list,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        convert_to_tensor=True,\n        prompt=self.query_instruct,\n        normalize_embeddings=self.normalize,\n        batch_size=self.batch_size,\n    )\n    scores = ner_entity_embeddings @ self.entity_embeddings.T\n    top_k_scores, top_k_values = torch.topk(scores, topk, dim=-1)\n    linked_entity_dict: dict[str, list] = {}\n    for i in range(len(ner_entity_list)):\n        linked_entity_dict[ner_entity_list[i]] = []\n\n        sorted_score = top_k_scores[i]\n        sorted_indices = top_k_values[i]\n        max_score = sorted_score[0].item()\n\n        for score, top_k_index in zip(sorted_score, sorted_indices):\n            linked_entity_dict[ner_entity_list[i]].append(\n                {\n                    \"entity\": self.entity_list[top_k_index],\n                    \"score\": score.item(),\n                    \"norm_score\": score.item() / max_score,\n                }\n            )\n    return linked_entity_dict\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.DPRELModel.__init__","title":"<code>__init__(model_name, root='tmp', use_cache=True, normalize=True, batch_size=32, query_instruct='', passage_instruct='', model_kwargs=None)</code>","text":"<p>Initialize DPR Entity Linking Model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name or path of the pre-trained model to load.</p> required <code>root</code> <code>str</code> <p>Root directory for cache storage. Defaults to \"tmp\".</p> <code>'tmp'</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cache for embeddings. Defaults to True.</p> <code>True</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the embeddings. Defaults to True.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>Batch size for encoding. Defaults to 32.</p> <code>32</code> <code>query_instruct</code> <code>str</code> <p>Instruction prefix for query encoding. Defaults to \"\".</p> <code>''</code> <code>passage_instruct</code> <code>str</code> <p>Instruction prefix for passage encoding. Defaults to \"\".</p> <code>''</code> <code>model_kwargs</code> <code>dict | None</code> <p>Additional arguments to pass to the model. Defaults to None.</p> <code>None</code> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def __init__(\n    self,\n    model_name: str,\n    root: str = \"tmp\",\n    use_cache: bool = True,\n    normalize: bool = True,\n    batch_size: int = 32,\n    query_instruct: str = \"\",\n    passage_instruct: str = \"\",\n    model_kwargs: dict | None = None,\n) -&gt; None:\n    \"\"\"Initialize DPR Entity Linking Model.\n\n    Args:\n        model_name (str): Name or path of the pre-trained model to load.\n        root (str, optional): Root directory for cache storage. Defaults to \"tmp\".\n        use_cache (bool, optional): Whether to use cache for embeddings. Defaults to True.\n        normalize (bool, optional): Whether to normalize the embeddings. Defaults to True.\n        batch_size (int, optional): Batch size for encoding. Defaults to 32.\n        query_instruct (str, optional): Instruction prefix for query encoding. Defaults to \"\".\n        passage_instruct (str, optional): Instruction prefix for passage encoding. Defaults to \"\".\n        model_kwargs (dict | None, optional): Additional arguments to pass to the model. Defaults to None.\n    \"\"\"\n\n    self.model_name = model_name\n    self.use_cache = use_cache\n    self.normalize = normalize\n    self.batch_size = batch_size\n    self.root = os.path.join(root, f\"{self.model_name.replace('/', '_')}_dpr_cache\")\n    if self.use_cache and not os.path.exists(self.root):\n        os.makedirs(self.root)\n    self.model = SentenceTransformer(\n        model_name, trust_remote_code=True, model_kwargs=model_kwargs\n    )\n    self.query_instruct = query_instruct\n    self.passage_instruct = passage_instruct\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.DPRELModel.index","title":"<code>index(entity_list)</code>","text":"<p>Index a list of entities by encoding them into embeddings and optionally caching the results.</p> <p>This method processes a list of entity strings, converting them into dense vector representations using a pre-trained model. To avoid redundant computation, it implements a caching mechanism based on the MD5 hash of the input entity list.</p> <p>Parameters:</p> Name Type Description Default <code>entity_list</code> <code>list</code> <p>A list of strings representing entities to be indexed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>The method stores the embeddings in self.entity_embeddings</li> <li>If caching is enabled and a cache file exists for the given entity list,   embeddings are loaded from cache instead of being recomputed</li> <li>Cache files are stored using the MD5 hash of the concatenated entity list as filename</li> <li>Embeddings are computed on GPU if available, otherwise on CPU</li> </ul> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def index(self, entity_list: list) -&gt; None:\n    \"\"\"\n    Index a list of entities by encoding them into embeddings and optionally caching the results.\n\n    This method processes a list of entity strings, converting them into dense vector representations\n    using a pre-trained model. To avoid redundant computation, it implements a caching mechanism\n    based on the MD5 hash of the input entity list.\n\n    Args:\n        entity_list (list): A list of strings representing entities to be indexed.\n\n    Returns:\n        None\n\n    Notes:\n        - The method stores the embeddings in self.entity_embeddings\n        - If caching is enabled and a cache file exists for the given entity list,\n          embeddings are loaded from cache instead of being recomputed\n        - Cache files are stored using the MD5 hash of the concatenated entity list as filename\n        - Embeddings are computed on GPU if available, otherwise on CPU\n    \"\"\"\n    self.entity_list = entity_list\n    # Get md5 fingerprint of the whole given entity list\n    fingerprint = hashlib.md5(\"\".join(entity_list).encode()).hexdigest()\n    cache_file = f\"{self.root}/{fingerprint}.pt\"\n    if os.path.exists(cache_file):\n        self.entity_embeddings = torch.load(\n            cache_file, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n    else:\n        self.entity_embeddings = self.model.encode(\n            entity_list,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n            convert_to_tensor=True,\n            show_progress_bar=True,\n            prompt=self.passage_instruct,\n            normalize_embeddings=self.normalize,\n            batch_size=self.batch_size,\n        )\n        if self.use_cache:\n            torch.save(self.entity_embeddings, cache_file)\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel","title":"<code>NVEmbedV2ELModel</code>","text":"<p>               Bases: <code>DPRELModel</code></p> <p>A DPR-based Entity Linking model specialized for NVEmbed V2 embeddings.</p> <p>This class extends DPRELModel with specific adaptations for handling NVEmbed V2 models, including increased sequence length and right-side padding.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The underlying model with max_seq_length of 32768 and right-side padding.</p> <p>Methods:</p> Name Description <code>add_eos</code> <p>Adds EOS token to input examples.</p> <code>__call__</code> <p>Processes entity list with EOS tokens before linking.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; model = NVEmbedV2ELModel('nvidia/NV-Embed-v2', query_instruct=\"Instruct: Given a entity, retrieve entities that are semantically equivalent to the given entity\\nQuery: \")\n&gt;&gt;&gt; model.index(['Paris', 'London', 'Berlin'])\n&gt;&gt;&gt; results = model(['paris city'], topk=2)\n&gt;&gt;&gt; print(results)\n{'paris city': [{'entity': 'Paris', 'score': 0.82, 'norm_score': 1.0},\n                {'entity': 'London', 'score': 0.35, 'norm_score': 0.43}]}\n</code></pre> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>class NVEmbedV2ELModel(DPRELModel):\n    \"\"\"\n    A DPR-based Entity Linking model specialized for NVEmbed V2 embeddings.\n\n    This class extends DPRELModel with specific adaptations for handling NVEmbed V2 models,\n    including increased sequence length and right-side padding.\n\n    Attributes:\n        model: The underlying model with max_seq_length of 32768 and right-side padding.\n\n    Methods:\n        add_eos(input_examples): Adds EOS token to input examples.\n        __call__(ner_entity_list): Processes entity list with EOS tokens before linking.\n\n    Examples:\n        &gt;&gt;&gt; model = NVEmbedV2ELModel('nvidia/NV-Embed-v2', query_instruct=\\\"Instruct: Given a entity, retrieve entities that are semantically equivalent to the given entity\\\\nQuery: \\\")\n        &gt;&gt;&gt; model.index(['Paris', 'London', 'Berlin'])\n        &gt;&gt;&gt; results = model(['paris city'], topk=2)\n        &gt;&gt;&gt; print(results)\n        {'paris city': [{'entity': 'Paris', 'score': 0.82, 'norm_score': 1.0},\n                        {'entity': 'London', 'score': 0.35, 'norm_score': 0.43}]}\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the DPR Entity Linking model.\n\n        This initialization extends the base class initialization and sets specific model parameters\n        for entity linking tasks. It configures the maximum sequence length to 32768 and sets\n        the tokenizer padding side to \"right\".\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            None\n        \"\"\"\n        super().__init__(\n            *args,\n            **kwargs,\n        )\n        self.model.max_seq_length = 32768\n        self.model.tokenizer.padding_side = \"right\"\n\n    def add_eos(self, input_examples: list[str]) -&gt; list[str]:\n        \"\"\"\n        Appends EOS (End of Sequence) token to each input example in the list.\n\n        Args:\n            input_examples (list[str]): List of input text strings.\n\n        Returns:\n            list[str]: List of input texts with EOS token appended to each example.\n        \"\"\"\n        input_examples = [\n            input_example + self.model.tokenizer.eos_token\n            for input_example in input_examples\n        ]\n        return input_examples\n\n    def __call__(self, ner_entity_list: list, *args: Any, **kwargs: Any) -&gt; dict:\n        \"\"\"\n        Execute entity linking for a list of named entities.\n\n        Args:\n            ner_entity_list (list): List of named entities to be linked.\n            *args (Any): Variable length argument list.\n            **kwargs (Any): Arbitrary keyword arguments.\n\n        Returns:\n            dict: Entity linking results mapping entities to their linked entries.\n        \"\"\"\n        ner_entity_list = self.add_eos(ner_entity_list)\n        return super().__call__(ner_entity_list, *args, **kwargs)\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel.__call__","title":"<code>__call__(ner_entity_list, *args, **kwargs)</code>","text":"<p>Execute entity linking for a list of named entities.</p> <p>Parameters:</p> Name Type Description Default <code>ner_entity_list</code> <code>list</code> <p>List of named entities to be linked.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Entity linking results mapping entities to their linked entries.</p> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def __call__(self, ner_entity_list: list, *args: Any, **kwargs: Any) -&gt; dict:\n    \"\"\"\n    Execute entity linking for a list of named entities.\n\n    Args:\n        ner_entity_list (list): List of named entities to be linked.\n        *args (Any): Variable length argument list.\n        **kwargs (Any): Arbitrary keyword arguments.\n\n    Returns:\n        dict: Entity linking results mapping entities to their linked entries.\n    \"\"\"\n    ner_entity_list = self.add_eos(ner_entity_list)\n    return super().__call__(ner_entity_list, *args, **kwargs)\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the DPR Entity Linking model.</p> <p>This initialization extends the base class initialization and sets specific model parameters for entity linking tasks. It configures the maximum sequence length to 32768 and sets the tokenizer padding side to \"right\".</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def __init__(\n    self,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the DPR Entity Linking model.\n\n    This initialization extends the base class initialization and sets specific model parameters\n    for entity linking tasks. It configures the maximum sequence length to 32768 and sets\n    the tokenizer padding side to \"right\".\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        None\n    \"\"\"\n    super().__init__(\n        *args,\n        **kwargs,\n    )\n    self.model.max_seq_length = 32768\n    self.model.tokenizer.padding_side = \"right\"\n</code></pre>"},{"location":"api/kg_construnction/el_model/#gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel.add_eos","title":"<code>add_eos(input_examples)</code>","text":"<p>Appends EOS (End of Sequence) token to each input example in the list.</p> <p>Parameters:</p> Name Type Description Default <code>input_examples</code> <code>list[str]</code> <p>List of input text strings.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of input texts with EOS token appended to each example.</p> Source code in <code>gfmrag/kg_construction/entity_linking_model/dpr_el_model.py</code> Python<pre><code>def add_eos(self, input_examples: list[str]) -&gt; list[str]:\n    \"\"\"\n    Appends EOS (End of Sequence) token to each input example in the list.\n\n    Args:\n        input_examples (list[str]): List of input text strings.\n\n    Returns:\n        list[str]: List of input texts with EOS token appended to each example.\n    \"\"\"\n    input_examples = [\n        input_example + self.model.tokenizer.eos_token\n        for input_example in input_examples\n    ]\n    return input_examples\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/","title":"KG Constructor","text":""},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.BaseKGConstructor","title":"<code>gfmrag.kg_construction.BaseKGConstructor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for knowledge graph construction.</p> <p>This class defines the interface for constructing knowledge graphs from datasets. Subclasses must implement create_kg() and get_document2entities() methods.</p> <p>Methods:</p> Name Description <code>create_kg</code> <p>Creates a knowledge graph from the specified dataset.</p> <code>get_document2entities</code> <p>Get mapping between documents and their associated entities.</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>class BaseKGConstructor(ABC):\n    \"\"\"\n    Abstract base class for knowledge graph construction.\n\n    This class defines the interface for constructing knowledge graphs from datasets.\n    Subclasses must implement create_kg() and get_document2entities() methods.\n\n    Attributes:\n        None\n\n    Methods:\n        create_kg: Creates a knowledge graph from the specified dataset.\n\n        get_document2entities: Get mapping between documents and their associated entities.\n    \"\"\"\n\n    @abstractmethod\n    def create_kg(self, data_root: str, data_name: str) -&gt; list[tuple[str, str, str]]:\n        \"\"\"\n        Create a knowledge graph from the dataset\n\n        Args:\n            data_root (str): path to the dataset\n            data_name (str): name of the dataset\n\n        Returns:\n            list[tuple[str, str, str]]: list of triples\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document2entities(self, data_root: str, data_name: str) -&gt; dict:\n        \"\"\"\n        Get the document to entities mapping from the dataset\n\n        Args:\n            data_root (str): path to the dataset\n            data_name (str): name of the dataset\n\n        Returns:\n            dict: document to entities mapping\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.BaseKGConstructor.create_kg","title":"<code>create_kg(data_root, data_name)</code>  <code>abstractmethod</code>","text":"<p>Create a knowledge graph from the dataset</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>path to the dataset</p> required <code>data_name</code> <code>str</code> <p>name of the dataset</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str]]</code> <p>list[tuple[str, str, str]]: list of triples</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>@abstractmethod\ndef create_kg(self, data_root: str, data_name: str) -&gt; list[tuple[str, str, str]]:\n    \"\"\"\n    Create a knowledge graph from the dataset\n\n    Args:\n        data_root (str): path to the dataset\n        data_name (str): name of the dataset\n\n    Returns:\n        list[tuple[str, str, str]]: list of triples\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.BaseKGConstructor.get_document2entities","title":"<code>get_document2entities(data_root, data_name)</code>  <code>abstractmethod</code>","text":"<p>Get the document to entities mapping from the dataset</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>path to the dataset</p> required <code>data_name</code> <code>str</code> <p>name of the dataset</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>document to entities mapping</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>@abstractmethod\ndef get_document2entities(self, data_root: str, data_name: str) -&gt; dict:\n    \"\"\"\n    Get the document to entities mapping from the dataset\n\n    Args:\n        data_root (str): path to the dataset\n        data_name (str): name of the dataset\n\n    Returns:\n        dict: document to entities mapping\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor","title":"<code>gfmrag.kg_construction.KGConstructor</code>","text":"<p>               Bases: <code>BaseKGConstructor</code></p> <p>A class for constructing Knowledge Graphs (KG) from text data using Open Information Extraction and Entity Linking.</p> <p>Parameters:</p> Name Type Description Default <code>open_ie_model</code> <code>BaseOPENIEModel</code> <p>Model for performing Open Information Extraction</p> required <code>el_model</code> <code>BaseELModel</code> <p>Model for Entity Linking</p> required <code>root</code> <code>str</code> <p>Root directory for storing temporary files. Defaults to \"tmp/kg_construction\".</p> <code>'tmp/kg_construction'</code> <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel processing. Defaults to 1.</p> <code>1</code> <code>cosine_sim_edges</code> <code>bool</code> <p>Whether to add edges based on cosine similarity between entities. Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Similarity threshold for adding edges between similar entities. Defaults to 0.8.</p> <code>0.8</code> <code>max_sim_neighbors</code> <code>int</code> <p>Maximum number of similar neighbors to consider per entity. Defaults to 100.</p> <code>100</code> <code>add_title</code> <code>bool</code> <p>Whether to prepend document titles to passages. Defaults to True.</p> <code>True</code> <code>force</code> <code>bool</code> <p>Whether to force recomputation of cached results. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>data_name</code> <code>str</code> <p>Name of the current dataset being processed</p> <code>tmp_dir</code> <code>str</code> <p>Temporary directory for storing intermediate results</p> <p>Methods:</p> Name Description <code>from_config</code> <p>Creates a KGConstructor instance from a configuration object</p> <code>create_kg</code> <p>Creates a knowledge graph from the documents in the specified dataset</p> <code>get_document2entities</code> <p>Gets mapping of documents to their extracted entities</p> <code>open_ie_extraction</code> <p>Performs Open IE on the dataset corpus</p> <code>create_graph</code> <p>Creates a knowledge graph from Open IE results</p> <code>augment_graph</code> <p>Augments the graph with similarity-based edges</p> Notes <p>The knowledge graph is constructed in multiple steps:</p> <ol> <li>Open Information Extraction to get initial triples</li> <li>Entity Linking to normalize entities</li> <li>Optional augmentation with similarity-based edges</li> <li>Creation of the final graph structure</li> </ol> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>class KGConstructor(BaseKGConstructor):\n    \"\"\"A class for constructing Knowledge Graphs (KG) from text data using Open Information Extraction and Entity Linking.\n\n\n    Args:\n        open_ie_model (BaseOPENIEModel): Model for performing Open Information Extraction\n        el_model (BaseELModel): Model for Entity Linking\n        root (str, optional): Root directory for storing temporary files. Defaults to \"tmp/kg_construction\".\n        num_processes (int, optional): Number of processes to use for parallel processing. Defaults to 1.\n        cosine_sim_edges (bool, optional): Whether to add edges based on cosine similarity between entities. Defaults to True.\n        threshold (float, optional): Similarity threshold for adding edges between similar entities. Defaults to 0.8.\n        max_sim_neighbors (int, optional): Maximum number of similar neighbors to consider per entity. Defaults to 100.\n        add_title (bool, optional): Whether to prepend document titles to passages. Defaults to True.\n        force (bool, optional): Whether to force recomputation of cached results. Defaults to False.\n\n    Attributes:\n        data_name (str): Name of the current dataset being processed\n        tmp_dir (str): Temporary directory for storing intermediate results\n\n    Methods:\n        from_config(cfg): Creates a KGConstructor instance from a configuration object\n        create_kg(data_root, data_name): Creates a knowledge graph from the documents in the specified dataset\n        get_document2entities(data_root, data_name): Gets mapping of documents to their extracted entities\n        open_ie_extraction(raw_path): Performs Open IE on the dataset corpus\n        create_graph(open_ie_result_path): Creates a knowledge graph from Open IE results\n        augment_graph(graph, kb_phrase_dict): Augments the graph with similarity-based edges\n\n    Notes:\n        The knowledge graph is constructed in multiple steps:\n\n        1. Open Information Extraction to get initial triples\n        2. Entity Linking to normalize entities\n        3. Optional augmentation with similarity-based edges\n        4. Creation of the final graph structure\n    \"\"\"\n\n    DELIMITER = KG_DELIMITER\n\n    def __init__(\n        self,\n        open_ie_model: BaseOPENIEModel,\n        el_model: BaseELModel,\n        root: str = \"tmp/kg_construction\",\n        num_processes: int = 1,\n        cosine_sim_edges: bool = True,\n        threshold: float = 0.8,\n        max_sim_neighbors: int = 100,\n        add_title: bool = True,\n        force: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the KGConstructor class.\n\n        Args:\n            open_ie_model (BaseOPENIEModel): Model for Open Information Extraction.\n            el_model (BaseELModel): Model for Entity Linking.\n            root (str, optional): Root directory for storing KG construction outputs. Defaults to \"tmp/kg_construction\".\n            num_processes (int, optional): Number of processes for parallel processing. Defaults to 1.\n            cosine_sim_edges (bool, optional): Whether to add cosine similarity edges. Defaults to True.\n            threshold (float, optional): Similarity threshold for adding edges. Defaults to 0.8.\n            max_sim_neighbors (int, optional): Maximum number of similar neighbors to connect. Defaults to 100.\n            add_title (bool, optional): Whether to add document titles as nodes. Defaults to True.\n            force (bool, optional): Whether to force reconstruction of existing outputs. Defaults to False.\n\n        Attributes:\n            open_ie_model: Model instance for Open Information Extraction\n            el_model: Model instance for Entity Linking\n            root: Root directory path\n            num_processes: Number of parallel processes\n            cosine_sim_edges: Flag for adding similarity edges\n            threshold: Similarity threshold value\n            max_sim_neighbors: Max number of similar neighbors\n            add_title: Flag for adding document titles\n            force: Flag for forced reconstruction\n            data_name: Name of the dataset being processed\n        \"\"\"\n\n        self.open_ie_model = open_ie_model\n        self.el_model = el_model\n        self.root = root\n        self.num_processes = num_processes\n        self.cosine_sim_edges = cosine_sim_edges\n        self.threshold = threshold\n        self.max_sim_neighbors = max_sim_neighbors\n        self.add_title = add_title\n        self.force = force\n        self.data_name = None\n\n    @property\n    def tmp_dir(self) -&gt; str:\n        \"\"\"\n        Returns the temporary directory path for data processing.\n\n        This property method creates and returns a directory path specific to the current\n        data_name under the root directory. The directory is created if it doesn't exist.\n\n        Returns:\n            str: Path to the temporary directory.\n\n        Raises:\n            AssertionError: If data_name is not set before accessing this property.\n        \"\"\"\n        assert (\n            self.data_name is not None\n        )  # data_name should be set before calling this property\n        tmp_dir = os.path.join(self.root, self.data_name)\n        if not os.path.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        return tmp_dir\n\n    @staticmethod\n    def from_config(cfg: DictConfig) -&gt; \"KGConstructor\":\n        \"\"\"\n        Creates a KGConstructor instance from a configuration.\n\n        This method initializes a KGConstructor using parameters specified in an OmegaConf\n        configuration object. It creates a unique fingerprint of the configuration and sets up\n        a temporary directory for storing processed data.\n\n        Args:\n            cfg (DictConfig): An OmegaConf configuration object containing the following parameters:\n\n                - root: Base directory for storing temporary files\n                - open_ie_model: Configuration for the Open IE model\n                - el_model: Configuration for the Entity Linking model\n                - num_processes: Number of processes to use\n                - cosine_sim_edges: Whether to use cosine similarity for edges\n                - threshold: Similarity threshold\n                - max_sim_neighbors: Maximum number of similar neighbors\n                - add_title: Whether to add titles\n                - force: Whether to force reprocessing\n\n        Returns:\n            KGConstructor: An initialized KGConstructor instance\n\n        Notes:\n            The method creates a fingerprint of the configuration (excluding 'force' parameters)\n            and uses it to create a temporary directory. The configuration is saved in this\n            directory for reference.\n        \"\"\"\n        # create a fingerprint of config for tmp directory\n        config = OmegaConf.to_container(cfg, resolve=True)\n        if \"force\" in config:\n            del config[\"force\"]\n        if \"force\" in config[\"el_model\"]:\n            del config[\"el_model\"][\"force\"]\n        fingerprint = hashlib.md5(json.dumps(config).encode()).hexdigest()\n\n        base_tmp_dir = os.path.join(cfg.root, fingerprint)\n        if not os.path.exists(base_tmp_dir):\n            os.makedirs(base_tmp_dir)\n            json.dump(\n                config,\n                open(os.path.join(base_tmp_dir, \"config.json\"), \"w\"),\n                indent=4,\n            )\n        return KGConstructor(\n            root=base_tmp_dir,\n            open_ie_model=instantiate(cfg.open_ie_model),\n            el_model=instantiate(cfg.el_model),\n            num_processes=cfg.num_processes,\n            cosine_sim_edges=cfg.cosine_sim_edges,\n            threshold=cfg.threshold,\n            max_sim_neighbors=cfg.max_sim_neighbors,\n            add_title=cfg.add_title,\n            force=cfg.force,\n        )\n\n    def create_kg(self, data_root: str, data_name: str) -&gt; list[tuple[str, str, str]]:\n        \"\"\"\n        Create a knowledge graph from raw data.\n\n        This method processes raw data to extract triples and construct a knowledge graph.\n        It first performs Open IE extraction on the raw data, then creates a graph structure,\n        and finally converts the graph into a list of triples.\n\n        Args:\n            data_root (str): Root directory path containing the data.\n            data_name (str): Name of the dataset to process.\n\n        Returns:\n            list[tuple[str, str, str]]: List of extracted triples in the format (head, relation, tail).\n\n        Note:\n            If self.force is True, it will clear all temporary files before processing.\n        \"\"\"\n        # Get dataset information\n        self.data_name = data_name  # type: ignore\n        raw_path = os.path.join(data_root, data_name, \"raw\")\n\n        if self.force:\n            # Clear cache in tmp directory\n            for tmp_file in os.listdir(self.tmp_dir):\n                os.remove(os.path.join(self.tmp_dir, tmp_file))\n\n        open_ie_result_path = self.open_ie_extraction(raw_path)\n        graph = self.create_graph(open_ie_result_path)\n        extracted_triples = [(h, r, t) for (h, t), r in graph.items()]\n        return extracted_triples\n\n    def get_document2entities(self, data_root: str, data_name: str) -&gt; dict:\n        \"\"\"\n        Retrieves a mapping of document titles to their associated entities from a preprocessed dataset.\n\n        This method requires that a knowledge graph has been previously created using create_kg().\n        If the necessary files do not exist, it will automatically call create_kg() first.\n\n        Args:\n            data_root (str): Root directory containing the dataset\n            data_name (str): Name of the dataset to process\n\n        Returns:\n            dict: A dictionary mapping document titles (str) to lists of entity IDs (list)\n\n        Raises:\n            Warning: If passage information file is not found and create_kg needs to be run first\n        \"\"\"\n        # Get dataset information\n        self.data_name = data_name  # type: ignore\n\n        if not os.path.exists(os.path.join(self.tmp_dir, \"passage_info.json\")):\n            logger.warning(\n                \"Document to entities mapping is not available. Run create_kg first\"\n            )\n            self.create_kg(data_root, data_name)\n\n        with open(os.path.join(self.tmp_dir, \"passage_info.json\")) as fin:\n            passage_info = json.load(fin)\n        document2entities = {doc[\"title\"]: doc[\"entities\"] for doc in passage_info}\n        return document2entities\n\n    def open_ie_extraction(self, raw_path: str) -&gt; str:\n        \"\"\"\n        Perform open information extraction on the dataset corpus\n\n        Args:\n            raw_path (str): Path to the raw dataset\n\n        Returns:\n            str: Path to the openie results\n        \"\"\"\n        # Read data corpus\n        with open(os.path.join(raw_path, \"dataset_corpus.json\")) as f:\n            corpus = json.load(f)\n            if self.add_title:\n                corpus = {\n                    title: title + \"\\n\" + passage for title, passage in corpus.items()\n                }\n        passage_to_title = {corpus[title]: title for title in corpus.keys()}\n\n        logger.info(f\"Number of passages: {len(corpus)}\")\n\n        open_ie_result_path = f\"{self.tmp_dir}/openie_results.jsonl\"\n        open_ie_results = {}\n        # check if the openie results are already computed\n        if os.path.exists(open_ie_result_path):\n            logger.info(f\"OpenIE results already exist at {open_ie_result_path}\")\n            with open(open_ie_result_path) as f:\n                for line in f:\n                    data = json.loads(line)\n                    open_ie_results[data[\"passage\"]] = data\n\n        remining_passages = [\n            passage for passage in corpus.values() if passage not in open_ie_results\n        ]\n        logger.info(\n            f\"Number of passages which require processing: {len(remining_passages)}\"\n        )\n\n        if len(remining_passages) &gt; 0:\n            with open(open_ie_result_path, \"a\") as f:\n                with ThreadPool(processes=self.num_processes) as pool:\n                    for result in tqdm(\n                        pool.imap(self.open_ie_model, remining_passages),\n                        total=len(remining_passages),\n                        desc=\"Perform OpenIE\",\n                    ):\n                        if isinstance(result, dict):\n                            passage_title = passage_to_title[result[\"passage\"]]\n                            result[\"title\"] = passage_title\n                            f.write(json.dumps(result) + \"\\n\")\n                            f.flush()\n\n        logger.info(f\"OpenIE results saved to {open_ie_result_path}\")\n        return open_ie_result_path\n\n    def create_graph(self, open_ie_result_path: str) -&gt; dict:\n        \"\"\"\n        Create a knowledge graph from the openie results\n\n        Args:\n            open_ie_result_path (str): Path to the openie results\n\n        Returns:\n            dict: Knowledge graph\n\n                - key: (head, tail)\n                - value: relation\n        \"\"\"\n\n        with open(open_ie_result_path) as f:\n            extracted_triples = [json.loads(line) for line in f]\n\n        # Create a knowledge graph from the openie results\n        passage_json = []  # document-level information\n        phrases = []  # clean triples\n        entities = []  # entities from clean triples\n        graph = {}  # {(h, t): r}\n        incorrectly_formatted_triples = []  # those triples that len(triples) != 3\n        triples_wo_ner_entity = []  # those triples that have entities out of ner entities\n        triple_tuples = []  # all clean triples\n\n        # Step 1: process OpenIE results\n        for row in tqdm(extracted_triples, total=len(extracted_triples)):\n            ner_entities = [processing_phrases(p) for p in row[\"extracted_entities\"]]\n            triples = row[\"extracted_triples\"]\n            doc_json = row\n\n            clean_triples = []\n            unclean_triples = []\n            doc_entities = set()  # clean entities related to each sample\n\n            # Populate Triples from OpenIE\n            for triple in triples:\n                if not isinstance(triple, list) or any(\n                    isinstance(i, list) or isinstance(i, tuple) for i in triple\n                ):\n                    continue\n\n                if len(triple) &gt; 1:\n                    if len(triple) != 3:\n                        clean_triple = [processing_phrases(p) for p in triple]\n                        incorrectly_formatted_triples.append(triple)\n                        unclean_triples.append(triple)\n                    else:\n                        clean_triple = [processing_phrases(p) for p in triple]\n                        # filter triples with '' or None\n                        if \"\" in clean_triple or None in clean_triple:\n                            incorrectly_formatted_triples.append(triple)  # modify\n                            unclean_triples.append(triple)\n                            continue\n\n                        clean_triples.append(clean_triple)\n                        phrases.extend(clean_triple)\n\n                        head_ent = clean_triple[0]\n                        tail_ent = clean_triple[2]\n\n                        if (\n                            head_ent not in ner_entities\n                            and tail_ent not in ner_entities\n                        ):\n                            triples_wo_ner_entity.append(triple)\n\n                        graph[(head_ent, tail_ent)] = clean_triple[1]\n\n                        for triple_entity in [clean_triple[0], clean_triple[2]]:\n                            entities.append(triple_entity)\n                            doc_entities.add(triple_entity)\n\n                doc_json[\"entities\"] = list(set(doc_entities))\n                doc_json[\"clean_triples\"] = clean_triples\n                doc_json[\"noisy_triples\"] = unclean_triples\n                triple_tuples.append(clean_triples)\n\n                passage_json.append(doc_json)\n\n        with open(os.path.join(self.tmp_dir, \"passage_info.json\"), \"w\") as f:\n            json.dump(passage_json, f, indent=4)\n\n        logging.info(f\"Total number of processed data: {len(triple_tuples)}\")\n\n        lose_facts = []  # clean triples\n        for triples in triple_tuples:\n            lose_facts.extend([tuple(t) for t in triples])\n        lose_fact_dict = {f: i for i, f in enumerate(lose_facts)}  # triples2id\n        unique_phrases = list(np.unique(entities))  # Number of entities from documents\n        unique_relations = np.unique(\n            list(graph.values()) + [\"equivalent\"]\n        )  # Number of relations from documents\n        kb_phrase_dict = {p: i for i, p in enumerate(unique_phrases)}  # entities2id\n        # Step 2: create raw graph\n        logger.info(\"Creating Graph from OpenIE results\")\n\n        if self.cosine_sim_edges:\n            self.augment_graph(\n                graph, kb_phrase_dict=kb_phrase_dict\n            )  # combine raw graph with synonyms edges\n\n        synonymy_edges = {edge for edge in graph.keys() if graph[edge] == \"equivalent\"}\n        stat_df = [\n            (\"Total Phrases\", len(phrases)),\n            (\"Unique Phrases\", len(unique_phrases)),\n            (\"Number of Individual Triples\", len(lose_facts)),\n            (\n                \"Number of Incorrectly Formatted Triples (ChatGPT Error)\",\n                len(incorrectly_formatted_triples),\n            ),\n            (\n                \"Number of Triples w/o NER Entities (ChatGPT Error)\",\n                len(triples_wo_ner_entity),\n            ),\n            (\"Number of Unique Individual Triples\", len(lose_fact_dict)),\n            (\"Number of Entities\", len(entities)),\n            (\"Number of Edges\", len(graph)),\n            (\"Number of Unique Entities\", len(np.unique(entities))),\n            (\"Number of Synonymy Edges\", len(synonymy_edges)),\n            (\"Number of Unique Relations\", len(unique_relations)),\n        ]\n\n        logger.info(\"\\n%s\", pd.DataFrame(stat_df).set_index(0))\n\n        return graph\n\n    def augment_graph(self, graph: dict[Any, Any], kb_phrase_dict: dict) -&gt; None:\n        \"\"\"\n        Augment the graph with synonym edges between similar phrases.\n\n        This method adds \"equivalent\" edges between phrases that are semantically similar based on embeddings.\n        Similar phrases are found using an entity linking model and filtered based on similarity thresholds.\n\n        Args:\n            graph (dict[Any, Any]): The knowledge graph to augment, represented as an edge dictionary\n                where keys are (phrase1, phrase2) tuples and values are edge types\n            kb_phrase_dict (dict): Dictionary mapping phrases to their unique IDs in the knowledge base\n\n        Returns:\n            None: The graph is modified in place by adding new edges\n\n        Notes:\n            - Only processes phrases with &gt;2 alphanumeric characters\n            - Adds up to self.max_sim_neighbors equivalent edges per phrase\n            - Only adds edges for pairs with similarity score above self.threshold\n            - Uses self.el_model for computing phrase similarities\n        \"\"\"\n        logger.info(\"Augmenting graph from similarity\")\n\n        unique_phrases = list(kb_phrase_dict.keys())\n        processed_phrases = [processing_phrases(p) for p in unique_phrases]\n\n        self.el_model.index(processed_phrases)\n\n        logger.info(\"Finding similar entities\")\n        sim_neighbors = self.el_model(processed_phrases, topk=self.max_sim_neighbors)\n\n        logger.info(\"Adding synonymy edges\")\n        for phrase, neighbors in tqdm(sim_neighbors.items()):\n            synonyms = []  # [(phrase_id, score)]\n            if len(re.sub(\"[^A-Za-z0-9]\", \"\", phrase)) &gt; 2:\n                phrase_id = kb_phrase_dict[phrase]\n                if phrase_id is not None:\n                    num_nns = 0\n                    for neighbor in neighbors:\n                        n_entity = neighbor[\"entity\"]\n                        n_score = neighbor[\"norm_score\"]\n                        if n_score &lt; self.threshold or num_nns &gt; self.max_sim_neighbors:\n                            break\n                        if n_entity != phrase:\n                            phrase2_id = kb_phrase_dict[n_entity]\n                            if phrase2_id is not None:\n                                phrase2 = n_entity\n                                synonyms.append((n_entity, n_score))\n                                graph[(phrase, phrase2)] = \"equivalent\"\n                                num_nns += 1\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.tmp_dir","title":"<code>tmp_dir</code>  <code>property</code>","text":"<p>Returns the temporary directory path for data processing.</p> <p>This property method creates and returns a directory path specific to the current data_name under the root directory. The directory is created if it doesn't exist.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the temporary directory.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If data_name is not set before accessing this property.</p>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.__init__","title":"<code>__init__(open_ie_model, el_model, root='tmp/kg_construction', num_processes=1, cosine_sim_edges=True, threshold=0.8, max_sim_neighbors=100, add_title=True, force=False)</code>","text":"<p>Initialize the KGConstructor class.</p> <p>Parameters:</p> Name Type Description Default <code>open_ie_model</code> <code>BaseOPENIEModel</code> <p>Model for Open Information Extraction.</p> required <code>el_model</code> <code>BaseELModel</code> <p>Model for Entity Linking.</p> required <code>root</code> <code>str</code> <p>Root directory for storing KG construction outputs. Defaults to \"tmp/kg_construction\".</p> <code>'tmp/kg_construction'</code> <code>num_processes</code> <code>int</code> <p>Number of processes for parallel processing. Defaults to 1.</p> <code>1</code> <code>cosine_sim_edges</code> <code>bool</code> <p>Whether to add cosine similarity edges. Defaults to True.</p> <code>True</code> <code>threshold</code> <code>float</code> <p>Similarity threshold for adding edges. Defaults to 0.8.</p> <code>0.8</code> <code>max_sim_neighbors</code> <code>int</code> <p>Maximum number of similar neighbors to connect. Defaults to 100.</p> <code>100</code> <code>add_title</code> <code>bool</code> <p>Whether to add document titles as nodes. Defaults to True.</p> <code>True</code> <code>force</code> <code>bool</code> <p>Whether to force reconstruction of existing outputs. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>open_ie_model</code> <p>Model instance for Open Information Extraction</p> <code>el_model</code> <p>Model instance for Entity Linking</p> <code>root</code> <p>Root directory path</p> <code>num_processes</code> <p>Number of parallel processes</p> <code>cosine_sim_edges</code> <p>Flag for adding similarity edges</p> <code>threshold</code> <p>Similarity threshold value</p> <code>max_sim_neighbors</code> <p>Max number of similar neighbors</p> <code>add_title</code> <p>Flag for adding document titles</p> <code>force</code> <p>Flag for forced reconstruction</p> <code>data_name</code> <p>Name of the dataset being processed</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def __init__(\n    self,\n    open_ie_model: BaseOPENIEModel,\n    el_model: BaseELModel,\n    root: str = \"tmp/kg_construction\",\n    num_processes: int = 1,\n    cosine_sim_edges: bool = True,\n    threshold: float = 0.8,\n    max_sim_neighbors: int = 100,\n    add_title: bool = True,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the KGConstructor class.\n\n    Args:\n        open_ie_model (BaseOPENIEModel): Model for Open Information Extraction.\n        el_model (BaseELModel): Model for Entity Linking.\n        root (str, optional): Root directory for storing KG construction outputs. Defaults to \"tmp/kg_construction\".\n        num_processes (int, optional): Number of processes for parallel processing. Defaults to 1.\n        cosine_sim_edges (bool, optional): Whether to add cosine similarity edges. Defaults to True.\n        threshold (float, optional): Similarity threshold for adding edges. Defaults to 0.8.\n        max_sim_neighbors (int, optional): Maximum number of similar neighbors to connect. Defaults to 100.\n        add_title (bool, optional): Whether to add document titles as nodes. Defaults to True.\n        force (bool, optional): Whether to force reconstruction of existing outputs. Defaults to False.\n\n    Attributes:\n        open_ie_model: Model instance for Open Information Extraction\n        el_model: Model instance for Entity Linking\n        root: Root directory path\n        num_processes: Number of parallel processes\n        cosine_sim_edges: Flag for adding similarity edges\n        threshold: Similarity threshold value\n        max_sim_neighbors: Max number of similar neighbors\n        add_title: Flag for adding document titles\n        force: Flag for forced reconstruction\n        data_name: Name of the dataset being processed\n    \"\"\"\n\n    self.open_ie_model = open_ie_model\n    self.el_model = el_model\n    self.root = root\n    self.num_processes = num_processes\n    self.cosine_sim_edges = cosine_sim_edges\n    self.threshold = threshold\n    self.max_sim_neighbors = max_sim_neighbors\n    self.add_title = add_title\n    self.force = force\n    self.data_name = None\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.augment_graph","title":"<code>augment_graph(graph, kb_phrase_dict)</code>","text":"<p>Augment the graph with synonym edges between similar phrases.</p> <p>This method adds \"equivalent\" edges between phrases that are semantically similar based on embeddings. Similar phrases are found using an entity linking model and filtered based on similarity thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>dict[Any, Any]</code> <p>The knowledge graph to augment, represented as an edge dictionary where keys are (phrase1, phrase2) tuples and values are edge types</p> required <code>kb_phrase_dict</code> <code>dict</code> <p>Dictionary mapping phrases to their unique IDs in the knowledge base</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The graph is modified in place by adding new edges</p> Notes <ul> <li>Only processes phrases with &gt;2 alphanumeric characters</li> <li>Adds up to self.max_sim_neighbors equivalent edges per phrase</li> <li>Only adds edges for pairs with similarity score above self.threshold</li> <li>Uses self.el_model for computing phrase similarities</li> </ul> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def augment_graph(self, graph: dict[Any, Any], kb_phrase_dict: dict) -&gt; None:\n    \"\"\"\n    Augment the graph with synonym edges between similar phrases.\n\n    This method adds \"equivalent\" edges between phrases that are semantically similar based on embeddings.\n    Similar phrases are found using an entity linking model and filtered based on similarity thresholds.\n\n    Args:\n        graph (dict[Any, Any]): The knowledge graph to augment, represented as an edge dictionary\n            where keys are (phrase1, phrase2) tuples and values are edge types\n        kb_phrase_dict (dict): Dictionary mapping phrases to their unique IDs in the knowledge base\n\n    Returns:\n        None: The graph is modified in place by adding new edges\n\n    Notes:\n        - Only processes phrases with &gt;2 alphanumeric characters\n        - Adds up to self.max_sim_neighbors equivalent edges per phrase\n        - Only adds edges for pairs with similarity score above self.threshold\n        - Uses self.el_model for computing phrase similarities\n    \"\"\"\n    logger.info(\"Augmenting graph from similarity\")\n\n    unique_phrases = list(kb_phrase_dict.keys())\n    processed_phrases = [processing_phrases(p) for p in unique_phrases]\n\n    self.el_model.index(processed_phrases)\n\n    logger.info(\"Finding similar entities\")\n    sim_neighbors = self.el_model(processed_phrases, topk=self.max_sim_neighbors)\n\n    logger.info(\"Adding synonymy edges\")\n    for phrase, neighbors in tqdm(sim_neighbors.items()):\n        synonyms = []  # [(phrase_id, score)]\n        if len(re.sub(\"[^A-Za-z0-9]\", \"\", phrase)) &gt; 2:\n            phrase_id = kb_phrase_dict[phrase]\n            if phrase_id is not None:\n                num_nns = 0\n                for neighbor in neighbors:\n                    n_entity = neighbor[\"entity\"]\n                    n_score = neighbor[\"norm_score\"]\n                    if n_score &lt; self.threshold or num_nns &gt; self.max_sim_neighbors:\n                        break\n                    if n_entity != phrase:\n                        phrase2_id = kb_phrase_dict[n_entity]\n                        if phrase2_id is not None:\n                            phrase2 = n_entity\n                            synonyms.append((n_entity, n_score))\n                            graph[(phrase, phrase2)] = \"equivalent\"\n                            num_nns += 1\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.create_graph","title":"<code>create_graph(open_ie_result_path)</code>","text":"<p>Create a knowledge graph from the openie results</p> <p>Parameters:</p> Name Type Description Default <code>open_ie_result_path</code> <code>str</code> <p>Path to the openie results</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Knowledge graph</p> <ul> <li>key: (head, tail)</li> <li>value: relation</li> </ul> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def create_graph(self, open_ie_result_path: str) -&gt; dict:\n    \"\"\"\n    Create a knowledge graph from the openie results\n\n    Args:\n        open_ie_result_path (str): Path to the openie results\n\n    Returns:\n        dict: Knowledge graph\n\n            - key: (head, tail)\n            - value: relation\n    \"\"\"\n\n    with open(open_ie_result_path) as f:\n        extracted_triples = [json.loads(line) for line in f]\n\n    # Create a knowledge graph from the openie results\n    passage_json = []  # document-level information\n    phrases = []  # clean triples\n    entities = []  # entities from clean triples\n    graph = {}  # {(h, t): r}\n    incorrectly_formatted_triples = []  # those triples that len(triples) != 3\n    triples_wo_ner_entity = []  # those triples that have entities out of ner entities\n    triple_tuples = []  # all clean triples\n\n    # Step 1: process OpenIE results\n    for row in tqdm(extracted_triples, total=len(extracted_triples)):\n        ner_entities = [processing_phrases(p) for p in row[\"extracted_entities\"]]\n        triples = row[\"extracted_triples\"]\n        doc_json = row\n\n        clean_triples = []\n        unclean_triples = []\n        doc_entities = set()  # clean entities related to each sample\n\n        # Populate Triples from OpenIE\n        for triple in triples:\n            if not isinstance(triple, list) or any(\n                isinstance(i, list) or isinstance(i, tuple) for i in triple\n            ):\n                continue\n\n            if len(triple) &gt; 1:\n                if len(triple) != 3:\n                    clean_triple = [processing_phrases(p) for p in triple]\n                    incorrectly_formatted_triples.append(triple)\n                    unclean_triples.append(triple)\n                else:\n                    clean_triple = [processing_phrases(p) for p in triple]\n                    # filter triples with '' or None\n                    if \"\" in clean_triple or None in clean_triple:\n                        incorrectly_formatted_triples.append(triple)  # modify\n                        unclean_triples.append(triple)\n                        continue\n\n                    clean_triples.append(clean_triple)\n                    phrases.extend(clean_triple)\n\n                    head_ent = clean_triple[0]\n                    tail_ent = clean_triple[2]\n\n                    if (\n                        head_ent not in ner_entities\n                        and tail_ent not in ner_entities\n                    ):\n                        triples_wo_ner_entity.append(triple)\n\n                    graph[(head_ent, tail_ent)] = clean_triple[1]\n\n                    for triple_entity in [clean_triple[0], clean_triple[2]]:\n                        entities.append(triple_entity)\n                        doc_entities.add(triple_entity)\n\n            doc_json[\"entities\"] = list(set(doc_entities))\n            doc_json[\"clean_triples\"] = clean_triples\n            doc_json[\"noisy_triples\"] = unclean_triples\n            triple_tuples.append(clean_triples)\n\n            passage_json.append(doc_json)\n\n    with open(os.path.join(self.tmp_dir, \"passage_info.json\"), \"w\") as f:\n        json.dump(passage_json, f, indent=4)\n\n    logging.info(f\"Total number of processed data: {len(triple_tuples)}\")\n\n    lose_facts = []  # clean triples\n    for triples in triple_tuples:\n        lose_facts.extend([tuple(t) for t in triples])\n    lose_fact_dict = {f: i for i, f in enumerate(lose_facts)}  # triples2id\n    unique_phrases = list(np.unique(entities))  # Number of entities from documents\n    unique_relations = np.unique(\n        list(graph.values()) + [\"equivalent\"]\n    )  # Number of relations from documents\n    kb_phrase_dict = {p: i for i, p in enumerate(unique_phrases)}  # entities2id\n    # Step 2: create raw graph\n    logger.info(\"Creating Graph from OpenIE results\")\n\n    if self.cosine_sim_edges:\n        self.augment_graph(\n            graph, kb_phrase_dict=kb_phrase_dict\n        )  # combine raw graph with synonyms edges\n\n    synonymy_edges = {edge for edge in graph.keys() if graph[edge] == \"equivalent\"}\n    stat_df = [\n        (\"Total Phrases\", len(phrases)),\n        (\"Unique Phrases\", len(unique_phrases)),\n        (\"Number of Individual Triples\", len(lose_facts)),\n        (\n            \"Number of Incorrectly Formatted Triples (ChatGPT Error)\",\n            len(incorrectly_formatted_triples),\n        ),\n        (\n            \"Number of Triples w/o NER Entities (ChatGPT Error)\",\n            len(triples_wo_ner_entity),\n        ),\n        (\"Number of Unique Individual Triples\", len(lose_fact_dict)),\n        (\"Number of Entities\", len(entities)),\n        (\"Number of Edges\", len(graph)),\n        (\"Number of Unique Entities\", len(np.unique(entities))),\n        (\"Number of Synonymy Edges\", len(synonymy_edges)),\n        (\"Number of Unique Relations\", len(unique_relations)),\n    ]\n\n    logger.info(\"\\n%s\", pd.DataFrame(stat_df).set_index(0))\n\n    return graph\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.create_kg","title":"<code>create_kg(data_root, data_name)</code>","text":"<p>Create a knowledge graph from raw data.</p> <p>This method processes raw data to extract triples and construct a knowledge graph. It first performs Open IE extraction on the raw data, then creates a graph structure, and finally converts the graph into a list of triples.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory path containing the data.</p> required <code>data_name</code> <code>str</code> <p>Name of the dataset to process.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, str]]</code> <p>list[tuple[str, str, str]]: List of extracted triples in the format (head, relation, tail).</p> Note <p>If self.force is True, it will clear all temporary files before processing.</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def create_kg(self, data_root: str, data_name: str) -&gt; list[tuple[str, str, str]]:\n    \"\"\"\n    Create a knowledge graph from raw data.\n\n    This method processes raw data to extract triples and construct a knowledge graph.\n    It first performs Open IE extraction on the raw data, then creates a graph structure,\n    and finally converts the graph into a list of triples.\n\n    Args:\n        data_root (str): Root directory path containing the data.\n        data_name (str): Name of the dataset to process.\n\n    Returns:\n        list[tuple[str, str, str]]: List of extracted triples in the format (head, relation, tail).\n\n    Note:\n        If self.force is True, it will clear all temporary files before processing.\n    \"\"\"\n    # Get dataset information\n    self.data_name = data_name  # type: ignore\n    raw_path = os.path.join(data_root, data_name, \"raw\")\n\n    if self.force:\n        # Clear cache in tmp directory\n        for tmp_file in os.listdir(self.tmp_dir):\n            os.remove(os.path.join(self.tmp_dir, tmp_file))\n\n    open_ie_result_path = self.open_ie_extraction(raw_path)\n    graph = self.create_graph(open_ie_result_path)\n    extracted_triples = [(h, r, t) for (h, t), r in graph.items()]\n    return extracted_triples\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.from_config","title":"<code>from_config(cfg)</code>  <code>staticmethod</code>","text":"<p>Creates a KGConstructor instance from a configuration.</p> <p>This method initializes a KGConstructor using parameters specified in an OmegaConf configuration object. It creates a unique fingerprint of the configuration and sets up a temporary directory for storing processed data.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>An OmegaConf configuration object containing the following parameters:</p> <ul> <li>root: Base directory for storing temporary files</li> <li>open_ie_model: Configuration for the Open IE model</li> <li>el_model: Configuration for the Entity Linking model</li> <li>num_processes: Number of processes to use</li> <li>cosine_sim_edges: Whether to use cosine similarity for edges</li> <li>threshold: Similarity threshold</li> <li>max_sim_neighbors: Maximum number of similar neighbors</li> <li>add_title: Whether to add titles</li> <li>force: Whether to force reprocessing</li> </ul> required <p>Returns:</p> Name Type Description <code>KGConstructor</code> <code>KGConstructor</code> <p>An initialized KGConstructor instance</p> Notes <p>The method creates a fingerprint of the configuration (excluding 'force' parameters) and uses it to create a temporary directory. The configuration is saved in this directory for reference.</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>@staticmethod\ndef from_config(cfg: DictConfig) -&gt; \"KGConstructor\":\n    \"\"\"\n    Creates a KGConstructor instance from a configuration.\n\n    This method initializes a KGConstructor using parameters specified in an OmegaConf\n    configuration object. It creates a unique fingerprint of the configuration and sets up\n    a temporary directory for storing processed data.\n\n    Args:\n        cfg (DictConfig): An OmegaConf configuration object containing the following parameters:\n\n            - root: Base directory for storing temporary files\n            - open_ie_model: Configuration for the Open IE model\n            - el_model: Configuration for the Entity Linking model\n            - num_processes: Number of processes to use\n            - cosine_sim_edges: Whether to use cosine similarity for edges\n            - threshold: Similarity threshold\n            - max_sim_neighbors: Maximum number of similar neighbors\n            - add_title: Whether to add titles\n            - force: Whether to force reprocessing\n\n    Returns:\n        KGConstructor: An initialized KGConstructor instance\n\n    Notes:\n        The method creates a fingerprint of the configuration (excluding 'force' parameters)\n        and uses it to create a temporary directory. The configuration is saved in this\n        directory for reference.\n    \"\"\"\n    # create a fingerprint of config for tmp directory\n    config = OmegaConf.to_container(cfg, resolve=True)\n    if \"force\" in config:\n        del config[\"force\"]\n    if \"force\" in config[\"el_model\"]:\n        del config[\"el_model\"][\"force\"]\n    fingerprint = hashlib.md5(json.dumps(config).encode()).hexdigest()\n\n    base_tmp_dir = os.path.join(cfg.root, fingerprint)\n    if not os.path.exists(base_tmp_dir):\n        os.makedirs(base_tmp_dir)\n        json.dump(\n            config,\n            open(os.path.join(base_tmp_dir, \"config.json\"), \"w\"),\n            indent=4,\n        )\n    return KGConstructor(\n        root=base_tmp_dir,\n        open_ie_model=instantiate(cfg.open_ie_model),\n        el_model=instantiate(cfg.el_model),\n        num_processes=cfg.num_processes,\n        cosine_sim_edges=cfg.cosine_sim_edges,\n        threshold=cfg.threshold,\n        max_sim_neighbors=cfg.max_sim_neighbors,\n        add_title=cfg.add_title,\n        force=cfg.force,\n    )\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.get_document2entities","title":"<code>get_document2entities(data_root, data_name)</code>","text":"<p>Retrieves a mapping of document titles to their associated entities from a preprocessed dataset.</p> <p>This method requires that a knowledge graph has been previously created using create_kg(). If the necessary files do not exist, it will automatically call create_kg() first.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory containing the dataset</p> required <code>data_name</code> <code>str</code> <p>Name of the dataset to process</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary mapping document titles (str) to lists of entity IDs (list)</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If passage information file is not found and create_kg needs to be run first</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def get_document2entities(self, data_root: str, data_name: str) -&gt; dict:\n    \"\"\"\n    Retrieves a mapping of document titles to their associated entities from a preprocessed dataset.\n\n    This method requires that a knowledge graph has been previously created using create_kg().\n    If the necessary files do not exist, it will automatically call create_kg() first.\n\n    Args:\n        data_root (str): Root directory containing the dataset\n        data_name (str): Name of the dataset to process\n\n    Returns:\n        dict: A dictionary mapping document titles (str) to lists of entity IDs (list)\n\n    Raises:\n        Warning: If passage information file is not found and create_kg needs to be run first\n    \"\"\"\n    # Get dataset information\n    self.data_name = data_name  # type: ignore\n\n    if not os.path.exists(os.path.join(self.tmp_dir, \"passage_info.json\")):\n        logger.warning(\n            \"Document to entities mapping is not available. Run create_kg first\"\n        )\n        self.create_kg(data_root, data_name)\n\n    with open(os.path.join(self.tmp_dir, \"passage_info.json\")) as fin:\n        passage_info = json.load(fin)\n    document2entities = {doc[\"title\"]: doc[\"entities\"] for doc in passage_info}\n    return document2entities\n</code></pre>"},{"location":"api/kg_construnction/kg_constructor/#gfmrag.kg_construction.KGConstructor.open_ie_extraction","title":"<code>open_ie_extraction(raw_path)</code>","text":"<p>Perform open information extraction on the dataset corpus</p> <p>Parameters:</p> Name Type Description Default <code>raw_path</code> <code>str</code> <p>Path to the raw dataset</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the openie results</p> Source code in <code>gfmrag/kg_construction/kg_constructor.py</code> Python<pre><code>def open_ie_extraction(self, raw_path: str) -&gt; str:\n    \"\"\"\n    Perform open information extraction on the dataset corpus\n\n    Args:\n        raw_path (str): Path to the raw dataset\n\n    Returns:\n        str: Path to the openie results\n    \"\"\"\n    # Read data corpus\n    with open(os.path.join(raw_path, \"dataset_corpus.json\")) as f:\n        corpus = json.load(f)\n        if self.add_title:\n            corpus = {\n                title: title + \"\\n\" + passage for title, passage in corpus.items()\n            }\n    passage_to_title = {corpus[title]: title for title in corpus.keys()}\n\n    logger.info(f\"Number of passages: {len(corpus)}\")\n\n    open_ie_result_path = f\"{self.tmp_dir}/openie_results.jsonl\"\n    open_ie_results = {}\n    # check if the openie results are already computed\n    if os.path.exists(open_ie_result_path):\n        logger.info(f\"OpenIE results already exist at {open_ie_result_path}\")\n        with open(open_ie_result_path) as f:\n            for line in f:\n                data = json.loads(line)\n                open_ie_results[data[\"passage\"]] = data\n\n    remining_passages = [\n        passage for passage in corpus.values() if passage not in open_ie_results\n    ]\n    logger.info(\n        f\"Number of passages which require processing: {len(remining_passages)}\"\n    )\n\n    if len(remining_passages) &gt; 0:\n        with open(open_ie_result_path, \"a\") as f:\n            with ThreadPool(processes=self.num_processes) as pool:\n                for result in tqdm(\n                    pool.imap(self.open_ie_model, remining_passages),\n                    total=len(remining_passages),\n                    desc=\"Perform OpenIE\",\n                ):\n                    if isinstance(result, dict):\n                        passage_title = passage_to_title[result[\"passage\"]]\n                        result[\"title\"] = passage_title\n                        f.write(json.dumps(result) + \"\\n\")\n                        f.flush()\n\n    logger.info(f\"OpenIE results saved to {open_ie_result_path}\")\n    return open_ie_result_path\n</code></pre>"},{"location":"api/kg_construnction/ner_model/","title":"NER Model","text":""},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model","title":"<code>gfmrag.kg_construction.ner_model</code>","text":""},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model.BaseNERModel","title":"<code>BaseNERModel</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>gfmrag/kg_construction/ner_model/base_model.py</code> Python<pre><code>class BaseNERModel(ABC):\n    @abstractmethod\n    def __init__(self, **kwargs: Any) -&gt; None:\n        pass\n\n    @abstractmethod\n    def __call__(self, text: str) -&gt; list:\n        \"\"\"\n        This method implements the callable functionality of the class to perform Named Entity Recognition\n        on input text. When an instance of the class is called directly, this method is invoked.\n\n        Args:\n            text (str): The input text to perform NER analysis on.\n\n        Returns:\n            list: A list of named entities found in the text. Each entity is represented\n                  according to the model's output format.\n\n        Examples:\n            &gt;&gt;&gt; ner = NERModel()\n            &gt;&gt;&gt; entities = ner(\"This is a sample text\")\n            &gt;&gt;&gt; print(entities)\n            [{'text': 'sample', 'label': 'EXAMPLE'}]\n\n        Note:\n            This is an abstract method that should be implemented by subclasses.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model.BaseNERModel.__call__","title":"<code>__call__(text)</code>  <code>abstractmethod</code>","text":"<p>This method implements the callable functionality of the class to perform Named Entity Recognition on input text. When an instance of the class is called directly, this method is invoked.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to perform NER analysis on.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of named entities found in the text. Each entity is represented   according to the model's output format.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; ner = NERModel()\n&gt;&gt;&gt; entities = ner(\"This is a sample text\")\n&gt;&gt;&gt; print(entities)\n[{'text': 'sample', 'label': 'EXAMPLE'}]\n</code></pre> Note <p>This is an abstract method that should be implemented by subclasses.</p> Source code in <code>gfmrag/kg_construction/ner_model/base_model.py</code> Python<pre><code>@abstractmethod\ndef __call__(self, text: str) -&gt; list:\n    \"\"\"\n    This method implements the callable functionality of the class to perform Named Entity Recognition\n    on input text. When an instance of the class is called directly, this method is invoked.\n\n    Args:\n        text (str): The input text to perform NER analysis on.\n\n    Returns:\n        list: A list of named entities found in the text. Each entity is represented\n              according to the model's output format.\n\n    Examples:\n        &gt;&gt;&gt; ner = NERModel()\n        &gt;&gt;&gt; entities = ner(\"This is a sample text\")\n        &gt;&gt;&gt; print(entities)\n        [{'text': 'sample', 'label': 'EXAMPLE'}]\n\n    Note:\n        This is an abstract method that should be implemented by subclasses.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model.LLMNERModel","title":"<code>LLMNERModel</code>","text":"<p>               Bases: <code>BaseNERModel</code></p> <p>A Named Entity Recognition (NER) model that uses Language Models (LLMs) for entity extraction.</p> <p>This class implements entity extraction using various LLM backends (OpenAI, Together, Ollama, llama.cpp) through the Langchain interface. It processes text input and returns a list of extracted named entities.</p> <p>Parameters:</p> Name Type Description Default <code>llm_api</code> <code>Literal['openai', 'nvidia', 'together', 'ollama', 'llama.cpp']</code> <p>The LLM backend to use. Defaults to \"openai\".</p> <code>'openai'</code> <code>model_name</code> <code>str</code> <p>Name of the specific model to use. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response. Defaults to 1024.</p> <code>1024</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Extracts named entities from the input text.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error in extracting or processing named entities.</p> Source code in <code>gfmrag/kg_construction/ner_model/llm_ner_model.py</code> Python<pre><code>class LLMNERModel(BaseNERModel):\n    \"\"\"A Named Entity Recognition (NER) model that uses Language Models (LLMs) for entity extraction.\n\n    This class implements entity extraction using various LLM backends (OpenAI, Together, Ollama, llama.cpp)\n    through the Langchain interface. It processes text input and returns a list of extracted named entities.\n\n    Args:\n        llm_api (Literal[\"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM backend to use. Defaults to \"openai\".\n        model_name (str): Name of the specific model to use. Defaults to \"gpt-4o-mini\".\n        max_tokens (int): Maximum number of tokens in the response. Defaults to 1024.\n\n    Methods:\n        __call__: Extracts named entities from the input text.\n\n    Raises:\n        Exception: If there's an error in extracting or processing named entities.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_api: Literal[\n            \"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"\n        ] = \"openai\",\n        model_name: str = \"gpt-4o-mini\",\n        max_tokens: int = 1024,\n    ):\n        \"\"\"Initialize the LLM-based NER model.\n\n        Args:\n            llm_api (Literal[\"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM API provider to use.\n                Defaults to \"openai\".\n            model_name (str): Name of the language model to use.\n                Defaults to \"gpt-4o-mini\".\n            max_tokens (int): Maximum number of tokens for model output.\n                Defaults to 1024.\n        \"\"\"\n\n        self.llm_api = llm_api\n        self.model_name = model_name\n        self.max_tokens = max_tokens\n\n        self.client = init_langchain_model(llm_api, model_name)\n\n    def __call__(self, text: str) -&gt; list:\n        \"\"\"Process text input to extract named entities using different chat models.\n\n        This method handles entity extraction using various chat models (OpenAI, Ollama, LlamaCpp),\n        with special handling for JSON mode responses.\n\n        Args:\n            text (str): The input text to extract named entities from.\n\n        Returns:\n            list: A list of processed named entities extracted from the text.\n                 Returns empty list if extraction fails.\n\n        Raises:\n            None: Exceptions are caught and handled internally, logging errors when they occur.\n\n        Examples:\n            &gt;&gt;&gt; ner_model = NERModel()\n            &gt;&gt;&gt; entities = ner_model(\"Sample text with named entities\")\n            &gt;&gt;&gt; print(entities)\n            ['Entity1', 'Entity2']\n        \"\"\"\n        query_ner_prompts = ChatPromptTemplate.from_messages(\n            [\n                SystemMessage(\"You're a very effective entity extraction system.\"),\n                HumanMessage(query_prompt_one_shot_input),\n                AIMessage(query_prompt_one_shot_output),\n                HumanMessage(query_prompt_template.format(text)),\n            ]\n        )\n        query_ner_messages = query_ner_prompts.format_prompt()\n\n        json_mode = False\n        if isinstance(self.client, ChatOpenAI):  # JSON mode\n            chat_completion = self.client.invoke(\n                query_ner_messages.to_messages(),\n                temperature=0,\n                max_tokens=self.max_tokens,\n                stop=[\"\\n\\n\"],\n                response_format={\"type\": \"json_object\"},\n            )\n            response_content = chat_completion.content\n            chat_completion.response_metadata[\"token_usage\"][\"total_tokens\"]\n            json_mode = True\n        elif isinstance(self.client, ChatOllama) or isinstance(\n            self.client, ChatLlamaCpp\n        ):\n            response_content = self.client.invoke(query_ner_messages.to_messages())\n            response_content = extract_json_dict(response_content)\n            len(response_content.split())\n        else:  # no JSON mode\n            chat_completion = self.client.invoke(\n                query_ner_messages.to_messages(),\n                temperature=0,\n                max_tokens=self.max_tokens,\n                stop=[\"\\n\\n\"],\n            )\n            response_content = chat_completion.content\n            response_content = extract_json_dict(response_content)\n            chat_completion.response_metadata[\"token_usage\"][\"total_tokens\"]\n\n        if not json_mode:\n            try:\n                assert \"named_entities\" in response_content\n                response_content = str(response_content)\n            except Exception as e:\n                print(\"Query NER exception\", e)\n                response_content = {\"named_entities\": []}\n\n        try:\n            ner_list = eval(response_content)[\"named_entities\"]\n            query_ner_list = [processing_phrases(ner) for ner in ner_list]\n            return query_ner_list\n        except Exception as e:\n            logger.error(f\"Error in extracting named entities: {e}\")\n            return []\n</code></pre>"},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model.LLMNERModel.__call__","title":"<code>__call__(text)</code>","text":"<p>Process text input to extract named entities using different chat models.</p> <p>This method handles entity extraction using various chat models (OpenAI, Ollama, LlamaCpp), with special handling for JSON mode responses.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to extract named entities from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of processed named entities extracted from the text.  Returns empty list if extraction fails.</p> <p>Raises:</p> Type Description <code>None</code> <p>Exceptions are caught and handled internally, logging errors when they occur.</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; ner_model = NERModel()\n&gt;&gt;&gt; entities = ner_model(\"Sample text with named entities\")\n&gt;&gt;&gt; print(entities)\n['Entity1', 'Entity2']\n</code></pre> Source code in <code>gfmrag/kg_construction/ner_model/llm_ner_model.py</code> Python<pre><code>def __call__(self, text: str) -&gt; list:\n    \"\"\"Process text input to extract named entities using different chat models.\n\n    This method handles entity extraction using various chat models (OpenAI, Ollama, LlamaCpp),\n    with special handling for JSON mode responses.\n\n    Args:\n        text (str): The input text to extract named entities from.\n\n    Returns:\n        list: A list of processed named entities extracted from the text.\n             Returns empty list if extraction fails.\n\n    Raises:\n        None: Exceptions are caught and handled internally, logging errors when they occur.\n\n    Examples:\n        &gt;&gt;&gt; ner_model = NERModel()\n        &gt;&gt;&gt; entities = ner_model(\"Sample text with named entities\")\n        &gt;&gt;&gt; print(entities)\n        ['Entity1', 'Entity2']\n    \"\"\"\n    query_ner_prompts = ChatPromptTemplate.from_messages(\n        [\n            SystemMessage(\"You're a very effective entity extraction system.\"),\n            HumanMessage(query_prompt_one_shot_input),\n            AIMessage(query_prompt_one_shot_output),\n            HumanMessage(query_prompt_template.format(text)),\n        ]\n    )\n    query_ner_messages = query_ner_prompts.format_prompt()\n\n    json_mode = False\n    if isinstance(self.client, ChatOpenAI):  # JSON mode\n        chat_completion = self.client.invoke(\n            query_ner_messages.to_messages(),\n            temperature=0,\n            max_tokens=self.max_tokens,\n            stop=[\"\\n\\n\"],\n            response_format={\"type\": \"json_object\"},\n        )\n        response_content = chat_completion.content\n        chat_completion.response_metadata[\"token_usage\"][\"total_tokens\"]\n        json_mode = True\n    elif isinstance(self.client, ChatOllama) or isinstance(\n        self.client, ChatLlamaCpp\n    ):\n        response_content = self.client.invoke(query_ner_messages.to_messages())\n        response_content = extract_json_dict(response_content)\n        len(response_content.split())\n    else:  # no JSON mode\n        chat_completion = self.client.invoke(\n            query_ner_messages.to_messages(),\n            temperature=0,\n            max_tokens=self.max_tokens,\n            stop=[\"\\n\\n\"],\n        )\n        response_content = chat_completion.content\n        response_content = extract_json_dict(response_content)\n        chat_completion.response_metadata[\"token_usage\"][\"total_tokens\"]\n\n    if not json_mode:\n        try:\n            assert \"named_entities\" in response_content\n            response_content = str(response_content)\n        except Exception as e:\n            print(\"Query NER exception\", e)\n            response_content = {\"named_entities\": []}\n\n    try:\n        ner_list = eval(response_content)[\"named_entities\"]\n        query_ner_list = [processing_phrases(ner) for ner in ner_list]\n        return query_ner_list\n    except Exception as e:\n        logger.error(f\"Error in extracting named entities: {e}\")\n        return []\n</code></pre>"},{"location":"api/kg_construnction/ner_model/#gfmrag.kg_construction.ner_model.LLMNERModel.__init__","title":"<code>__init__(llm_api='openai', model_name='gpt-4o-mini', max_tokens=1024)</code>","text":"<p>Initialize the LLM-based NER model.</p> <p>Parameters:</p> Name Type Description Default <code>llm_api</code> <code>Literal['openai', 'nvidia', 'together', 'ollama', 'llama.cpp']</code> <p>The LLM API provider to use. Defaults to \"openai\".</p> <code>'openai'</code> <code>model_name</code> <code>str</code> <p>Name of the language model to use. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens for model output. Defaults to 1024.</p> <code>1024</code> Source code in <code>gfmrag/kg_construction/ner_model/llm_ner_model.py</code> Python<pre><code>def __init__(\n    self,\n    llm_api: Literal[\n        \"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"\n    ] = \"openai\",\n    model_name: str = \"gpt-4o-mini\",\n    max_tokens: int = 1024,\n):\n    \"\"\"Initialize the LLM-based NER model.\n\n    Args:\n        llm_api (Literal[\"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM API provider to use.\n            Defaults to \"openai\".\n        model_name (str): Name of the language model to use.\n            Defaults to \"gpt-4o-mini\".\n        max_tokens (int): Maximum number of tokens for model output.\n            Defaults to 1024.\n    \"\"\"\n\n    self.llm_api = llm_api\n    self.model_name = model_name\n    self.max_tokens = max_tokens\n\n    self.client = init_langchain_model(llm_api, model_name)\n</code></pre>"},{"location":"api/kg_construnction/openie_model/","title":"OpenIE Model","text":""},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model","title":"<code>gfmrag.kg_construction.openie_model</code>","text":""},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.BaseOPENIEModel","title":"<code>BaseOPENIEModel</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>gfmrag/kg_construction/openie_model/base_model.py</code> Python<pre><code>class BaseOPENIEModel(ABC):\n    @abstractmethod\n    def __init__(self, **kwargs: Any) -&gt; None:\n        pass\n\n    @abstractmethod\n    def __call__(self, text: str) -&gt; dict:\n        \"\"\"\n        Perform OpenIE on the given text.\n\n        Args:\n            text (str): input text\n\n        Returns:\n            dict: dict of passage, extracted entities, extracted_triples\n\n                - passage (str): input text\n                - extracted_entities (list): list of extracted entities\n                - extracted_triples (list): list of extracted triples\n\n        Examples:\n            &gt;&gt;&gt; openie_model = OPENIEModel()\n            &gt;&gt;&gt; result = openie_model(\"Emmanuel Macron is the president of France\")\n            &gt;&gt;&gt; print(result)\n            {'passage': 'Emmanuel Macron is the president of France', 'extracted_entities': ['Emmanuel Macron', 'France'], 'extracted_triples': [['Emmanuel Macron', 'president of', 'France']]}\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.BaseOPENIEModel.__call__","title":"<code>__call__(text)</code>  <code>abstractmethod</code>","text":"<p>Perform OpenIE on the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict of passage, extracted entities, extracted_triples</p> <ul> <li>passage (str): input text</li> <li>extracted_entities (list): list of extracted entities</li> <li>extracted_triples (list): list of extracted triples</li> </ul> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; openie_model = OPENIEModel()\n&gt;&gt;&gt; result = openie_model(\"Emmanuel Macron is the president of France\")\n&gt;&gt;&gt; print(result)\n{'passage': 'Emmanuel Macron is the president of France', 'extracted_entities': ['Emmanuel Macron', 'France'], 'extracted_triples': [['Emmanuel Macron', 'president of', 'France']]}\n</code></pre> Source code in <code>gfmrag/kg_construction/openie_model/base_model.py</code> Python<pre><code>@abstractmethod\ndef __call__(self, text: str) -&gt; dict:\n    \"\"\"\n    Perform OpenIE on the given text.\n\n    Args:\n        text (str): input text\n\n    Returns:\n        dict: dict of passage, extracted entities, extracted_triples\n\n            - passage (str): input text\n            - extracted_entities (list): list of extracted entities\n            - extracted_triples (list): list of extracted triples\n\n    Examples:\n        &gt;&gt;&gt; openie_model = OPENIEModel()\n        &gt;&gt;&gt; result = openie_model(\"Emmanuel Macron is the president of France\")\n        &gt;&gt;&gt; print(result)\n        {'passage': 'Emmanuel Macron is the president of France', 'extracted_entities': ['Emmanuel Macron', 'France'], 'extracted_triples': [['Emmanuel Macron', 'president of', 'France']]}\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.LLMOPENIEModel","title":"<code>LLMOPENIEModel</code>","text":"<p>               Bases: <code>BaseOPENIEModel</code></p> <p>A class for performing Open Information Extraction (OpenIE) using Large Language Models.</p> <p>This class implements OpenIE functionality by performing Named Entity Recognition (NER) and relation extraction using various LLM backends like OpenAI, Together, Ollama, or llama.cpp.</p> <p>Parameters:</p> Name Type Description Default <code>llm_api</code> <code>Literal['openai', 'together', 'ollama', 'llama.cpp']</code> <p>The LLM backend to use. Defaults to \"openai\".</p> <code>'openai'</code> <code>model_name</code> <code>str</code> <p>Name of the specific model to use. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>max_ner_tokens</code> <code>int</code> <p>Maximum number of tokens for NER output. Defaults to 1024.</p> <code>1024</code> <code>max_triples_tokens</code> <code>int</code> <p>Maximum number of tokens for relation triples output. Defaults to 4096.</p> <code>4096</code> <p>Attributes:</p> Name Type Description <code>llm_api</code> <p>The LLM backend being used</p> <code>model_name</code> <p>Name of the model being used</p> <code>max_ner_tokens</code> <p>Token limit for NER</p> <code>max_triples_tokens</code> <p>Token limit for relation triples</p> <code>client</code> <p>Initialized language model client</p> <p>Methods:</p> Name Description <code>ner</code> <p>Performs Named Entity Recognition on input text</p> <code>openie_post_ner_extract</code> <p>Extracts relation triples after NER</p> <code>__call__</code> <p>Main method to perform complete OpenIE pipeline</p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; openie_model = LLMOPENIEModel()\n&gt;&gt;&gt; result = openie_model(\"Emmanuel Macron is the president of France\")\n&gt;&gt;&gt; print(result)\n{'passage': 'Emmanuel Macron is the president of France', 'extracted_entities': ['Emmanuel Macron', 'France'], 'extracted_triples': [['Emmanuel Macron', 'president of', 'France']]}\n</code></pre> Source code in <code>gfmrag/kg_construction/openie_model/llm_openie_model.py</code> Python<pre><code>class LLMOPENIEModel(BaseOPENIEModel):\n    \"\"\"\n    A class for performing Open Information Extraction (OpenIE) using Large Language Models.\n\n    This class implements OpenIE functionality by performing Named Entity Recognition (NER)\n    and relation extraction using various LLM backends like OpenAI, Together, Ollama, or llama.cpp.\n\n    Args:\n        llm_api (Literal[\"openai\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM backend to use.\n            Defaults to \"openai\".\n        model_name (str): Name of the specific model to use. Defaults to \"gpt-4o-mini\".\n        max_ner_tokens (int): Maximum number of tokens for NER output. Defaults to 1024.\n        max_triples_tokens (int): Maximum number of tokens for relation triples output.\n            Defaults to 4096.\n\n    Attributes:\n        llm_api: The LLM backend being used\n        model_name: Name of the model being used\n        max_ner_tokens: Token limit for NER\n        max_triples_tokens: Token limit for relation triples\n        client: Initialized language model client\n\n    Methods:\n        ner: Performs Named Entity Recognition on input text\n        openie_post_ner_extract: Extracts relation triples after NER\n        __call__: Main method to perform complete OpenIE pipeline\n\n    Examples:\n        &gt;&gt;&gt; openie_model = LLMOPENIEModel()\n        &gt;&gt;&gt; result = openie_model(\"Emmanuel Macron is the president of France\")\n        &gt;&gt;&gt; print(result)\n        {'passage': 'Emmanuel Macron is the president of France', 'extracted_entities': ['Emmanuel Macron', 'France'], 'extracted_triples': [['Emmanuel Macron', 'president of', 'France']]}\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_api: Literal[\n            \"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"\n        ] = \"openai\",\n        model_name: str = \"gpt-4o-mini\",\n        max_ner_tokens: int = 1024,\n        max_triples_tokens: int = 4096,\n    ):\n        \"\"\"Initialize LLM-based OpenIE model.\n\n        Args:\n            llm_api (Literal[\"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM API provider to use.\n                Defaults to \"openai\".\n            model_name (str): Name of the language model to use. Defaults to \"gpt-4o-mini\".\n            max_ner_tokens (int): Maximum number of tokens for NER processing. Defaults to 1024.\n            max_triples_tokens (int): Maximum number of tokens for triple extraction. Defaults to 4096.\n\n        Attributes:\n            llm_api: The selected LLM API provider\n            model_name: Name of the language model\n            max_ner_tokens: Token limit for NER\n            max_triples_tokens: Token limit for triples\n            client: Initialized language model client\n        \"\"\"\n        self.llm_api = llm_api\n        self.model_name = model_name\n        self.max_ner_tokens = max_ner_tokens\n        self.max_triples_tokens = max_triples_tokens\n\n        self.client = init_langchain_model(llm_api, model_name)\n\n    def ner(self, text: str) -&gt; list:\n        \"\"\"\n        Performs Named Entity Recognition (NER) on the input text using different LLM clients.\n\n        Args:\n            text (str): Input text to extract named entities from.\n\n        Returns:\n            list: A list of named entities extracted from the text. Returns empty list if extraction fails.\n\n        Note:\n            - For OpenAI client, uses JSON mode with specific parameters\n            - For Ollama and LlamaCpp clients, extracts JSON from regular response\n            - For other clients, extracts JSON from regular response without JSON mode\n            - Handles exceptions by returning empty list and logging error\n        \"\"\"\n        ner_messages = ner_prompts.format_prompt(user_input=text)\n\n        try:\n            if isinstance(self.client, ChatOpenAI):  # JSON mode\n                chat_completion = self.client.invoke(\n                    ner_messages.to_messages(),\n                    temperature=0,\n                    max_tokens=self.max_ner_tokens,\n                    stop=[\"\\n\\n\"],\n                    response_format={\"type\": \"json_object\"},\n                )\n                response_content = chat_completion.content\n                response_content = eval(response_content)\n\n            elif isinstance(self.client, ChatOllama) or isinstance(\n                self.client, ChatLlamaCpp\n            ):\n                response_content = self.client.invoke(\n                    ner_messages.to_messages()\n                ).content\n                response_content = extract_json_dict(response_content)\n\n            else:  # no JSON mode\n                chat_completion = self.client.invoke(\n                    ner_messages.to_messages(), temperature=0\n                )\n                response_content = chat_completion.content\n                response_content = extract_json_dict(response_content)\n\n            if \"named_entities\" not in response_content:\n                response_content = []\n            else:\n                response_content = response_content[\"named_entities\"]\n\n        except Exception as e:\n            logger.error(f\"Error in extracting named entities: {e}\")\n            response_content = []\n\n        return response_content\n\n    def openie_post_ner_extract(self, text: str, entities: list) -&gt; str:\n        \"\"\"\n        Extracts open information (triples) from text using LLM, considering pre-identified named entities.\n\n        Args:\n            text (str): The input text to extract information from.\n            entities (list): List of pre-identified named entities in the text.\n\n        Returns:\n            str: JSON string containing the extracted triples. Returns empty JSON object \"{}\" if extraction fails.\n\n        Raises:\n            Exception: Logs any errors that occur during the extraction process.\n\n        Notes:\n            - For ChatOpenAI client, uses JSON mode for structured output\n            - For ChatOllama and ChatLlamaCpp clients, extracts JSON from unstructured response\n            - For other clients, extracts JSON from response content\n            - Uses temperature=0 and configured max_tokens for consistent outputs\n        \"\"\"\n        named_entity_json = {\"named_entities\": entities}\n        openie_messages = openie_post_ner_prompts.format_prompt(\n            passage=text, named_entity_json=json.dumps(named_entity_json)\n        )\n        try:\n            if isinstance(self.client, ChatOpenAI):  # JSON mode\n                chat_completion = self.client.invoke(\n                    openie_messages.to_messages(),\n                    temperature=0,\n                    max_tokens=self.max_triples_tokens,\n                    response_format={\"type\": \"json_object\"},\n                )\n                response_content = chat_completion.content\n\n            elif isinstance(self.client, ChatOllama) or isinstance(\n                self.client, ChatLlamaCpp\n            ):\n                response_content = self.client.invoke(\n                    openie_messages.to_messages()\n                ).content\n                response_content = extract_json_dict(response_content)\n                response_content = str(response_content)\n            else:  # no JSON mode\n                chat_completion = self.client.invoke(\n                    openie_messages.to_messages(),\n                    temperature=0,\n                    max_tokens=self.max_triples_tokens,\n                )\n                response_content = chat_completion.content\n                response_content = extract_json_dict(response_content)\n                response_content = str(response_content)\n\n        except Exception as e:\n            logger.error(f\"Error in OpenIE: {e}\")\n            response_content = \"{}\"\n\n        return response_content\n\n    def __call__(self, text: str) -&gt; dict:\n        \"\"\"\n        Perform OpenIE on the given text.\n\n        Args:\n            text (str): input text\n\n        Returns:\n            dict: dict of passage, extracted entities, extracted_triples\n\n                - passage (str): input text\n                - extracted_entities (list): list of extracted entities\n                - extracted_triples (list): list of extracted triples\n        \"\"\"\n        res = {\"passage\": text, \"extracted_entities\": [], \"extracted_triples\": []}\n\n        # ner_messages = ner_prompts.format_prompt(user_input=text)\n        doc_entities = self.ner(text)\n        try:\n            doc_entities = list(np.unique(doc_entities))\n        except Exception as e:\n            logger.error(f\"Results has nested lists: {e}\")\n            doc_entities = list(np.unique(list(chain.from_iterable(doc_entities))))\n        if not doc_entities:\n            logger.warning(\n                \"No entities extracted. Possibly model not following instructions\"\n            )\n        triples = self.openie_post_ner_extract(text, doc_entities)\n        res[\"extracted_entities\"] = doc_entities\n        try:\n            res[\"extracted_triples\"] = eval(triples)[\"triples\"]\n        except Exception:\n            logger.error(f\"Error in parsing triples: {triples}\")\n\n        return res\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.LLMOPENIEModel.__call__","title":"<code>__call__(text)</code>","text":"<p>Perform OpenIE on the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input text</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict of passage, extracted entities, extracted_triples</p> <ul> <li>passage (str): input text</li> <li>extracted_entities (list): list of extracted entities</li> <li>extracted_triples (list): list of extracted triples</li> </ul> Source code in <code>gfmrag/kg_construction/openie_model/llm_openie_model.py</code> Python<pre><code>def __call__(self, text: str) -&gt; dict:\n    \"\"\"\n    Perform OpenIE on the given text.\n\n    Args:\n        text (str): input text\n\n    Returns:\n        dict: dict of passage, extracted entities, extracted_triples\n\n            - passage (str): input text\n            - extracted_entities (list): list of extracted entities\n            - extracted_triples (list): list of extracted triples\n    \"\"\"\n    res = {\"passage\": text, \"extracted_entities\": [], \"extracted_triples\": []}\n\n    # ner_messages = ner_prompts.format_prompt(user_input=text)\n    doc_entities = self.ner(text)\n    try:\n        doc_entities = list(np.unique(doc_entities))\n    except Exception as e:\n        logger.error(f\"Results has nested lists: {e}\")\n        doc_entities = list(np.unique(list(chain.from_iterable(doc_entities))))\n    if not doc_entities:\n        logger.warning(\n            \"No entities extracted. Possibly model not following instructions\"\n        )\n    triples = self.openie_post_ner_extract(text, doc_entities)\n    res[\"extracted_entities\"] = doc_entities\n    try:\n        res[\"extracted_triples\"] = eval(triples)[\"triples\"]\n    except Exception:\n        logger.error(f\"Error in parsing triples: {triples}\")\n\n    return res\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.LLMOPENIEModel.__init__","title":"<code>__init__(llm_api='openai', model_name='gpt-4o-mini', max_ner_tokens=1024, max_triples_tokens=4096)</code>","text":"<p>Initialize LLM-based OpenIE model.</p> <p>Parameters:</p> Name Type Description Default <code>llm_api</code> <code>Literal['openai', 'nvidia', 'together', 'ollama', 'llama.cpp']</code> <p>The LLM API provider to use. Defaults to \"openai\".</p> <code>'openai'</code> <code>model_name</code> <code>str</code> <p>Name of the language model to use. Defaults to \"gpt-4o-mini\".</p> <code>'gpt-4o-mini'</code> <code>max_ner_tokens</code> <code>int</code> <p>Maximum number of tokens for NER processing. Defaults to 1024.</p> <code>1024</code> <code>max_triples_tokens</code> <code>int</code> <p>Maximum number of tokens for triple extraction. Defaults to 4096.</p> <code>4096</code> <p>Attributes:</p> Name Type Description <code>llm_api</code> <p>The selected LLM API provider</p> <code>model_name</code> <p>Name of the language model</p> <code>max_ner_tokens</code> <p>Token limit for NER</p> <code>max_triples_tokens</code> <p>Token limit for triples</p> <code>client</code> <p>Initialized language model client</p> Source code in <code>gfmrag/kg_construction/openie_model/llm_openie_model.py</code> Python<pre><code>def __init__(\n    self,\n    llm_api: Literal[\n        \"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"\n    ] = \"openai\",\n    model_name: str = \"gpt-4o-mini\",\n    max_ner_tokens: int = 1024,\n    max_triples_tokens: int = 4096,\n):\n    \"\"\"Initialize LLM-based OpenIE model.\n\n    Args:\n        llm_api (Literal[\"openai\", \"nvidia\", \"together\", \"ollama\", \"llama.cpp\"]): The LLM API provider to use.\n            Defaults to \"openai\".\n        model_name (str): Name of the language model to use. Defaults to \"gpt-4o-mini\".\n        max_ner_tokens (int): Maximum number of tokens for NER processing. Defaults to 1024.\n        max_triples_tokens (int): Maximum number of tokens for triple extraction. Defaults to 4096.\n\n    Attributes:\n        llm_api: The selected LLM API provider\n        model_name: Name of the language model\n        max_ner_tokens: Token limit for NER\n        max_triples_tokens: Token limit for triples\n        client: Initialized language model client\n    \"\"\"\n    self.llm_api = llm_api\n    self.model_name = model_name\n    self.max_ner_tokens = max_ner_tokens\n    self.max_triples_tokens = max_triples_tokens\n\n    self.client = init_langchain_model(llm_api, model_name)\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.LLMOPENIEModel.ner","title":"<code>ner(text)</code>","text":"<p>Performs Named Entity Recognition (NER) on the input text using different LLM clients.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to extract named entities from.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of named entities extracted from the text. Returns empty list if extraction fails.</p> Note <ul> <li>For OpenAI client, uses JSON mode with specific parameters</li> <li>For Ollama and LlamaCpp clients, extracts JSON from regular response</li> <li>For other clients, extracts JSON from regular response without JSON mode</li> <li>Handles exceptions by returning empty list and logging error</li> </ul> Source code in <code>gfmrag/kg_construction/openie_model/llm_openie_model.py</code> Python<pre><code>def ner(self, text: str) -&gt; list:\n    \"\"\"\n    Performs Named Entity Recognition (NER) on the input text using different LLM clients.\n\n    Args:\n        text (str): Input text to extract named entities from.\n\n    Returns:\n        list: A list of named entities extracted from the text. Returns empty list if extraction fails.\n\n    Note:\n        - For OpenAI client, uses JSON mode with specific parameters\n        - For Ollama and LlamaCpp clients, extracts JSON from regular response\n        - For other clients, extracts JSON from regular response without JSON mode\n        - Handles exceptions by returning empty list and logging error\n    \"\"\"\n    ner_messages = ner_prompts.format_prompt(user_input=text)\n\n    try:\n        if isinstance(self.client, ChatOpenAI):  # JSON mode\n            chat_completion = self.client.invoke(\n                ner_messages.to_messages(),\n                temperature=0,\n                max_tokens=self.max_ner_tokens,\n                stop=[\"\\n\\n\"],\n                response_format={\"type\": \"json_object\"},\n            )\n            response_content = chat_completion.content\n            response_content = eval(response_content)\n\n        elif isinstance(self.client, ChatOllama) or isinstance(\n            self.client, ChatLlamaCpp\n        ):\n            response_content = self.client.invoke(\n                ner_messages.to_messages()\n            ).content\n            response_content = extract_json_dict(response_content)\n\n        else:  # no JSON mode\n            chat_completion = self.client.invoke(\n                ner_messages.to_messages(), temperature=0\n            )\n            response_content = chat_completion.content\n            response_content = extract_json_dict(response_content)\n\n        if \"named_entities\" not in response_content:\n            response_content = []\n        else:\n            response_content = response_content[\"named_entities\"]\n\n    except Exception as e:\n        logger.error(f\"Error in extracting named entities: {e}\")\n        response_content = []\n\n    return response_content\n</code></pre>"},{"location":"api/kg_construnction/openie_model/#gfmrag.kg_construction.openie_model.LLMOPENIEModel.openie_post_ner_extract","title":"<code>openie_post_ner_extract(text, entities)</code>","text":"<p>Extracts open information (triples) from text using LLM, considering pre-identified named entities.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to extract information from.</p> required <code>entities</code> <code>list</code> <p>List of pre-identified named entities in the text.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string containing the extracted triples. Returns empty JSON object \"{}\" if extraction fails.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Logs any errors that occur during the extraction process.</p> Notes <ul> <li>For ChatOpenAI client, uses JSON mode for structured output</li> <li>For ChatOllama and ChatLlamaCpp clients, extracts JSON from unstructured response</li> <li>For other clients, extracts JSON from response content</li> <li>Uses temperature=0 and configured max_tokens for consistent outputs</li> </ul> Source code in <code>gfmrag/kg_construction/openie_model/llm_openie_model.py</code> Python<pre><code>def openie_post_ner_extract(self, text: str, entities: list) -&gt; str:\n    \"\"\"\n    Extracts open information (triples) from text using LLM, considering pre-identified named entities.\n\n    Args:\n        text (str): The input text to extract information from.\n        entities (list): List of pre-identified named entities in the text.\n\n    Returns:\n        str: JSON string containing the extracted triples. Returns empty JSON object \"{}\" if extraction fails.\n\n    Raises:\n        Exception: Logs any errors that occur during the extraction process.\n\n    Notes:\n        - For ChatOpenAI client, uses JSON mode for structured output\n        - For ChatOllama and ChatLlamaCpp clients, extracts JSON from unstructured response\n        - For other clients, extracts JSON from response content\n        - Uses temperature=0 and configured max_tokens for consistent outputs\n    \"\"\"\n    named_entity_json = {\"named_entities\": entities}\n    openie_messages = openie_post_ner_prompts.format_prompt(\n        passage=text, named_entity_json=json.dumps(named_entity_json)\n    )\n    try:\n        if isinstance(self.client, ChatOpenAI):  # JSON mode\n            chat_completion = self.client.invoke(\n                openie_messages.to_messages(),\n                temperature=0,\n                max_tokens=self.max_triples_tokens,\n                response_format={\"type\": \"json_object\"},\n            )\n            response_content = chat_completion.content\n\n        elif isinstance(self.client, ChatOllama) or isinstance(\n            self.client, ChatLlamaCpp\n        ):\n            response_content = self.client.invoke(\n                openie_messages.to_messages()\n            ).content\n            response_content = extract_json_dict(response_content)\n            response_content = str(response_content)\n        else:  # no JSON mode\n            chat_completion = self.client.invoke(\n                openie_messages.to_messages(),\n                temperature=0,\n                max_tokens=self.max_triples_tokens,\n            )\n            response_content = chat_completion.content\n            response_content = extract_json_dict(response_content)\n            response_content = str(response_content)\n\n    except Exception as e:\n        logger.error(f\"Error in OpenIE: {e}\")\n        response_content = \"{}\"\n\n    return response_content\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/","title":"QA Constructor","text":""},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.BaseQAConstructor","title":"<code>gfmrag.kg_construction.BaseQAConstructor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for constructing Question-Answering (QA) datasets.</p> <p>Methods:</p> Name Description <code>prepare_data</code> <p>Abstract method that must be implemented by subclasses to prepare QA data. Takes data location parameters and returns processed data as a list of dictionaries.</p> Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>class BaseQAConstructor(ABC):\n    \"\"\"An abstract base class for constructing Question-Answering (QA) datasets.\n\n    Attributes:\n        None\n\n    Methods:\n        prepare_data:\n            Abstract method that must be implemented by subclasses to prepare QA data.\n            Takes data location parameters and returns processed data as a list of dictionaries.\n\n    \"\"\"\n\n    @abstractmethod\n    def prepare_data(self, data_root: str, data_name: str, file: str) -&gt; list[dict]:\n        \"\"\"\n        Prepare QA data for training and evaluation\n\n        Args:\n            data_root (str): path to the dataset\n            data_name (str): name of the dataset\n            file (str): file name to process\n        Returns:\n            list[dict]: list of processed data\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.BaseQAConstructor.prepare_data","title":"<code>prepare_data(data_root, data_name, file)</code>  <code>abstractmethod</code>","text":"<p>Prepare QA data for training and evaluation</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>path to the dataset</p> required <code>data_name</code> <code>str</code> <p>name of the dataset</p> required <code>file</code> <code>str</code> <p>file name to process</p> required Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>@abstractmethod\ndef prepare_data(self, data_root: str, data_name: str, file: str) -&gt; list[dict]:\n    \"\"\"\n    Prepare QA data for training and evaluation\n\n    Args:\n        data_root (str): path to the dataset\n        data_name (str): name of the dataset\n        file (str): file name to process\n    Returns:\n        list[dict]: list of processed data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.QAConstructor","title":"<code>gfmrag.kg_construction.QAConstructor</code>","text":"<p>               Bases: <code>BaseQAConstructor</code></p> <p>QA Constructor for building question-answer datasets with entity linking and named entity recognition.</p> <p>This class processes raw QA datasets by performing Named Entity Recognition (NER) on questions and Entity Linking (EL) to connect identified entities to a knowledge graph (KG).</p> <p>Parameters:</p> Name Type Description Default <code>ner_model</code> <code>BaseNERModel</code> <p>Model for Named Entity Recognition</p> required <code>el_model</code> <code>BaseELModel</code> <p>Model for Entity Linking</p> required <code>root</code> <code>str</code> <p>Root directory for temporary files. Defaults to \"tmp/qa_construction\"</p> <code>'tmp/qa_construction'</code> <code>num_processes</code> <code>int</code> <p>Number of processes for parallel processing. Defaults to 1</p> <code>1</code> <code>force</code> <code>bool</code> <p>Whether to force recomputation of cached results. Defaults to False</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>ner_model</code> <p>The NER model instance</p> <code>el_model</code> <p>The EL model instance</p> <code>root</code> <p>Root directory path</p> <code>num_processes</code> <p>Number of parallel processes</p> <code>data_name</code> <p>Name of the current dataset being processed</p> <code>force</code> <p>Whether to force recompute results</p> <code>DELIMITER</code> <p>Delimiter used in knowledge graph files</p> <p>Methods:</p> Name Description <code>from_config</code> <p>Creates a QAConstructor instance from a configuration</p> <code>prepare_data</code> <p>Processes raw QA data to add entity information</p> <p>The class expects a knowledge graph and document-to-entities mapping to be pre-computed and stored in the processed/stage1 directory of the dataset.</p> Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>class QAConstructor(BaseQAConstructor):\n    \"\"\"QA Constructor for building question-answer datasets with entity linking and named entity recognition.\n\n    This class processes raw QA datasets by performing Named Entity Recognition (NER) on questions\n    and Entity Linking (EL) to connect identified entities to a knowledge graph (KG).\n\n    Args:\n        ner_model (BaseNERModel): Model for Named Entity Recognition\n        el_model (BaseELModel): Model for Entity Linking\n        root (str, optional): Root directory for temporary files. Defaults to \"tmp/qa_construction\"\n        num_processes (int, optional): Number of processes for parallel processing. Defaults to 1\n        force (bool, optional): Whether to force recomputation of cached results. Defaults to False\n\n    Attributes:\n        ner_model: The NER model instance\n        el_model: The EL model instance\n        root: Root directory path\n        num_processes: Number of parallel processes\n        data_name: Name of the current dataset being processed\n        force: Whether to force recompute results\n        DELIMITER: Delimiter used in knowledge graph files\n\n    Methods:\n        from_config: Creates a QAConstructor instance from a configuration\n        prepare_data: Processes raw QA data to add entity information\n\n    The class expects a knowledge graph and document-to-entities mapping to be pre-computed\n    and stored in the processed/stage1 directory of the dataset.\n    \"\"\"\n\n    DELIMITER = KG_DELIMITER\n\n    def __init__(\n        self,\n        ner_model: BaseNERModel,\n        el_model: BaseELModel,\n        root: str = \"tmp/qa_construction\",\n        num_processes: int = 1,\n        force: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the Question Answer Constructor.\n\n        This constructor processes text data through Named Entity Recognition (NER) and Entity Linking (EL) models\n        to generate question-answer pairs.\n\n        Args:\n            ner_model (BaseNERModel): Model for Named Entity Recognition.\n            el_model (BaseELModel): Model for Entity Linking.\n            root (str, optional): Root directory for saving processed data. Defaults to \"tmp/qa_construction\".\n            num_processes (int, optional): Number of processes for parallel processing. Defaults to 1.\n            force (bool, optional): If True, forces reprocessing of existing data. Defaults to False.\n\n        Attributes:\n            ner_model (BaseNERModel): Initialized NER model instance.\n            el_model (BaseELModel): Initialized EL model instance.\n            root (str): Root directory path.\n            num_processes (int): Number of parallel processes.\n            data_name (None): Name of the dataset, initialized as None.\n            force (bool): Force reprocessing flag.\n        \"\"\"\n\n        self.ner_model = ner_model\n        self.el_model = el_model\n        self.root = root\n        self.num_processes = num_processes\n        self.data_name = None\n        self.force = force\n\n    @property\n    def tmp_dir(self) -&gt; str:\n        \"\"\"\n        Returns the temporary directory path for data processing.\n\n        This property method creates and returns a directory path specific to the current\n        data_name under the root directory. The directory is created if it doesn't exist.\n\n        Returns:\n            str: Path to the temporary directory.\n\n        Raises:\n            AssertionError: If data_name is not set before accessing this property.\n        \"\"\"\n        assert (\n            self.data_name is not None\n        )  # data_name should be set before calling this property\n        tmp_dir = os.path.join(self.root, self.data_name)\n        if not os.path.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        return tmp_dir\n\n    @staticmethod\n    def from_config(cfg: DictConfig) -&gt; \"QAConstructor\":\n        \"\"\"Creates a QAConstructor instance from a configuration.\n\n        This method initializes a QAConstructor using configuration parameters, creating a unique\n        temporary directory based on the config fingerprint to store processing artifacts.\n\n        Args:\n            cfg (DictConfig): Configuration object containing:\n\n                - root: Base directory path\n                - ner_model: Named Entity Recognition model configuration\n                - el_model: Entity Linking model configuration\n                - num_processes: Number of processes to use\n                - force: Force reprocessing flag (optional)\n\n        Returns:\n            QAConstructor: Initialized QAConstructor instance with specified configuration\n\n        Note:\n            The method creates a temporary directory using MD5 hash of the config as fingerprint,\n            excluding the 'force' parameter. The full config is saved in this directory as\n            'config.json'.\n        \"\"\"\n        # create a fingerprint of config for tmp directory\n        config = OmegaConf.to_container(cfg, resolve=True)\n        if \"force\" in config:\n            del config[\"force\"]\n        fingerprint = hashlib.md5(json.dumps(config).encode()).hexdigest()\n\n        base_tmp_dir = os.path.join(cfg.root, fingerprint)\n        if not os.path.exists(base_tmp_dir):\n            os.makedirs(base_tmp_dir)\n            json.dump(\n                config,\n                open(os.path.join(base_tmp_dir, \"config.json\"), \"w\"),\n                indent=4,\n            )\n        return QAConstructor(\n            root=base_tmp_dir,\n            ner_model=instantiate(cfg.ner_model),\n            el_model=instantiate(cfg.el_model),\n            num_processes=cfg.num_processes,\n            force=cfg.force,\n        )\n\n    def prepare_data(self, data_root: str, data_name: str, file: str) -&gt; list[dict]:\n        \"\"\"\n        Prepares data for question answering by processing raw data, performing Named Entity Recognition (NER),\n        and Entity Linking (EL).\n\n        Args:\n            data_root (str): Root directory path containing the dataset.\n            data_name (str): Name of the dataset.\n            file (str): Filename of the raw data.\n\n        Returns:\n            list[dict]: A list of processed data samples. Each sample is a dictionary containing:\n                - Original sample fields\n                - question_entities (list): Linked entities found in the question\n                - supporting_entities (list): Entities from supporting facts\n\n        Raises:\n            FileNotFoundError: If the required KG file is not found in the processed directory.\n\n        Notes:\n            - Requires a pre-constructed knowledge graph (KG) file in the processed directory\n            - Uses cached NER results if available, otherwise performs NER processing\n            - Performs entity linking on identified entities\n            - Combines question entities with supporting fact entities\n        \"\"\"\n        # Get dataset information\n        self.data_name = data_name  # type: ignore\n        raw_path = os.path.join(data_root, data_name, \"raw\", file)\n        processed_path = os.path.join(data_root, data_name, \"processed\", \"stage1\")\n\n        if self.force:\n            # Clear cache in tmp directory\n            for tmp_file in os.listdir(self.tmp_dir):\n                os.remove(os.path.join(self.tmp_dir, tmp_file))\n\n        if not os.path.exists(os.path.join(processed_path, \"kg.txt\")):\n            raise FileNotFoundError(\n                \"KG file not found. Please run KG construction first\"\n            )\n\n        # Read KG\n        entities = set()\n        with open(os.path.join(processed_path, \"kg.txt\")) as f:\n            for line in f:\n                try:\n                    u, _, v = line.strip().split(self.DELIMITER)\n                except Exception as e:\n                    logger.error(f\"Error in line: {line}, {e}, Skipping\")\n                    continue\n                entities.add(u)\n                entities.add(v)\n        # Read document2entities\n        with open(os.path.join(processed_path, \"document2entities.json\")) as f:\n            doc2entities = json.load(f)\n\n        # Load data\n        with open(raw_path) as f:\n            data = json.load(f)\n\n        ner_results = {}\n        # Try to read ner results\n        if os.path.exists(os.path.join(self.tmp_dir, \"ner_results.jsonl\")):\n            with open(os.path.join(self.tmp_dir, \"ner_results.jsonl\")) as f:\n                ner_logs = [json.loads(line) for line in f]\n                ner_results = {log[\"id\"]: log for log in ner_logs}\n\n        unprocessed_data = [\n            sample for sample in data if sample[\"id\"] not in ner_results\n        ]\n\n        def _ner_process(sample: dict) -&gt; dict:\n            id = sample[\"id\"]\n            question = sample[\"question\"]\n            ner_ents = self.ner_model(question)\n            return {\n                \"id\": id,\n                \"question\": question,\n                \"ner_ents\": ner_ents,\n            }\n\n        # NER\n        with ThreadPool(self.num_processes) as pool:\n            with open(os.path.join(self.tmp_dir, \"ner_results.jsonl\"), \"a\") as f:\n                for res in tqdm(\n                    pool.imap(_ner_process, unprocessed_data),\n                    total=len(unprocessed_data),\n                ):\n                    ner_results[res[\"id\"]] = res\n                    f.write(json.dumps(res) + \"\\n\")\n\n        # EL\n        self.el_model.index(list(entities))\n\n        ner_entities = []\n        for res in ner_results.values():\n            ner_entities.extend(res[\"ner_ents\"])\n\n        el_results = self.el_model(ner_entities, topk=1)\n\n        # Prepare final data\n        final_data = []\n        for sample in data:\n            id = sample[\"id\"]\n            ner_ents = ner_results[id][\"ner_ents\"]\n            question_entities = []\n            for ent in ner_ents:\n                question_entities.append(el_results[ent][0][\"entity\"])\n\n            supporting_facts = sample.get(\"supporting_facts\", [])\n            supporting_entities = []\n            for item in list(set(supporting_facts)):\n                supporting_entities.extend(doc2entities.get(item, []))\n\n            final_data.append(\n                {\n                    **sample,\n                    \"question_entities\": question_entities,\n                    \"supporting_entities\": supporting_entities,\n                }\n            )\n\n        return final_data\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.QAConstructor.tmp_dir","title":"<code>tmp_dir</code>  <code>property</code>","text":"<p>Returns the temporary directory path for data processing.</p> <p>This property method creates and returns a directory path specific to the current data_name under the root directory. The directory is created if it doesn't exist.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the temporary directory.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If data_name is not set before accessing this property.</p>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.QAConstructor.__init__","title":"<code>__init__(ner_model, el_model, root='tmp/qa_construction', num_processes=1, force=False)</code>","text":"<p>Initialize the Question Answer Constructor.</p> <p>This constructor processes text data through Named Entity Recognition (NER) and Entity Linking (EL) models to generate question-answer pairs.</p> <p>Parameters:</p> Name Type Description Default <code>ner_model</code> <code>BaseNERModel</code> <p>Model for Named Entity Recognition.</p> required <code>el_model</code> <code>BaseELModel</code> <p>Model for Entity Linking.</p> required <code>root</code> <code>str</code> <p>Root directory for saving processed data. Defaults to \"tmp/qa_construction\".</p> <code>'tmp/qa_construction'</code> <code>num_processes</code> <code>int</code> <p>Number of processes for parallel processing. Defaults to 1.</p> <code>1</code> <code>force</code> <code>bool</code> <p>If True, forces reprocessing of existing data. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>ner_model</code> <code>BaseNERModel</code> <p>Initialized NER model instance.</p> <code>el_model</code> <code>BaseELModel</code> <p>Initialized EL model instance.</p> <code>root</code> <code>str</code> <p>Root directory path.</p> <code>num_processes</code> <code>int</code> <p>Number of parallel processes.</p> <code>data_name</code> <code>None</code> <p>Name of the dataset, initialized as None.</p> <code>force</code> <code>bool</code> <p>Force reprocessing flag.</p> Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>def __init__(\n    self,\n    ner_model: BaseNERModel,\n    el_model: BaseELModel,\n    root: str = \"tmp/qa_construction\",\n    num_processes: int = 1,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the Question Answer Constructor.\n\n    This constructor processes text data through Named Entity Recognition (NER) and Entity Linking (EL) models\n    to generate question-answer pairs.\n\n    Args:\n        ner_model (BaseNERModel): Model for Named Entity Recognition.\n        el_model (BaseELModel): Model for Entity Linking.\n        root (str, optional): Root directory for saving processed data. Defaults to \"tmp/qa_construction\".\n        num_processes (int, optional): Number of processes for parallel processing. Defaults to 1.\n        force (bool, optional): If True, forces reprocessing of existing data. Defaults to False.\n\n    Attributes:\n        ner_model (BaseNERModel): Initialized NER model instance.\n        el_model (BaseELModel): Initialized EL model instance.\n        root (str): Root directory path.\n        num_processes (int): Number of parallel processes.\n        data_name (None): Name of the dataset, initialized as None.\n        force (bool): Force reprocessing flag.\n    \"\"\"\n\n    self.ner_model = ner_model\n    self.el_model = el_model\n    self.root = root\n    self.num_processes = num_processes\n    self.data_name = None\n    self.force = force\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.QAConstructor.from_config","title":"<code>from_config(cfg)</code>  <code>staticmethod</code>","text":"<p>Creates a QAConstructor instance from a configuration.</p> <p>This method initializes a QAConstructor using configuration parameters, creating a unique temporary directory based on the config fingerprint to store processing artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration object containing:</p> <ul> <li>root: Base directory path</li> <li>ner_model: Named Entity Recognition model configuration</li> <li>el_model: Entity Linking model configuration</li> <li>num_processes: Number of processes to use</li> <li>force: Force reprocessing flag (optional)</li> </ul> required <p>Returns:</p> Name Type Description <code>QAConstructor</code> <code>QAConstructor</code> <p>Initialized QAConstructor instance with specified configuration</p> Note <p>The method creates a temporary directory using MD5 hash of the config as fingerprint, excluding the 'force' parameter. The full config is saved in this directory as 'config.json'.</p> Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>@staticmethod\ndef from_config(cfg: DictConfig) -&gt; \"QAConstructor\":\n    \"\"\"Creates a QAConstructor instance from a configuration.\n\n    This method initializes a QAConstructor using configuration parameters, creating a unique\n    temporary directory based on the config fingerprint to store processing artifacts.\n\n    Args:\n        cfg (DictConfig): Configuration object containing:\n\n            - root: Base directory path\n            - ner_model: Named Entity Recognition model configuration\n            - el_model: Entity Linking model configuration\n            - num_processes: Number of processes to use\n            - force: Force reprocessing flag (optional)\n\n    Returns:\n        QAConstructor: Initialized QAConstructor instance with specified configuration\n\n    Note:\n        The method creates a temporary directory using MD5 hash of the config as fingerprint,\n        excluding the 'force' parameter. The full config is saved in this directory as\n        'config.json'.\n    \"\"\"\n    # create a fingerprint of config for tmp directory\n    config = OmegaConf.to_container(cfg, resolve=True)\n    if \"force\" in config:\n        del config[\"force\"]\n    fingerprint = hashlib.md5(json.dumps(config).encode()).hexdigest()\n\n    base_tmp_dir = os.path.join(cfg.root, fingerprint)\n    if not os.path.exists(base_tmp_dir):\n        os.makedirs(base_tmp_dir)\n        json.dump(\n            config,\n            open(os.path.join(base_tmp_dir, \"config.json\"), \"w\"),\n            indent=4,\n        )\n    return QAConstructor(\n        root=base_tmp_dir,\n        ner_model=instantiate(cfg.ner_model),\n        el_model=instantiate(cfg.el_model),\n        num_processes=cfg.num_processes,\n        force=cfg.force,\n    )\n</code></pre>"},{"location":"api/kg_construnction/qa_constructor/#gfmrag.kg_construction.QAConstructor.prepare_data","title":"<code>prepare_data(data_root, data_name, file)</code>","text":"<p>Prepares data for question answering by processing raw data, performing Named Entity Recognition (NER), and Entity Linking (EL).</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory path containing the dataset.</p> required <code>data_name</code> <code>str</code> <p>Name of the dataset.</p> required <code>file</code> <code>str</code> <p>Filename of the raw data.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of processed data samples. Each sample is a dictionary containing: - Original sample fields - question_entities (list): Linked entities found in the question - supporting_entities (list): Entities from supporting facts</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the required KG file is not found in the processed directory.</p> Notes <ul> <li>Requires a pre-constructed knowledge graph (KG) file in the processed directory</li> <li>Uses cached NER results if available, otherwise performs NER processing</li> <li>Performs entity linking on identified entities</li> <li>Combines question entities with supporting fact entities</li> </ul> Source code in <code>gfmrag/kg_construction/qa_constructor.py</code> Python<pre><code>def prepare_data(self, data_root: str, data_name: str, file: str) -&gt; list[dict]:\n    \"\"\"\n    Prepares data for question answering by processing raw data, performing Named Entity Recognition (NER),\n    and Entity Linking (EL).\n\n    Args:\n        data_root (str): Root directory path containing the dataset.\n        data_name (str): Name of the dataset.\n        file (str): Filename of the raw data.\n\n    Returns:\n        list[dict]: A list of processed data samples. Each sample is a dictionary containing:\n            - Original sample fields\n            - question_entities (list): Linked entities found in the question\n            - supporting_entities (list): Entities from supporting facts\n\n    Raises:\n        FileNotFoundError: If the required KG file is not found in the processed directory.\n\n    Notes:\n        - Requires a pre-constructed knowledge graph (KG) file in the processed directory\n        - Uses cached NER results if available, otherwise performs NER processing\n        - Performs entity linking on identified entities\n        - Combines question entities with supporting fact entities\n    \"\"\"\n    # Get dataset information\n    self.data_name = data_name  # type: ignore\n    raw_path = os.path.join(data_root, data_name, \"raw\", file)\n    processed_path = os.path.join(data_root, data_name, \"processed\", \"stage1\")\n\n    if self.force:\n        # Clear cache in tmp directory\n        for tmp_file in os.listdir(self.tmp_dir):\n            os.remove(os.path.join(self.tmp_dir, tmp_file))\n\n    if not os.path.exists(os.path.join(processed_path, \"kg.txt\")):\n        raise FileNotFoundError(\n            \"KG file not found. Please run KG construction first\"\n        )\n\n    # Read KG\n    entities = set()\n    with open(os.path.join(processed_path, \"kg.txt\")) as f:\n        for line in f:\n            try:\n                u, _, v = line.strip().split(self.DELIMITER)\n            except Exception as e:\n                logger.error(f\"Error in line: {line}, {e}, Skipping\")\n                continue\n            entities.add(u)\n            entities.add(v)\n    # Read document2entities\n    with open(os.path.join(processed_path, \"document2entities.json\")) as f:\n        doc2entities = json.load(f)\n\n    # Load data\n    with open(raw_path) as f:\n        data = json.load(f)\n\n    ner_results = {}\n    # Try to read ner results\n    if os.path.exists(os.path.join(self.tmp_dir, \"ner_results.jsonl\")):\n        with open(os.path.join(self.tmp_dir, \"ner_results.jsonl\")) as f:\n            ner_logs = [json.loads(line) for line in f]\n            ner_results = {log[\"id\"]: log for log in ner_logs}\n\n    unprocessed_data = [\n        sample for sample in data if sample[\"id\"] not in ner_results\n    ]\n\n    def _ner_process(sample: dict) -&gt; dict:\n        id = sample[\"id\"]\n        question = sample[\"question\"]\n        ner_ents = self.ner_model(question)\n        return {\n            \"id\": id,\n            \"question\": question,\n            \"ner_ents\": ner_ents,\n        }\n\n    # NER\n    with ThreadPool(self.num_processes) as pool:\n        with open(os.path.join(self.tmp_dir, \"ner_results.jsonl\"), \"a\") as f:\n            for res in tqdm(\n                pool.imap(_ner_process, unprocessed_data),\n                total=len(unprocessed_data),\n            ):\n                ner_results[res[\"id\"]] = res\n                f.write(json.dumps(res) + \"\\n\")\n\n    # EL\n    self.el_model.index(list(entities))\n\n    ner_entities = []\n    for res in ner_results.values():\n        ner_entities.extend(res[\"ner_ents\"])\n\n    el_results = self.el_model(ner_entities, topk=1)\n\n    # Prepare final data\n    final_data = []\n    for sample in data:\n        id = sample[\"id\"]\n        ner_ents = ner_results[id][\"ner_ents\"]\n        question_entities = []\n        for ent in ner_ents:\n            question_entities.append(el_results[ent][0][\"entity\"])\n\n        supporting_facts = sample.get(\"supporting_facts\", [])\n        supporting_entities = []\n        for item in list(set(supporting_facts)):\n            supporting_entities.extend(doc2entities.get(item, []))\n\n        final_data.append(\n            {\n                **sample,\n                \"question_entities\": question_entities,\n                \"supporting_entities\": supporting_entities,\n            }\n        )\n\n    return final_data\n</code></pre>"},{"location":"config/doc_ranker_config/","title":"Document Ranker Config","text":""},{"location":"config/doc_ranker_config/#simple-ranker","title":"Simple Ranker","text":"<p>Example</p> gfmrag/workflow/config/doc_ranker/simple_ranker.yaml<pre><code>_target_: gfmrag.doc_rankers.SimpleRanker\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.doc_rankers.SimpleRanker</code> The class name of SimpleRanker"},{"location":"config/doc_ranker_config/#idf-ranker","title":"IDF Ranker","text":"<p>Example</p> gfmrag/workflow/config/doc_ranker/idf_ranker.yaml<pre><code>_target_: gfmrag.doc_rankers.IDFWeightedRanker\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.doc_rankers.IDFWeightedRanker</code> The class name of IDFWeightedRanker"},{"location":"config/doc_ranker_config/#top-k-ranker","title":"Top-k Ranker","text":"<p>Example</p> gfmrag/workflow/config/doc_ranker/topk_ranker.yaml<pre><code>_target_: gfmrag.doc_rankers.TopKRanker\ntop_k: 10\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.doc_rankers.TopKRanker</code> The class name of TopKRanker <code>top_k</code> Integer The top-k entities used for document retrieval"},{"location":"config/doc_ranker_config/#idf-top-k-ranker","title":"IDF Top-k Ranker","text":"<p>Example</p> gfmrag/workflow/config/doc_ranker/idf_topk_ranker.yaml<pre><code>_target_: gfmrag.doc_rankers.IDFWeightedTopKRanker\ntop_k: 20\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.doc_rankers.IDFWeightedTopKRanker</code> The class name of IDFWeightedTopKRanker <code>top_k</code> Integer The top-k entities used for document retrieval"},{"location":"config/el_model_config/","title":"Entity Linking Model Config","text":""},{"location":"config/el_model_config/#colbert-el-model-configuration","title":"Colbert EL Model Configuration","text":"<p>An example of a Colbert EL model configuration file is shown below:</p> <p>colbertv2.0</p> gfmrag/workflow/config/el_model/colbert_el_model.yaml<pre><code>_target_: gfmrag.kg_construction.entity_linking_model.ColbertELModel\ncheckpoint_path: tmp/colbertv2.0\nroot: tmp\ndoc_index_name: nbits_2\nphrase_index_name: nbits_2\n</code></pre> <p>To use colbertv2.0 model, you need to download the checkpoint file and unzip it into the <code>checkpoint_path</code> (default: <code>tmp/</code>).</p> Bash<pre><code>wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz\ntar -zxvf colbertv2.0.tar.gz -C tmp/\n</code></pre> <p>An example checkpoint file structure is shown below:</p> Text Only<pre><code>tmp/colbertv2.0/\n\u251c\u2500\u2500 artifact.metadata\n\u251c\u2500\u2500 tokenizer.json\n\u251c\u2500\u2500 special_tokens_map.json\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 tokenizer_config.json\n\u251c\u2500\u2500 vocab.txt\n\u2514\u2500\u2500 pytorch_model.bin\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.kg_construction.entity_linking_model.ColbertELModel</code> The class name of Colbert EL model <code>checkpoint_path</code> None The path to the checkpoint file. <code>root</code> None The root directory of the model. <code>doc_index_name</code> None The name of the document index. <code>phrase_index_name</code> None The name of the phrase index. <p>Please refer to ColbertELModel for details on the other parameters.</p>"},{"location":"config/el_model_config/#dense-pre-train-text-embedding-model-configuration","title":"Dense Pre-train Text Embedding Model Configuration","text":"<p>This configuration supports most of the dense pre-train text embedding models of SentenceTransformer. An example of a dense pre-train text embedding model configuration file is shown below:</p> <p>DPR EL Model</p> gfmrag/workflow/config/el_model/dpr_el_model.yaml<pre><code>_target_: gfmrag.kg_construction.entity_linking_model.DPRELModel\nmodel_name: BAAI/bge-large-en-v1.5\nroot: tmp\nuse_cache: True\nnormalize: True\nquery_instruct: null\npassage_instruct: null\nmodel_kwargs: null\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.kg_construction.entity_linking_model.DPRELModel</code> The class name of Dense Pre-train Text Embedding model <code>model_name</code> None The name of the dense pre-train text embedding model. <code>root</code> None The root directory of the model. <code>use_cache</code> <code>True</code>, <code>False</code> Whether to use cache. <code>normalize</code> <code>True</code>, <code>False</code> Whether to normalize the embeddings. <code>query_instruct</code> None The instruction for the query. <code>passage_instruct</code> None The instruction for the passage. <code>model_kwargs</code> None The additional model arguments. <p>Please refer to DPR EL Model for details on the other parameters.</p>"},{"location":"config/el_model_config/#nvidia-embedding-model-configuration","title":"Nvidia Embedding Model Configuration","text":"<p>This configuration supports most of the Nvidia embedding models. An example of a Nvidia embedding model configuration file is shown below:</p> <p>nvidia/NV-Embed-v2</p> gfmrag/workflow/config/el_model/nv_embed_v2.yaml<pre><code>_target_: gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel\nmodel_name: nvidia/NV-Embed-v2\nroot: tmp\nuse_cache: True\nnormalize: True\nquery_instruct: \"Instruct: Given a entity, retrieve entities that are semantically equivalent to the given entity\\nQuery: \"\npassage_instruct: null\nmodel_kwargs:\n  torch_dtype: bfloat16\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel</code> The class name of Nvidia Embedding model <code>model_name</code> <code>nvidia/NV-Embed-v2</code> The name of the Nvidia embedding model. <code>root</code> None The root directory of the model. <code>use_cache</code> <code>True</code>, <code>False</code> Whether to use cache. <code>normalize</code> <code>True</code>, <code>False</code> Whether to normalize the embeddings. <code>query_instruct</code> <code>Instruct: Given a entity, retrieve entities that are semantically equivalent to the given entity\\nQuery:</code> The instruction for the query. <code>passage_instruct</code> None The instruction for the passage. <code>model_kwargs</code> <code>{}</code> The additional model arguments. <p>Please refer to NVEmbedV2 EL Model for details on the other parameters.</p>"},{"location":"config/gfmrag_finetune_config/","title":"GFM-RAG Fine-tuning Configuration","text":"<p>An example configuration file for GFM fine-tuning is shown below:</p> <p>Example</p> gfmrag/workflow/config/stage2_qa_finetune.yaml<pre><code>hydra:\n  run:\n    dir: outputs/qa_finetune/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker # The document ranker to use\n  - text_emb_model: mpnet # The text embedding model to use\n\nseed: 1024\n\ndatasets:\n  _target_: gfmrag.datasets.QADataset # The QA dataset class\n  cfgs:\n    root: ./data # data root directory\n    force_rebuild: False # Whether to force rebuild the dataset\n    text_emb_model_cfgs: ${text_emb_model} # The text embedding model configuration\n  train_names: # List of training dataset names\n    - hotpotqa_train_example\n  valid_names: # List of validation dataset names\n    - hotpotqa_test\n    - musique_test\n    - 2wikimultihopqa_test\n\n# GFM model configuration\nmodel:\n  _target_: gfmrag.models.GNNRetriever\n  entity_model:\n    _target_: gfmrag.ultra.models.QueryNBFNet\n    input_dim: 512\n    hidden_dims: [512, 512, 512, 512, 512, 512]\n    message_func: distmult\n    aggregate_func: sum\n    short_cut: yes\n    layer_norm: yes\n\n# Loss configuration\ntask:\n  strict_negative: yes\n  metric:\n    [mrr, hits@1, hits@2, hits@3, hits@5, hits@10, hits@20, hits@50, hits@100]\n  losses:\n    - name: ent_bce_loss\n      loss:\n        _target_: gfmrag.losses.BCELoss\n        adversarial_temperature: 0.2\n      cfg:\n        weight: 0.3\n        is_doc_loss: False\n    - name: ent_pcr_loss\n      loss:\n        _target_: gfmrag.losses.ListCELoss\n      cfg:\n        weight: 0.7\n        is_doc_loss: False\n\n# Optimizer configuration\noptimizer:\n  _target_: torch.optim.AdamW\n  lr: 5.0e-4\n\n# Training configuration\ntrain:\n  batch_size: 8\n  num_epoch: 20\n  log_interval: 100\n  batch_per_epoch: null\n  save_best_only: yes\n  save_pretrained: yes # Save the model for QA inference\n  do_eval: yes\n  timeout: 60 # timeout minutes for multi-gpu training\n  init_entities_weight: True\n\n  checkpoint: null\n</code></pre>"},{"location":"config/gfmrag_finetune_config/#general-configuration","title":"General Configuration","text":"Parameter Options Note <code>run.dir</code> None The output directory of the log"},{"location":"config/gfmrag_finetune_config/#defaults","title":"Defaults","text":"Parameter Options Note <code>text_emb_model</code> None The text embedding model to use"},{"location":"config/gfmrag_finetune_config/#training-datasets","title":"Training datasets","text":"Parameter Options Note <code>_target_</code> None QADataset <code>cfgs.root</code> None root dictionary of the datasets saving path <code>cfgs.force_rebuild</code> None whether to force rebuild the dataset <code>cfgs.text_emb_model_cfgs</code> None text embedding modelconfiguration <code>train_names</code> <code>[]</code> List of training dataset names <code>valid_names</code> <code>[]</code> List of validation dataset names"},{"location":"config/gfmrag_finetune_config/#gfm-model-configuration","title":"GFM model configuration","text":"Parameter Options Note <code>_target_</code> None QueryGNN model <code>entity_model</code> None EntityNBFNet model <code>input_dim</code> None input dimension of the model <code>hidden_dims</code> <code>[]</code> hidden dimensions of the model <code>message_func</code> <code>transe</code>,<code>rotate</code>,<code>distmult</code> message function of the model <code>aggregate_func</code> <code>pna</code>,<code>min</code>,<code>max</code>,<code>mean</code>,<code>sum</code> aggregate function of the model <code>short_cut</code> <code>True</code>, <code>False</code> whether to use short cut <code>layer_norm</code> <code>True</code>, <code>False</code> whether to use layer norm"},{"location":"config/gfmrag_finetune_config/#loss-configuration","title":"Loss configuration","text":"Parameter Options Note ``` <code>strict_negative</code> None whether to use strict negative sampling <code>metric</code> None evaluation metrics to use <code>losses</code> None list of losses to use <code>losses[].name</code> None name of the loss <code>losses[]._target_</code> None loss function to use <code>losses[].cfg</code> None configuration of the loss <code>losses[].cfg.weight</code> None weight of the loss <code>losses[].cfg.is_doc_loss</code> None whether the loss is for document"},{"location":"config/gfmrag_finetune_config/#optimizer-configuration","title":"Optimizer configuration","text":"Parameter Options Note <code>optimizer._target_</code> None torch optimizer for the model <code>optimizer.lr</code> None learning rate for the optimizer"},{"location":"config/gfmrag_finetune_config/#training-configuration","title":"Training configuration","text":"Parameter Options Note <code>batch_size</code> None batch size for the training <code>num_epoch</code> None number of epochs for training <code>log_interval</code> None logging interval for the training <code>fast_test</code> None number of samples for fast test <code>save_best_only</code> None whether to save the best model based on the metric <code>save_pretrained</code> None whether to save the model for QA inference <code>batch_per_epoch</code> None number of batches per epoch for training <code>timeout</code> None timeout minutes for multi-gpu training <code>checkpoint</code> None checkpoint path for the training"},{"location":"config/gfmrag_pretrain_config/","title":"GFM-RAG Pre-training Configuration","text":"<p>An example configuration file for GFM pre-training is shown below:</p> <p>Example</p> gfmrag/workflow/config/stage2_kg_pretrain.yaml<pre><code>hydra:\n  run:\n    dir: outputs/kg_pretrain/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - text_emb_model: mpnet # The text embedding model to use\n\nseed: 1024\n\ndatasets:\n  _target_: gfmrag.datasets.KGDataset # The KG dataset class\n  cfgs:\n    root: ./data # data root directory\n    force_rebuild: False # Whether to force rebuild the dataset\n    text_emb_model_cfgs: ${text_emb_model} # The text embedding model configuration\n  train_names: # List of training dataset names\n    - hotpotqa_train_example\n  valid_names: []\n\n# GFM model configuration\nmodel:\n  _target_: gfmrag.models.QueryGNN\n  entity_model:\n    _target_: gfmrag.ultra.models.EntityNBFNet\n    input_dim: 512\n    hidden_dims: [512, 512, 512, 512, 512, 512]\n    message_func: distmult\n    aggregate_func: sum\n    short_cut: yes\n    layer_norm: yes\n\n# Loss configuration\ntask:\n  num_negative: 256\n  strict_negative: yes\n  adversarial_temperature: 1\n  metric: [mr, mrr, hits@1, hits@3, hits@10]\n\noptimizer:\n  _target_: torch.optim.AdamW\n  lr: 5.0e-4\n\n# Training configuration\ntrain:\n  batch_size: 8\n  num_epoch: 10\n  log_interval: 100\n  fast_test: 500\n  save_best_only: no\n  save_pretrained: no # Save the model for QA inference\n  batch_per_epoch: null\n  timeout: 60 # timeout minutes for multi-gpu training\n  # Checkpoint configuration\n  checkpoint: null\n</code></pre>"},{"location":"config/gfmrag_pretrain_config/#general-configuration","title":"General Configuration","text":"Parameter Options Note <code>run.dir</code> None The output directory of the log"},{"location":"config/gfmrag_pretrain_config/#defaults","title":"Defaults","text":"Parameter Options Note <code>text_emb_model</code> None The text embedding model to use"},{"location":"config/gfmrag_pretrain_config/#training-datasets","title":"Training datasets","text":"Parameter Options Note <code>_target_</code> None KGDataset <code>cfgs.root</code> None root dictionary of the datasets saving path <code>cfgs.force_rebuild</code> None whether to force rebuild the dataset <code>cfgs.text_emb_model_cfgs</code> None text embedding model configuration <code>train_names</code> <code>[]</code> List of training dataset names <code>valid_names</code> <code>[]</code> List of validation dataset names"},{"location":"config/gfmrag_pretrain_config/#gfm-model-configuration","title":"GFM model configuration","text":"Parameter Options Note <code>_target_</code> None QueryGNN model <code>entity_model</code> None EntityNBFNet model <code>input_dim</code> None input dimension of the model <code>hidden_dims</code> <code>[]</code> hidden dimensions of the model <code>message_func</code> <code>transe</code>,<code>rotate</code>,<code>distmult</code> message function of the model <code>aggregate_func</code> <code>pna</code>,<code>min</code>,<code>max</code>,<code>mean</code>,<code>sum</code> aggregate function of the model <code>short_cut</code> <code>True</code>, <code>False</code> whether to use short cut <code>layer_norm</code> <code>True</code>, <code>False</code> whether to use layer norm"},{"location":"config/gfmrag_pretrain_config/#loss-configuration","title":"Loss configuration","text":"Parameter Options Note <code>num_negative</code> None number of negative samples for each query <code>strict_negative</code> None whether to use strict negative sampling <code>adversarial_temperature</code> None adversarial temperature for negative sampling <code>metric</code> <code>[]</code> evaluation metrics for the model"},{"location":"config/gfmrag_pretrain_config/#optimizer-configuration","title":"Optimizer configuration","text":"Parameter Options Note <code>optimizer._target_</code> None torch optimizer for the model <code>optimizer.lr</code> None learning rate for the optimizer"},{"location":"config/gfmrag_pretrain_config/#training-configuration","title":"Training configuration","text":"Parameter Options Note <code>batch_size</code> None batch size for the training <code>num_epoch</code> None number of epochs for training <code>log_interval</code> None logging interval for the training <code>fast_test</code> None number of samples for fast test <code>save_best_only</code> None whether to save the best model based on the metric <code>save_pretrained</code> None whether to save the model for QA inference <code>batch_per_epoch</code> None number of batches per epoch for training <code>timeout</code> None timeout minutes for multi-gpu training <code>checkpoint</code> None checkpoint path for the training"},{"location":"config/gfmrag_retriever_config/","title":"GFM-RAG Configuration","text":"<p>An example configuration file for GFM-RAG is shown below:</p> <p>Example</p> gfmrag/workflow/config/stage3_qa_ircot_inference.yaml<pre><code>hydra:\n  run:\n    dir: outputs/qa_agent_inference/${dataset.data_name}/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker # The document ranker to use\n  - agent_prompt: hotpotqa_ircot # The agent prompt to use\n  - qa_prompt: hotpotqa # The QA prompt to use\n  - ner_model: llm_ner_model # The NER model to use\n  - el_model: colbert_el_model # The EL model to use\n  - qa_evaluator: hotpotqa # The QA evaluator to use\n\nseed: 1024\n\ndataset:\n  root: ./data # data root directory\n  data_name: hotpotqa_test # data name\n\nllm:\n  _target_: gfmrag.llms.ChatGPT # The language model to use\n  model_name_or_path: gpt-3.5-turbo # The model name or path\n  retry: 5 # Number of retries\n\ngraph_retriever:\n  model_path: rmanluo/GFM-RAG-8M # Checkpoint path of the pre-trained GFM-RAG model\n  doc_ranker: ${doc_ranker} # The document ranker to use\n  ner_model: ${ner_model} # The NER model to usek\n  el_model: ${el_model} # The EL model to use\n  qa_evaluator: ${qa_evaluator} # The QA evaluator to use\n  init_entities_weight: True # Whether to initialize the entities weight\n\ntest:\n  top_k: 10 # Number of documents to retrieve\n  max_steps: 2 # Maximum number of steps\n  max_test_samples: -1 # -1 for all samples\n  resume: null # Resume from previous prediction\n</code></pre>"},{"location":"config/gfmrag_retriever_config/#general-configuration","title":"General Configuration","text":"Parameter Options Note <code>run.dir</code> None The output directory of the log"},{"location":"config/gfmrag_retriever_config/#defaults","title":"Defaults","text":"Parameter Options Note <code>ner_model</code> None The config of the ner_model <code>el_model</code> None The config of the el_model <code>doc_ranker</code> None The config of the doc_ranker <code>qa_evaluator</code> None The config of the qa_evaluator <code>agent_prompt</code> None The config of the PromptBuilder used for IRCOT <code>qa_prompt</code> None The config of the PromptBuilder"},{"location":"config/gfmrag_retriever_config/#dataset","title":"Dataset","text":"Parameter Options Note <code>root</code> None The data root directory <code>data_name</code> None The data name"},{"location":"config/gfmrag_retriever_config/#llm","title":"LLM","text":"Parameter Options Note <code>_target_</code> None The language model to use <code>model_name_or_path</code> None The model name or path Additional parameters None Parameters to initialize a language model <p>Please refer to the LLMs page for more details.</p>"},{"location":"config/gfmrag_retriever_config/#graph-retriever","title":"Graph Retriever","text":"Parameter Options Note <code>_target_</code> None The graph retriever to use <code>model_path</code> None Checkpoint path of the pre-trained GFM-RAG model <code>doc_ranker</code> None The document ranker to use <code>ner_model</code> None The NER model to use <code>el_model</code> None The EL model to use <code>qa_evaluator</code> None The QA evaluator to use <code>init_entities_weight</code> <code>True</code>,<code>False</code> Whether to initialize the entities weight"},{"location":"config/gfmrag_retriever_config/#test","title":"Test","text":"Parameter Options Note <code>top_k</code> None Number of documents to retrieve <code>max_steps</code> None Maximum number of steps, <code>1</code> for single step <code>max_test_samples</code> None Maximum number of samples to test (<code>-1</code> for all samples) <code>resume</code> None Resume from previous prediction"},{"location":"config/kg_index_config/","title":"KG-index Configuration","text":"<p>An example of a KG-index configuration file is shown below:</p> <p>Example</p> gfmrag/workflow/config/stage1_index_dataset.yaml<pre><code>hydra:\n  run:\n    dir: outputs/kg_construction/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - ner_model: llm_ner_model # The NER model to use\n  - openie_model: llm_openie_model # The OpenIE model to use\n  - el_model: colbert_el_model # The EL model to use\n\ndataset:\n  root: ./data # data root directory\n  data_name: hotpotqa # data name\n\nkg_constructor:\n  _target_: gfmrag.kg_construction.KGConstructor # The KGConstructor class\n  open_ie_model: ${openie_model}\n  ner_model: ${ner_model}\n  el_model: ${el_model}\n  root: tmp/kg_construction # Temporary directory for storing intermediate files during KG construction\n  num_processes: 10 # Number of processes to use\n  cosine_sim_edges: True # Whether to conduct entities resolution using cosine similarity\n  threshold: 0.8 # Threshold for cosine similarity\n  max_sim_neighbors: 100 # Maximum number of similar neighbors to add\n  add_title: True # Whether to add the title to the content of the document during OpenIE\n  force: False # Whether to force recompute the KG\n\nqa_constructor:\n  _target_: gfmrag.kg_construction.QAConstructor # The QAConstructor class\n  root: tmp/qa_construction # Temporary directory for storing intermediate files during QA construction\n  ner_model: ${ner_model}\n  el_model: ${el_model}\n  num_processes: 10 # Number of processes to use\n  force: False # Whether to force recompute the QA data\n</code></pre>"},{"location":"config/kg_index_config/#general-configuration","title":"General Configuration","text":"Parameter Options Note <code>run.dir</code> None The output directory of the log"},{"location":"config/kg_index_config/#defaults","title":"Defaults","text":"Parameter Options Note <code>ner_model</code> None The config of the ner_model <code>openie_model</code> None The config of the openie_model <code>el_model</code> None The config of the el_model"},{"location":"config/kg_index_config/#dataset","title":"Dataset","text":"Parameter Options Note <code>root</code> None The data root directory <code>data_name</code> None The data name"},{"location":"config/kg_index_config/#kg-constructor","title":"KG Constructor","text":"Parameter Options Note <code>_target_</code> None The class of KGConstructor <code>open_ie_model</code> None The config of the openie_model <code>ner_model</code> None The config of the ner_model <code>el_model</code> None The config of the el_model <code>root</code> None The temporary directory for storing intermediate files during KG construction <code>num_processes</code> None The number of processes to use <code>cosine_sim_edges</code> None Whether to conduct entities resolution using cosine similarity <code>threshold</code> None Threshold for cosine similarity <code>max_sim_neighbors</code> None Maximum number of similar neighbors to add <code>add_title</code> None Whether to add the title to the content of the document during OpenIE <code>force</code> None Whether to force recompute the KG <p>Please refer to KG Constructor for details of parameters.</p>"},{"location":"config/kg_index_config/#qa-constructor","title":"QA Constructor","text":"Parameter Options Note <code>_target_</code> None The class of QAConstructor <code>root</code> None The temporary directory for storing intermediate files during QA construction <code>ner_model</code> None The config of the ner_model <code>el_model</code> None The config of the el_model <code>num_processes</code> None The number of processes to use <code>force</code> None Whether to force recompute the QA data <p>Please refer to QAConstructor for details of parameters.</p>"},{"location":"config/ner_model_config/","title":"NER Model Config","text":""},{"location":"config/ner_model_config/#llm-ner-model-configuration","title":"LLM NER Model Configuration","text":"<p>An example of a LLM NER model configuration file is shown below:</p> <p>Example</p> gfmrag/workflow/config/ner_model/llm_ner_model.yaml<pre><code>_target_: gfmrag.kg_construction.ner_model.LLMNERModel\nllm_api: openai\nmodel_name: gpt-4o-mini\nmax_tokens: 300\n</code></pre> Parameter Options Note <code>_target_</code> None The class name of LLM NER model <code>llm_api</code> <code>openai</code>, <code>nvidia</code>, <code>together</code>, <code>ollama</code>, <code>llama.cpp</code> The API to use for the LLM model. <code>model_name</code> None The name of the LLM model to use. For example, <code>gpt-4o-mini</code> <code>max_tokens</code> None The maximum number of tokens to use for the LLM model. <p>Please refer to LLM NER model for details on the other parameters.</p>"},{"location":"config/openie_model_config/","title":"OpenIE Model Config","text":""},{"location":"config/openie_model_config/#llm-openie-model-configuration","title":"LLM OpenIE Model Configuration","text":"<p>An example of a LLM OpenIE model configuration file is shown below:</p> <p>Example</p> gfmrag/workflow/config/openie_model/llm_openie_model.yaml<pre><code>_target_: gfmrag.kg_construction.openie_model.LLMOPENIEModel\nllm_api: openai\nmodel_name: gpt-4o-mini\nmax_ner_tokens: 300\nmax_triples_tokens: 4096\n</code></pre> Parameter Options Note <code>_target_</code> None The class name of LLM OpenIE model <code>llm_api</code> <code>openai</code>, <code>nvidia</code>, <code>together</code>, <code>ollama</code>, <code>llama.cpp</code> The API to use for the LLM model. <code>model_name</code> None The name of the LLM model to use. For example, <code>gpt-4o-mini</code> <code>max_ner_tokens</code> None The maximum number of tokens to use for the NER model. <code>max_triples_tokens</code> None The maximum number of tokens to use for the triples model. <p>Please refer to LLMOPENIEModel for more details of the other parameters.</p>"},{"location":"config/text_embedding_config/","title":"Text Embedding Model Configuration","text":""},{"location":"config/text_embedding_config/#pre-train-text-embedding-model-configuration","title":"Pre-train Text Embedding Model Configuration","text":"<p>This configuration supports most of the pre-train text embedding models of SentenceTransformer. Examples of DPR text embedding model configuration files are shown below:</p> <p>all-mpnet-base-v2</p> gfmrag/workflow/config/text_emb_model/mpnet.yaml<pre><code>_target_: gfmrag.text_emb_models.BaseTextEmbModel\ntext_emb_model_name: sentence-transformers/all-mpnet-base-v2\nnormalize: False\nbatch_size: 32\nquery_instruct: null\npassage_instruct: null\nmodel_kwargs: null\n</code></pre> <p>BAAI/bge-large-en</p> gfmrag/workflow/config/text_emb_model/bge_large_en.yaml<pre><code>_target_: gfmrag.text_emb_models.BaseTextEmbModel\ntext_emb_model_name: BAAI/bge-large-en\nnormalize: True\nbatch_size: 32\nquery_instruct: \"Represent this sentence for searching relevant passages: \"\npassage_instruct: null\nmodel_kwargs: null\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.text_emb_models.BaseTextEmbModel</code> The class name of Text Embedding model <code>text_emb_model_name</code> None The name of the pre-train text embedding model. <code>normalize</code> <code>True</code>, <code>False</code> Whether to normalize the embeddings. <code>query_instruct</code> None The instruction for the query. <code>passage_instruct</code> None The instruction for the passage. <code>model_kwargs</code> <code>{}</code> The additional model arguments."},{"location":"config/text_embedding_config/#nvidia-embedding-model-configuration","title":"Nvidia Embedding Model Configuration","text":"<p>This configuration supports the Nvidia embedding models. An example of a Nvidia embedding model configuration file is shown below:</p> <p>nvidia/NV-Embed-v2</p> gfmrag/workflow/config/text_emb_model/nv_embed_v2.yaml<pre><code>_target_: gfmrag.text_emb_models.NVEmbedV2\ntext_emb_model_name: nvidia/NV-Embed-v2\nnormalize: True\nbatch_size: 32\nquery_instruct: \"Instruct: Given a question, retrieve entities that can help answer the question\\nQuery: \"\npassage_instruct: null\nmodel_kwargs:\n  torch_dtype: bfloat16\n</code></pre> Parameter Options Note <code>_target_</code> <code>gfmrag.kg_construction.entity_linking_model.NVEmbedV2ELModel</code> The class name of Nvidia Embedding model <code>text_emb_model_name</code> <code>nvidia/NV-Embed-v2</code> The name of the Nvidia embedding model. <code>normalize</code> <code>True</code>, <code>False</code> Whether to normalize the embeddings. <code>query_instruct</code> <code>Instruct: Given an entity, retrieve entities that are semantically equivalent to the given entity\\nQuery:</code> The instruction for the query. <code>passage_instruct</code> None The instruction for the passage. <code>model_kwargs</code> <code>{}</code> The additional model arguments."},{"location":"workflow/data_preparation/","title":"Build Your Own Dataset","text":"<p>This guide explains how to prepare your dataset for use with GFM-RAG.</p>"},{"location":"workflow/data_preparation/#directory-structure","title":"Directory Structure","text":"<p>You need to prepare the following files:</p> <ul> <li><code>dataset_corpus.json</code>: A JSON file containing the entire document corpus.</li> <li><code>train.json</code> (optional): A JSON file containing the training data.</li> <li><code>test.json</code> (optional): A JSON file containing the test data.</li> </ul> <p>Place your files in the following structure: Text Only<pre><code>data_name/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 dataset_corpus.json\n\u2502   \u251c\u2500\u2500 train.json # (optional)\n\u2502   \u2514\u2500\u2500 test.json # (optional)\n\u2514\u2500\u2500 processed/ # Output directory\n</code></pre></p>"},{"location":"workflow/data_preparation/#data-format","title":"Data Format","text":""},{"location":"workflow/data_preparation/#dataset_corpusjson","title":"<code>dataset_corpus.json</code>","text":"<p>The <code>dataset_corpus.json</code> is a dictionary where each key is the title or unique id of a document and the value is the text of the document. Each document should be structured as follows:</p> <ul> <li><code>key</code>: The title or unique id of the document.</li> <li><code>value</code>: The text of the document.</li> </ul> <p>Example: JSON<pre><code>{\n    \"Fred Gehrke\":\n        \"Clarence Fred Gehrke (April 24, 1918 \u2013 February 9, 2002) was an American football player and executive.  He played in the National Football League (NFL) for the Cleveland / Los Angeles Rams, San Francisco 49ers and Chicago Cardinals from 1940 through 1950.  To boost team morale, Gehrke designed and painted the Los Angeles Rams logo in 1948, which was the first painted on the helmets of an NFL team.  He later served as the general manager of the Denver Broncos from 1977 through 1981.  He is the great-grandfather of Miami Marlin Christian Yelich\"\n    ,\n    \"Manny Machado\":\n        \"Manuel Arturo Machado (] ; born July 6, 1992) is an American professional baseball third baseman and shortstop for the Baltimore Orioles of Major League Baseball (MLB).  He attended Brito High School in Miami and was drafted by the Orioles with the third overall pick in the 2010 Major League Baseball draft.  He bats and throws right-handed.\"\n    ,\n    ...\n }\n</code></pre></p>"},{"location":"workflow/data_preparation/#trainjson-and-testjson","title":"<code>train.json</code> and <code>test.json</code>","text":"<p>If you want to train and evaluate the model, you need to provide training and test data in the form of a JSON file. Each entry in the JSON file should contain the following fields:</p> <ul> <li><code>id</code>: A unique identifier for the example.</li> <li><code>question</code>: The question or query.</li> <li><code>supporting_facts</code>: A list of supporting facts for the question. Each supporting fact is a list containing the title of the document that can be found in the <code>dataset_corpus.json</code> file.</li> </ul> <p>Each entry can also contain additional fields depending on the task. For example:</p> <ul> <li><code>answer</code>: The answer to the question.</li> </ul> <p>The additional fields will be copied during the following steps of the pipeline.</p> <p>Example: JSON<pre><code>[\n    {\n        \"id\": \"5adf5e285542992d7e9f9323\",\n        \"question\": \"When was the judge born who made notable contributions to the trial of the man who tortured, raped, and murdered eight student nurses from South Chicago Community Hospital on the night of July 13-14, 1966?\",\n        \"answer\": \"June 4, 1931\",\n        \"supporting_facts\": [\n            \"Louis B. Garippo\",\n            \"Richard Speck\"\n        ]\n    },\n    {\n        \"id\": \"5a7f7b365542992097ad2f80\",\n        \"question\": \"Did the Beaulieu Mine or the McIntyre Mines yield gold and copper?\",\n        \"answer\": \"The McIntyre also yielded a considerable amount of copper\",\n        \"supporting_facts\": [\n            \"Beaulieu Mine\",\n            \"McIntyre Mines\"\n        ]\n    }\n    ...\n]\n</code></pre></p>"},{"location":"workflow/inference/","title":"GFM-RAG Retrieval","text":"<p>GFM-RAG can be directly used for retrieval on a given dataset without fine-tuning. We provide an easy-to-use GFMRetriever interface for inference.</p> <p>Note</p> <p>We have already released the pre-trained model, which can be used directly for retrieval. The model will be automatically downloaded by specifying it in the configuration. YAML<pre><code>graph_retriever:\n  model_path: rmanluo/GFM-RAG-8M\n</code></pre></p>"},{"location":"workflow/inference/#config","title":"Config","text":"<p>You need to create a configuration file for inference.</p> gfmrag/workflow/config/stage3_qa_ircot_inference.yaml gfmrag/workflow/config/stage3_qa_ircot_inference.yaml<pre><code>hydra:\n  run:\n    dir: outputs/qa_agent_inference/${dataset.data_name}/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker # The document ranker to use\n  - agent_prompt: hotpotqa_ircot # The agent prompt to use\n  - qa_prompt: hotpotqa # The QA prompt to use\n  - ner_model: llm_ner_model # The NER model to use\n  - el_model: colbert_el_model # The EL model to use\n  - qa_evaluator: hotpotqa # The QA evaluator to use\n\nseed: 1024\n\ndataset:\n  root: ./data # data root directory\n  data_name: hotpotqa_test # data name\n\nllm:\n  _target_: gfmrag.llms.ChatGPT # The language model to use\n  model_name_or_path: gpt-3.5-turbo # The model name or path\n  retry: 5 # Number of retries\n\ngraph_retriever:\n  model_path: rmanluo/GFM-RAG-8M # Checkpoint path of the pre-trained GFM-RAG model\n  doc_ranker: ${doc_ranker} # The document ranker to use\n  ner_model: ${ner_model} # The NER model to usek\n  el_model: ${el_model} # The EL model to use\n  qa_evaluator: ${qa_evaluator} # The QA evaluator to use\n  init_entities_weight: True # Whether to initialize the entities weight\n\ntest:\n  top_k: 10 # Number of documents to retrieve\n  max_steps: 2 # Maximum number of steps\n  max_test_samples: -1 # -1 for all samples\n  resume: null # Resume from previous prediction\n</code></pre> <p>Details of the configuration parameters are explained in the GFM-RAG Configuration page.</p>"},{"location":"workflow/inference/#initialize-gfmretriever","title":"Initialize GFMRetriever","text":"<p>You can initialize the GFMRetriever with the following code. It will load the pre-trained GFM-RAG model and the KG-index for retrieval.</p> Python<pre><code>import logging\nimport os\n\nimport hydra\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom gfmrag import GFMRetriever\n\nlogger = logging.getLogger(__name__)\n\n\n@hydra.main(\n    config_path=\"config\", config_name=\"stage3_qa_ircot_inference\", version_base=None\n)\ndef main(cfg: DictConfig) -&gt; None:\n    output_dir = HydraConfig.get().runtime.output_dir\n    logger.info(f\"Config:\\n {OmegaConf.to_yaml(cfg)}\")\n    logger.info(f\"Current working directory: {os.getcwd()}\")\n    logger.info(f\"Output directory: {output_dir}\")\n\n    gfmrag_retriever = GFMRetriever.from_config(cfg)\n</code></pre>"},{"location":"workflow/inference/#document-retrieval","title":"Document Retrieval","text":"<p>You can use GFM-RAG retriever to reason over the KG-index and obtain documents for a given query. Python<pre><code>docs = retriever.retrieve(\"Who is the president of France?\", top_k=5)\n</code></pre></p>"},{"location":"workflow/inference/#question-answering","title":"Question Answering","text":"Python<pre><code>from hydra.utils import instantiate\nfrom gfmrag.llms import BaseLanguageModel\nfrom gfmrag.prompt_builder import QAPromptBuilder\n\nllm = instantiate(cfg.llm)\nqa_prompt_builder = QAPromptBuilder(cfg.qa_prompt)\n\nmessage = qa_prompt_builder.build_input_prompt(current_query, retrieved_docs)\nanswer = llm.generate_sentence(message)  # Answer: \"Emmanuel Macron\"\n</code></pre>"},{"location":"workflow/inference/#gfm-rag-agent-for-multi-step-retrieval","title":"GFM-RAG + Agent for Multi-step Retrieval","text":"<p>You can also integrate the GFM-RAG with arbitrary reasoning agents to perform multi-step RAG. Here is an example of IRCOT + GFM-RAG:</p> <p>You can run the following command to perform multi-step reasoning:</p> gfmrag/workflow/stage3_qa_ircot_inference.py <p> gfmrag/workflow/stage3_qa_ircot_inference.py<pre><code>import json\nimport logging\nimport os\n\nimport hydra\nfrom hydra.core.hydra_config import HydraConfig\nfrom hydra.utils import instantiate\nfrom omegaconf import DictConfig, OmegaConf\nfrom tqdm import tqdm\n\nfrom gfmrag import GFMRetriever\nfrom gfmrag.evaluation import RetrievalEvaluator\nfrom gfmrag.llms import BaseLanguageModel\nfrom gfmrag.prompt_builder import QAPromptBuilder\nfrom gfmrag.ultra import query_utils\n\n# A logger for this file\nlogger = logging.getLogger(__name__)\n\n\ndef agent_reasoning(\n    cfg: DictConfig,\n    gfmrag_retriever: GFMRetriever,\n    llm: BaseLanguageModel,\n    qa_prompt_builder: QAPromptBuilder,\n    query: str,\n) -&gt; dict:\n    step = 1\n    current_query = query\n    thoughts: list[str] = []\n    retrieved_docs = gfmrag_retriever.retrieve(current_query, top_k=cfg.test.top_k)\n    logs = []\n    while step &lt;= cfg.test.max_steps:\n        message = qa_prompt_builder.build_input_prompt(\n            current_query, retrieved_docs, thoughts\n        )\n        response = llm.generate_sentence(message)\n\n        if isinstance(response, Exception):\n            raise response from None\n\n        thoughts.append(response)\n\n        logs.append(\n            {\n                \"step\": step,\n                \"query\": current_query,\n                \"retrieved_docs\": retrieved_docs,\n                \"response\": response,\n                \"thoughts\": thoughts,\n            }\n        )\n\n        if \"So the answer is:\" in response:\n            break\n\n        step += 1\n\n        new_ret_docs = gfmrag_retriever.retrieve(response, top_k=cfg.test.top_k)\n\n        retrieved_docs_dict = {doc[\"title\"]: doc for doc in retrieved_docs}\n        for doc in new_ret_docs:\n            if doc[\"title\"] in retrieved_docs_dict:\n                if doc[\"norm_score\"] &gt; retrieved_docs_dict[doc[\"title\"]][\"norm_score\"]:\n                    retrieved_docs_dict[doc[\"title\"]][\"score\"] = doc[\"score\"]\n                    retrieved_docs_dict[doc[\"title\"]][\"norm_score\"] = doc[\"norm_score\"]\n            else:\n                retrieved_docs_dict[doc[\"title\"]] = doc\n        # Sort the retrieved docs by score\n        retrieved_docs = sorted(\n            retrieved_docs_dict.values(), key=lambda x: x[\"norm_score\"], reverse=True\n        )\n        # Only keep the top k\n        retrieved_docs = retrieved_docs[: cfg.test.top_k]\n\n    final_response = \" \".join(thoughts)\n    return {\"response\": final_response, \"retrieved_docs\": retrieved_docs, \"logs\": logs}\n\n\n@hydra.main(\n    config_path=\"config\", config_name=\"stage3_qa_ircot_inference\", version_base=None\n)\ndef main(cfg: DictConfig) -&gt; None:\n    output_dir = HydraConfig.get().runtime.output_dir\n    logger.info(f\"Config:\\n {OmegaConf.to_yaml(cfg)}\")\n    logger.info(f\"Current working directory: {os.getcwd()}\")\n    logger.info(f\"Output directory: {output_dir}\")\n\n    gfmrag_retriever = GFMRetriever.from_config(cfg)\n    llm = instantiate(cfg.llm)\n    agent_prompt_builder = QAPromptBuilder(cfg.agent_prompt)\n    qa_prompt_builder = QAPromptBuilder(cfg.qa_prompt)\n    test_data = gfmrag_retriever.qa_data.raw_test_data\n    max_samples = (\n        cfg.test.max_test_samples if cfg.test.max_test_samples &gt; 0 else len(test_data)\n    )\n    processed_data = {}\n    if cfg.test.resume:\n        logger.info(f\"Resuming from previous prediction {cfg.test.resume}\")\n        try:\n            with open(cfg.test.resume) as f:\n                for line in f:\n                    result = json.loads(line)\n                    processed_data[result[\"id\"]] = result\n        except Exception as e:\n            logger.error(f\"Could not resume from previous prediction {e}\")\n    with open(os.path.join(output_dir, \"prediction.jsonl\"), \"w\") as f:\n        for i in tqdm(range(max_samples)):\n            sample = test_data[i]\n            if i &gt;= max_samples:\n                break\n            query = sample[\"question\"]\n            if sample[\"id\"] in processed_data:\n                result = processed_data[sample[\"id\"]]\n            else:\n                result = agent_reasoning(\n                    cfg, gfmrag_retriever, llm, agent_prompt_builder, query\n                )\n\n                # Generate QA response\n                retrieved_docs = result[\"retrieved_docs\"]\n                message = qa_prompt_builder.build_input_prompt(query, retrieved_docs)\n                qa_response = llm.generate_sentence(message)\n\n                result = {\n                    \"id\": sample[\"id\"],\n                    \"question\": sample[\"question\"],\n                    \"answer\": sample[\"answer\"],\n                    \"answer_aliases\": sample.get(\n                        \"answer_aliases\", []\n                    ),  # Some datasets have answer aliases\n                    \"supporting_facts\": sample[\"supporting_facts\"],\n                    \"response\": qa_response,\n                    \"retrieved_docs\": retrieved_docs,\n                    \"logs\": result[\"logs\"],\n                }\n            f.write(json.dumps(result) + \"\\n\")\n            f.flush()\n\n    result_path = os.path.join(output_dir, \"prediction.jsonl\")\n    # Evaluation\n    evaluator = instantiate(cfg.qa_evaluator, prediction_file=result_path)\n    metrics = evaluator.evaluate()\n    query_utils.print_metrics(metrics, logger)\n\n    # Eval retrieval results\n    retrieval_evaluator = RetrievalEvaluator(prediction_file=result_path)\n    retrieval_metrics = retrieval_evaluator.evaluate()\n    query_utils.print_metrics(retrieval_metrics, logger)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.stage3_qa_ircot_inference\n</code></pre> <p>You can overwrite the configuration like this:</p> Bash<pre><code>python -m gfmrag.workflow.stage3_qa_ircot_inference test.max_steps=3\n</code></pre>"},{"location":"workflow/inference/#batch-retrieval","title":"Batch Retrieval","text":"<p>You can also perform batch retrieval with GFM-RAG with multi GPUs supports by running the following command:</p> gfmrag/workflow/config/stage3_qa_inference.yaml gfmrag/workflow/config/stage3_qa_inference.yaml<pre><code>hydra:\n  run:\n    dir: outputs/qa_inference/${dataset.data_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker\n  - qa_prompt: hotpotqa\n  - qa_evaluator: hotpotqa\n\nseed: 1024\n\ndataset:\n  root: ./data\n  data_name: hotpotqa_test\n\nllm:\n  _target_: gfmrag.llms.ChatGPT\n  model_name_or_path: gpt-3.5-turbo\n  retry: 5\n\ngraph_retriever:\n  model_path: rmanluo/GFM-RAG-8M\n\ntest:\n  retrieval_batch_size: 8\n  top_k: 5\n  save_retrieval: False\n  save_top_k_entity: 10\n  n_threads: 5\n  retrieved_result_path: null\n  prediction_result_path: null\n  init_entities_weight: True\n</code></pre> gfmrag/workflow/stage3_qa_inference.py <p> gfmrag/workflow/stage3_qa_inference.py<pre><code>import json\nimport logging\nimport os\nfrom multiprocessing.dummy import Pool as ThreadPool\n\nimport hydra\nimport torch\nfrom hydra.core.hydra_config import HydraConfig\nfrom hydra.utils import instantiate\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\n\nfrom gfmrag import utils\nfrom gfmrag.datasets import QADataset\nfrom gfmrag.prompt_builder import QAPromptBuilder\nfrom gfmrag.ultra import query_utils\n\n# A logger for this file\nlogger = logging.getLogger(__name__)\n\n\n@torch.no_grad()\ndef doc_retrieval(\n    cfg: DictConfig,\n    model: nn.Module,\n    qa_data: Dataset,\n    device: torch.device,\n) -&gt; list[dict]:\n    world_size = utils.get_world_size()\n    rank = utils.get_rank()\n\n    _, test_data = qa_data._data\n    graph = qa_data.kg\n    ent2docs = qa_data.ent2docs\n\n    # Retrieve the supporting documents for each query\n    sampler = torch_data.DistributedSampler(test_data, world_size, rank, shuffle=False)\n    test_loader = torch_data.DataLoader(\n        test_data, cfg.test.retrieval_batch_size, sampler=sampler\n    )\n\n    # Create doc retriever\n    doc_ranker = instantiate(cfg.doc_ranker, ent2doc=ent2docs)\n\n    if cfg.test.init_entities_weight:\n        entities_weight = utils.get_entities_weight(ent2docs)\n    else:\n        entities_weight = None\n\n    model.eval()\n    all_predictions: list[dict] = []\n    for batch in tqdm(test_loader):\n        batch = query_utils.cuda(batch, device=device)\n        ent_pred = model(graph, batch, entities_weight=entities_weight)\n        doc_pred = doc_ranker(ent_pred)  # Ent2docs mapping\n        idx = batch[\"sample_id\"]\n        all_predictions.extend(\n            {\"id\": i, \"ent_pred\": e, \"doc_pred\": d}\n            for i, e, d in zip(idx.cpu(), ent_pred.cpu(), doc_pred.cpu())\n        )\n\n    # Gather the predictions across all processes\n    if utils.get_world_size() &gt; 1:\n        gathered_predictions = [None] * torch.distributed.get_world_size()\n        torch.distributed.all_gather_object(gathered_predictions, all_predictions)\n    else:\n        gathered_predictions = [all_predictions]  # type: ignore\n\n    sorted_predictions = sorted(\n        [item for sublist in gathered_predictions for item in sublist],  # type: ignore\n        key=lambda x: x[\"id\"],\n    )\n    utils.synchronize()\n    return sorted_predictions\n\n\ndef ans_prediction(\n    cfg: DictConfig, output_dir: str, qa_data: Dataset, retrieval_result: list[dict]\n) -&gt; str:\n    llm = instantiate(cfg.llm)\n    doc_retriever = utils.DocumentRetriever(qa_data.doc, qa_data.id2doc)\n    test_data = qa_data.raw_test_data\n    id2ent = {v: k for k, v in qa_data.ent2id.items()}\n\n    prompt_builder = QAPromptBuilder(cfg.qa_prompt)\n\n    def predict(qa_input: tuple[dict, torch.Tensor]) -&gt; dict | Exception:\n        data, retrieval_doc = qa_input\n        retrieved_ent_idx = torch.topk(\n            retrieval_doc[\"ent_pred\"], cfg.test.save_top_k_entity, dim=-1\n        ).indices\n        retrieved_ent = [id2ent[i.item()] for i in retrieved_ent_idx]\n        retrieved_docs = doc_retriever(retrieval_doc[\"doc_pred\"], top_k=cfg.test.top_k)\n\n        message = prompt_builder.build_input_prompt(data[\"question\"], retrieved_docs)\n\n        response = llm.generate_sentence(message)\n        if isinstance(response, Exception):\n            return response\n        else:\n            return {\n                \"id\": data[\"id\"],\n                \"question\": data[\"question\"],\n                \"answer\": data[\"answer\"],\n                \"answer_aliases\": data.get(\n                    \"answer_aliases\", []\n                ),  # Some datasets have answer aliases\n                \"response\": response,\n                \"retrieved_ent\": retrieved_ent,\n                \"retrieved_docs\": retrieved_docs,\n            }\n\n    with open(os.path.join(output_dir, \"prediction.jsonl\"), \"w\") as f:\n        with ThreadPool(cfg.test.n_threads) as pool:\n            for results in tqdm(\n                pool.imap(predict, zip(test_data, retrieval_result)),\n                total=len(test_data),\n            ):\n                if isinstance(results, Exception):\n                    logger.error(f\"Error: {results}\")\n                    continue\n\n                f.write(json.dumps(results) + \"\\n\")\n                f.flush()\n\n    return os.path.join(output_dir, \"prediction.jsonl\")\n\n\n@hydra.main(config_path=\"config\", config_name=\"stage3_qa_inference\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    output_dir = HydraConfig.get().runtime.output_dir\n    utils.init_distributed_mode()\n    torch.manual_seed(cfg.seed + utils.get_rank())\n    if utils.get_rank() == 0:\n        logger.info(f\"Config:\\n {OmegaConf.to_yaml(cfg)}\")\n        logger.info(f\"Current working directory: {os.getcwd()}\")\n        logger.info(f\"Output directory: {output_dir}\")\n\n    model, model_config = utils.load_model_from_pretrained(\n        cfg.graph_retriever.model_path\n    )\n    qa_data = QADataset(\n        **cfg.dataset,\n        text_emb_model_cfgs=OmegaConf.create(model_config[\"text_emb_model_config\"]),\n    )\n    device = utils.get_device()\n    model = model.to(device)\n\n    qa_data.kg = qa_data.kg.to(device)\n    qa_data.ent2docs = qa_data.ent2docs.to(device)\n\n    if cfg.test.retrieved_result_path:\n        retrieval_result = torch.load(cfg.test.retrieved_result_path, weights_only=True)\n    else:\n        if cfg.test.prediction_result_path:\n            retrieval_result = None\n        else:\n            retrieval_result = doc_retrieval(cfg, model, qa_data, device=device)\n    if utils.is_main_process():\n        if cfg.test.save_retrieval and retrieval_result is not None:\n            logger.info(\n                f\"Ranking saved to disk: {os.path.join(output_dir, 'retrieval_result.pt')}\"\n            )\n            torch.save(\n                retrieval_result, os.path.join(output_dir, \"retrieval_result.pt\")\n            )\n        if cfg.test.prediction_result_path:\n            output_path = cfg.test.prediction_result_path\n        else:\n            output_path = ans_prediction(cfg, output_dir, qa_data, retrieval_result)\n\n        # Evaluation\n        evaluator = instantiate(cfg.qa_evaluator, prediction_file=output_path)\n        metrics = evaluator.evaluate()\n        query_utils.print_metrics(metrics, logger)\n        return metrics\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.stage3_qa_inference\n# Multi-GPU retrieval\ntorchrun --nproc_per_node=4 -m gfmrag.workflow.stage3_qa_inference\n</code></pre> <p>You can overwrite the configuration like this:</p> Bash<pre><code>python -m gfmrag.workflow.stage3_qa_inference test.retrieval_batch_size=4\n</code></pre>"},{"location":"workflow/kg_index/","title":"KG-index Construction","text":"<p>This guide explains how to create a knowledge graph index and process QA data that can be used by GFM-RAG for training and testing.</p>"},{"location":"workflow/kg_index/#data-preparation","title":"Data Preparation","text":"<p>Please follow the instructions in the Data Preparation to prepare your dataset in the following structure:</p> Text Only<pre><code>data_name/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 dataset_corpus.json\n\u2502   \u251c\u2500\u2500 train.json # (optional)\n\u2502   \u2514\u2500\u2500 test.json # (optional)\n\u2514\u2500\u2500 processed/ # Output directory\n</code></pre>"},{"location":"workflow/kg_index/#config","title":"Config","text":"<p>You need to create a KG-index configuration file.</p> gfmrag/workflow/config/stage1_index_dataset.yaml gfmrag/workflow/config/stage1_index_dataset.yaml<pre><code>hydra:\n  run:\n    dir: outputs/kg_construction/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - ner_model: llm_ner_model # The NER model to use\n  - openie_model: llm_openie_model # The OpenIE model to use\n  - el_model: colbert_el_model # The EL model to use\n\ndataset:\n  root: ./data # data root directory\n  data_name: hotpotqa # data name\n\nkg_constructor:\n  _target_: gfmrag.kg_construction.KGConstructor # The KGConstructor class\n  open_ie_model: ${openie_model}\n  ner_model: ${ner_model}\n  el_model: ${el_model}\n  root: tmp/kg_construction # Temporary directory for storing intermediate files during KG construction\n  num_processes: 10 # Number of processes to use\n  cosine_sim_edges: True # Whether to conduct entities resolution using cosine similarity\n  threshold: 0.8 # Threshold for cosine similarity\n  max_sim_neighbors: 100 # Maximum number of similar neighbors to add\n  add_title: True # Whether to add the title to the content of the document during OpenIE\n  force: False # Whether to force recompute the KG\n\nqa_constructor:\n  _target_: gfmrag.kg_construction.QAConstructor # The QAConstructor class\n  root: tmp/qa_construction # Temporary directory for storing intermediate files during QA construction\n  ner_model: ${ner_model}\n  el_model: ${el_model}\n  num_processes: 10 # Number of processes to use\n  force: False # Whether to force recompute the QA data\n</code></pre> <p>Details of the configuration parameters are explained in the KG-index Configuration page.</p>"},{"location":"workflow/kg_index/#index-dataset","title":"Index dataset","text":"<p>This method performs two main tasks:</p> <ol> <li>Creates and saves knowledge graph related files (<code>kg.txt</code> and <code>document2entities.json</code>) from the <code>dataset_corpus.json</code> file</li> <li>Identify the query entities and supporting entities in training and testing data if available in the raw data directory</li> </ol> <p>Files created:</p> <ul> <li><code>kg.txt</code>: Contains knowledge graph triples</li> <li><code>document2entities.json</code>: Maps documents to their entities</li> <li><code>train.json</code>: Processed training data (if raw exists)</li> <li><code>test.json</code>: Processed test data (if raw exists)</li> </ul> <p>Directory structure: Text Only<pre><code>root/\n\u2514\u2500\u2500 data_name/\n    \u251c\u2500\u2500 raw/\n    \u2502   \u251c\u2500\u2500 dataset_corpus.json\n    \u2502   \u251c\u2500\u2500 train.json (optional)\n    \u2502   \u2514\u2500\u2500 test.json (optional)\n    \u2514\u2500\u2500 processed/\n        \u2514\u2500\u2500 stage1/\n            \u251c\u2500\u2500 kg.txt\n            \u251c\u2500\u2500 document2entities.json\n            \u251c\u2500\u2500 train.json\n            \u2514\u2500\u2500 test.json\n</code></pre></p> <p>To index the data, run the following command:</p> gfmrag/workflow/stage1_index_dataset.py <p> gfmrag/workflow/stage1_index_dataset.py<pre><code>import logging\nimport os\n\nimport dotenv\nimport hydra\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom gfmrag import KGIndexer\nfrom gfmrag.kg_construction import KGConstructor, QAConstructor\n\nlogger = logging.getLogger(__name__)\n\ndotenv.load_dotenv()\n\n\n@hydra.main(config_path=\"config\", config_name=\"stage1_index_dataset\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    output_dir = HydraConfig.get().runtime.output_dir\n    logger.info(f\"Config:\\n {OmegaConf.to_yaml(cfg)}\")\n    logger.info(f\"Current working directory: {os.getcwd()}\")\n    logger.info(f\"Output directory: {output_dir}\")\n\n    kg_constructor = KGConstructor.from_config(cfg.kg_constructor)\n    qa_constructor = QAConstructor.from_config(cfg.qa_constructor)\n\n    kg_indexer = KGIndexer(kg_constructor, qa_constructor)\n    kg_indexer.index_data(cfg.dataset)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.stage1_index_dataset\n</code></pre> <p>You can overwrite the configuration like this:</p> Bash<pre><code>python -m gfmrag.workflow.stage1_index_dataset kg_constructor.num_processes=5\n</code></pre>"},{"location":"workflow/kg_index/#output-files","title":"Output Files","text":""},{"location":"workflow/kg_index/#kgtxt","title":"<code>kg.txt</code>","text":"<p>The <code>kg.txt</code> file contains the knowledge graph triples in the following format:</p> Text Only<pre><code>subject,relation,object\n</code></pre> <p>Example: Text Only<pre><code>fred gehrke,was,american football player\nfred gehrke,was,executive\nfred gehrke,played for,cleveland   los angeles rams\n</code></pre></p>"},{"location":"workflow/kg_index/#document2entitiesjson","title":"<code>document2entities.json</code>","text":"<p>The <code>document2entities.json</code> file contains the mapping of documents to their entities in the following format:</p> <ul> <li><code>key</code>: The title or unique id of the document.</li> <li><code>value</code>: A list of entities in the document.</li> </ul> <p>Example: JSON<pre><code>{\n    \"Fred Gehrke\": [\n        \"1977\",\n        \"1981\",\n        \"american football player\",\n        \"chicago cardinals\",\n        \"cleveland   los angeles rams\",\n        \"executive\",\n        \"first painted logo on helmets\",\n        \"fred gehrke\",\n        \"gehrke\",\n        \"general manager of denver broncos\",\n        \"los angeles rams\",\n        \"los angeles rams logo\",\n        \"miami marlin christian yelich\",\n        \"san francisco 49ers\"\n    ],\n    \"Manny Machado\": [\n        \"2010 major league baseball draft\",\n        \"american\",\n        \"baltimore orioles\",\n        \"brito high school\",\n        \"july 6  1992\",\n        \"major league baseball\",\n        \"manny machado\",\n        \"right handed\"\n    ],\n}\n</code></pre></p>"},{"location":"workflow/kg_index/#trainjson-and-testjson","title":"<code>train.json</code> and <code>test.json</code>","text":"<p>The <code>train.json</code> and <code>test.json</code> files contain the processed training and test data in the following format:</p> <ul> <li><code>id</code>: A unique identifier for the example.</li> <li><code>question</code>: The question or query.</li> <li><code>supporting_facts</code>: A list of supporting facts for the question. Each supporting fact is a list containing the title of the document that can be found in the <code>dataset_corpus.json</code> file.</li> <li><code>question_entities</code>: A list of entities in the question.</li> <li><code>supporting_entities</code>: A list of entities in the supporting facts.</li> <li>Additional fields copied from the raw data</li> </ul> <p>Examples JSON<pre><code>[\n    {\n        \"id\": \"5abc553a554299700f9d7871\",\n        \"question\": \"Kyle Ezell is a professor at what School of Architecture building at Ohio State?\",\n        \"answer\": \"Knowlton Hall\",\n        \"supporting_facts\": [\n            \"Knowlton Hall\",\n            \"Kyle Ezell\"\n        ],\n        \"question_entities\": [\n            \"kyle ezell\",\n            \"architectural association school of architecture\",\n            \"ohio state\"\n        ],\n        \"supporting_entities\": [\n            \"10 million donation\",\n            \"2004\",\n            \"architecture\",\n            \"austin e  knowlton\",\n            \"austin e  knowlton school of architecture\",\n            \"bachelor s in architectural engineering\",\n            \"city and regional planning\",\n            \"columbus  ohio  united states\",\n            \"ives hall\",\n            \"july 2002\",\n            \"knowlton hall\",\n            \"ksa\",\n            \"landscape architecture\",\n            \"ohio\",\n            \"replacement for ives hall\",\n            \"the ohio state university\",\n            \"the ohio state university in 1931\",\n            \"american urban planning practitioner\",\n            \"expressing local culture\",\n            \"knowlton school\",\n            \"kyle ezell\",\n            \"lawrenceburg  tennessee\",\n            \"professor\",\n            \"the ohio state university\",\n            \"theorist\",\n            \"undergraduate planning program\",\n            \"vibrant downtowns\",\n            \"writer\"\n        ]\n    },\n    ...\n]\n</code></pre></p>"},{"location":"workflow/path_interpretation/","title":"Path Interpretations","text":"<p>GFM-RAG exhibits the multi-hop reasoning ability powered by the multi-layer GFM. You can visualize the paths captured by the GFM-RAG to provide path interpretations for the prediction.</p> <p>The paths' importance to the final prediction can be quantified by the partial derivative of the prediction score with respect to the triples at each layer (hop), defined as:</p> \\[     s_1,s_2,\\ldots,s_L=\\arg\\mathop{\\text{top-}}k\\frac{\\partial p_e(q)}{\\partial s_l} \\] <p>The top-\\(k\\) path interpretations can be obtained by the top-\\(k\\) longest paths with beam search.</p> <p>You can visualize the paths and their importance using the following code:</p> gfmrag/workflow/config/exp_visualize_path.yaml gfmrag/workflow/config/exp_visualize_path.yaml<pre><code>hydra:\n  run:\n    dir: outputs/experiments/visualize/${dataset.data_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker\n\nseed: 1024\n\ndataset:\n  root: ./data\n  data_name: hotpotqa_test\n\ngraph_retriever:\n  model_path: rmanluo/GFM-RAG-8M\n\ntest:\n  retrieval_batch_size: 8\n  top_k: 5\n  save_retrieval: False\n  save_top_k_entity: 10\n  n_threads: 5\n  retrieved_result_path: null\n  prediction_result_path: null\n  init_entities_weight: True\n  max_sample: 10\n</code></pre> gfmrag/workflow/experiments/visualize_path.py <p> gfmrag/workflow/experiments/visualize_path.py<pre><code>--8&lt;--\"gfmrag/workflow/experiments/visualize_path.py\"\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.experiments.visualize_path\n</code></pre> <p>Path Examples</p> Text Only<pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nQuestion: What football club was owned by the singer of \"Grow Some Funk of Your Own\"?\nAnswer: Watford Football Club\nQuestion Entities: ['grow some funk of your own', 'football club']\nSupporting Facts: ['Grow Some Funk of Your Own', 'Elton John']\nPredicted Paths:\n--------------------------------------------------------\nTarget Entity: watford football club\n1.0950: [ grow some funk of your own, is a song by, elton john ] =&gt; [ elton john, equivalent, sir elton hercules john ] =&gt; [ sir elton hercules john, inverse_named a stand after, watford football club ]\n0.9154: [ grow some funk of your own, is a song by, elton john ] =&gt; [ elton john, equivalent, sir elton hercules john ] =&gt; [ sir elton hercules john, owned, watford football club ]\n</code></pre>"},{"location":"workflow/training/","title":"GFM-RAG Training","text":"<p>You can further fine-tune the pre-trained GFM-RAG model on your own dataset to improve the performance of the model on your specific domain.</p>"},{"location":"workflow/training/#data-preparation","title":"Data Preparation","text":"<p>Please follow the instructions in the Data Preparation to prepare your dataset in the following structure: Make sure to have the <code>train.json</code> to perform the fine-tuning.</p> Text Only<pre><code>data_name/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 dataset_corpus.json\n\u2502   \u251c\u2500\u2500 train.json\n\u2502   \u2514\u2500\u2500 test.json # (optional)\n\u2514\u2500\u2500 processed/\n    \u2514\u2500\u2500 stage1/\n        \u251c\u2500\u2500 kg.txt\n        \u251c\u2500\u2500 document2entities.json\n        \u251c\u2500\u2500 train.json\n        \u2514\u2500\u2500 test.json # (optional)\n</code></pre>"},{"location":"workflow/training/#gfm-fine-tuning","title":"GFM Fine-tuning","text":"<p>During fine-tuning, the GFM model will be trained on the query-documents pairs <code>train.json</code> from the labeled dataset to learn complex relationships for retrieval.</p> <p>It can be conducted on your own dataset to improve the performance of the model on your specific domain.</p> <p>An example of the training data:</p> JSON<pre><code>[\n    {\n        \"id\": \"5abc553a554299700f9d7871\",\n        \"question\": \"Kyle Ezell is a professor at what School of Architecture building at Ohio State?\",\n        \"answer\": \"Knowlton Hall\",\n        \"supporting_facts\": [\n            \"Knowlton Hall\",\n            \"Kyle Ezell\"\n        ],\n        \"question_entities\": [\n            \"kyle ezell\",\n            \"architectural association school of architecture\",\n            \"ohio state\"\n        ],\n        \"supporting_entities\": [\n            \"10 million donation\",\n            \"2004\",\n            \"architecture\",\n            \"austin e  knowlton\",\n            \"austin e  knowlton school of architecture\",\n            \"bachelor s in architectural engineering\",\n            \"city and regional planning\",\n            \"columbus  ohio  united states\",\n            \"ives hall\",\n            \"july 2002\",\n            \"knowlton hall\",\n            \"ksa\",\n        ]\n    },\n    ...\n]\n</code></pre> <p>Note</p> <p>We have already released the pre-trained model checkpoint, which can be used for further finetuning. The model will be automatically downloaded by specifying it in the configuration. YAML<pre><code>checkpoint: rmanluo/GFM-RAG-8M\n</code></pre></p> <p>You need to create a configuration file for fine-tuning.</p> gfmrag/workflow/config/stage2_qa_finetune.yaml gfmrag/workflow/config/stage2_qa_finetune.yaml<pre><code>hydra:\n  run:\n    dir: outputs/qa_finetune/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - doc_ranker: idf_topk_ranker # The document ranker to use\n  - text_emb_model: mpnet # The text embedding model to use\n\nseed: 1024\n\ndatasets:\n  _target_: gfmrag.datasets.QADataset # The QA dataset class\n  cfgs:\n    root: ./data # data root directory\n    force_rebuild: False # Whether to force rebuild the dataset\n    text_emb_model_cfgs: ${text_emb_model} # The text embedding model configuration\n  train_names: # List of training dataset names\n    - hotpotqa_train_example\n  valid_names: # List of validation dataset names\n    - hotpotqa_test\n    - musique_test\n    - 2wikimultihopqa_test\n\n# GFM model configuration\nmodel:\n  _target_: gfmrag.models.GNNRetriever\n  entity_model:\n    _target_: gfmrag.ultra.models.QueryNBFNet\n    input_dim: 512\n    hidden_dims: [512, 512, 512, 512, 512, 512]\n    message_func: distmult\n    aggregate_func: sum\n    short_cut: yes\n    layer_norm: yes\n\n# Loss configuration\ntask:\n  strict_negative: yes\n  metric:\n    [mrr, hits@1, hits@2, hits@3, hits@5, hits@10, hits@20, hits@50, hits@100]\n  losses:\n    - name: ent_bce_loss\n      loss:\n        _target_: gfmrag.losses.BCELoss\n        adversarial_temperature: 0.2\n      cfg:\n        weight: 0.3\n        is_doc_loss: False\n    - name: ent_pcr_loss\n      loss:\n        _target_: gfmrag.losses.ListCELoss\n      cfg:\n        weight: 0.7\n        is_doc_loss: False\n\n# Optimizer configuration\noptimizer:\n  _target_: torch.optim.AdamW\n  lr: 5.0e-4\n\n# Training configuration\ntrain:\n  batch_size: 8\n  num_epoch: 20\n  log_interval: 100\n  batch_per_epoch: null\n  save_best_only: yes\n  save_pretrained: yes # Save the model for QA inference\n  do_eval: yes\n  timeout: 60 # timeout minutes for multi-gpu training\n  init_entities_weight: True\n\n  checkpoint: null\n</code></pre> <p>Details of the configuration parameters are explained in the GFM-RAG Fine-tuning Configuration page.</p> <p>You can fine-tune the pre-trained GFM-RAG model on your dataset using the following command:</p> gfmrag/workflow/stage2_qa_finetune.py <p> gfmrag/workflow/stage2_qa_finetune.py<pre><code>import logging\nimport os\nfrom itertools import islice\n\nimport hydra\nimport numpy as np\nimport torch\nfrom hydra.core.hydra_config import HydraConfig\nfrom hydra.utils import instantiate\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch import nn\nfrom torch.nn import functional as F  # noqa:N812\nfrom torch.utils import data as torch_data\nfrom tqdm import tqdm\n\nfrom gfmrag import utils\nfrom gfmrag.ultra import query_utils\n\n# A logger for this file\nlogger = logging.getLogger(__name__)\n\nseparator = \"&gt;\" * 30\nline = \"-\" * 30\n\n\ndef train_and_validate(\n    cfg: DictConfig,\n    output_dir: str,\n    model: nn.Module,\n    train_datasets: dict,\n    valid_datasets: dict,\n    device: torch.device,\n    batch_per_epoch: int | None = None,\n) -&gt; None:\n    if cfg.train.num_epoch == 0:\n        return\n\n    world_size = utils.get_world_size()\n    rank = utils.get_rank()\n\n    # Create dataloader for each dataset\n    train_dataloader_dict = {}\n    for data_name, dataset in train_datasets.items():\n        train_data = dataset[\"data\"]\n        sampler = torch_data.DistributedSampler(train_data, world_size, rank)\n        train_loader = torch_data.DataLoader(\n            train_data, cfg.train.batch_size, sampler=sampler\n        )\n        train_dataloader_dict[data_name] = train_loader\n    data_name_list = list(train_dataloader_dict.keys())\n\n    batch_per_epoch = batch_per_epoch or len(train_loader)\n\n    optimizer = instantiate(cfg.optimizer, model.parameters())\n\n    num_params = sum(p.numel() for p in model.parameters())\n    logger.warning(line)\n    logger.warning(f\"Number of parameters: {num_params}\")\n\n    # Initialize Losses\n    loss_fn_list = []\n    has_doc_loss = False\n    for loss_cfg in cfg.task.losses:\n        loss_fn = instantiate(loss_cfg.loss)\n        if loss_cfg.cfg.is_doc_loss:\n            has_doc_loss = True\n        loss_fn_list.append(\n            {\n                \"name\": loss_cfg.name,\n                \"loss_fn\": loss_fn,\n                **loss_cfg.cfg,\n            }\n        )\n\n    if world_size &gt; 1:\n        parallel_model = nn.parallel.DistributedDataParallel(model, device_ids=[device])\n    else:\n        parallel_model = model\n\n    best_result = float(\"-inf\")\n    best_epoch = -1\n\n    batch_id = 0\n    for i in range(0, cfg.train.num_epoch):\n        epoch = i + 1\n        parallel_model.train()\n\n        if utils.get_rank() == 0:\n            logger.warning(separator)\n            logger.warning(f\"Epoch {epoch} begin\")\n\n        losses: dict[str, list] = {loss_dict[\"name\"]: [] for loss_dict in loss_fn_list}\n        losses[\"loss\"] = []\n\n        for dataloader in train_dataloader_dict.values():\n            dataloader.sampler.set_epoch(epoch)\n        # np.random.seed(epoch)  # TODO: should we use the same dataloader for all processes?\n        shuffled_data_name_list = np.random.permutation(\n            data_name_list\n        )  # Shuffle the dataloaders\n        for data_name in shuffled_data_name_list:\n            train_loader = train_dataloader_dict[data_name]\n            graph = train_datasets[data_name][\"graph\"]\n            ent2docs = train_datasets[data_name][\"ent2docs\"]\n            entities_weight = None\n            if cfg.train.init_entities_weight:\n                entities_weight = utils.get_entities_weight(ent2docs)\n            for batch in tqdm(\n                islice(train_loader, batch_per_epoch),\n                desc=f\"Training Batches: {epoch}\",\n                total=batch_per_epoch,\n                disable=not utils.is_main_process(),\n            ):\n                batch = query_utils.cuda(batch, device=device)\n                pred = parallel_model(graph, batch, entities_weight=entities_weight)\n                target = batch[\"supporting_entities_masks\"]  # supporting_entities_mask\n\n                if has_doc_loss:\n                    doc_pred = torch.sparse.mm(pred, ent2docs)\n                    doc_target = batch[\"supporting_docs_masks\"]  # supporting_docs_mask\n\n                loss = 0\n                tmp_losses = {}\n                for loss_dict in loss_fn_list:\n                    loss_fn = loss_dict[\"loss_fn\"]\n                    weight = loss_dict[\"weight\"]\n                    if loss_dict[\"is_doc_loss\"]:\n                        single_loss = loss_fn(doc_pred, doc_target)\n                    else:\n                        single_loss = loss_fn(pred, target)\n                    tmp_losses[loss_dict[\"name\"]] = single_loss.item()\n                    loss += weight * single_loss\n                tmp_losses[\"loss\"] = loss.item()  # type: ignore\n\n                loss.backward()  # type: ignore\n                optimizer.step()\n                optimizer.zero_grad()\n\n                for loss_log in tmp_losses:\n                    losses[loss_log].append(tmp_losses[loss_log])\n\n                if utils.get_rank() == 0 and batch_id % cfg.train.log_interval == 0:\n                    logger.warning(separator)\n                    for loss_log in tmp_losses:\n                        logger.warning(f\"{loss_log}: {tmp_losses[loss_log]:g}\")\n                batch_id += 1\n\n        if utils.get_rank() == 0:\n            logger.warning(separator)\n            logger.warning(f\"Epoch {epoch} end\")\n            logger.warning(line)\n            for loss_log in losses:\n                logger.warning(\n                    f\"Avg: {loss_log}: {sum(losses[loss_log]) / len(losses[loss_log]):g}\"\n                )\n\n        utils.synchronize()\n\n        if cfg.train.do_eval:\n            if rank == 0:\n                logger.warning(separator)\n                logger.warning(\"Evaluate on valid\")\n            result = test(cfg, model, valid_datasets, device=device)\n        else:\n            result = float(\"inf\")\n            best_result = float(\"-inf\")\n        if rank == 0:\n            if result &gt; best_result:\n                best_result = result\n                best_epoch = epoch\n                logger.warning(\"Save checkpoint to model_best.pth\")\n                state = {\n                    \"model\": model.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                }\n                torch.save(state, os.path.join(output_dir, \"model_best.pth\"))\n            if not cfg.train.save_best_only:\n                logger.warning(f\"Save checkpoint to model_epoch_{epoch}.pth\")\n                state = {\n                    \"model\": model.state_dict(),\n                    \"optimizer\": optimizer.state_dict(),\n                }\n                torch.save(state, os.path.join(output_dir, f\"model_epoch_{epoch}.pth\"))\n            logger.warning(f\"Best mrr: {best_result:g} at epoch {best_epoch}\")\n\n    if rank == 0:\n        logger.warning(\"Load checkpoint from model_best.pth\")\n    state = torch.load(\n        os.path.join(output_dir, \"model_best.pth\"),\n        map_location=device,\n        weights_only=False,\n    )\n    model.load_state_dict(state[\"model\"])\n    utils.synchronize()\n\n\n@torch.no_grad()\ndef test(\n    cfg: DictConfig,\n    model: nn.Module,\n    test_datasets: dict,\n    device: torch.device,\n    return_metrics: bool = False,\n) -&gt; float | dict:\n    world_size = utils.get_world_size()\n    rank = utils.get_rank()\n\n    # process sequentially of test datasets\n    all_metrics = {}\n    all_mrr = []\n    for data_name, q_data in test_datasets.items():\n        test_data = q_data[\"data\"]\n        graph = q_data[\"graph\"]\n        ent2docs = q_data[\"ent2docs\"]\n        sampler = torch_data.DistributedSampler(test_data, world_size, rank)\n        test_loader = torch_data.DataLoader(\n            test_data, cfg.train.batch_size, sampler=sampler\n        )\n\n        model.eval()\n        ent_preds = []\n        ent_targets = []\n        doc_preds = []\n        doc_targets = []\n\n        # Create doc retriever\n        doc_ranker = instantiate(\n            cfg.doc_ranker,\n            ent2doc=ent2docs,\n        )\n\n        entities_weight = None\n        if cfg.train.init_entities_weight:\n            entities_weight = utils.get_entities_weight(ent2docs)\n\n        for batch in tqdm(\n            test_loader,\n            desc=f\"Testing {data_name}\",\n            disable=not utils.is_main_process(),\n        ):\n            batch = query_utils.cuda(batch, device=device)\n            ent_pred = model(graph, batch, entities_weight=entities_weight)\n            doc_pred = doc_ranker(ent_pred)  # Ent2docs mapping\n            target_entities_mask = batch[\n                \"supporting_entities_masks\"\n            ]  # supporting_entities_mask\n            target_docs_mask = batch[\"supporting_docs_masks\"]  # supporting_docs_mask\n            target_entities = target_entities_mask.bool()\n            target_docs = target_docs_mask.bool()\n            ent_ranking, target_ent_ranking = utils.batch_evaluate(\n                ent_pred, target_entities\n            )\n            doc_ranking, target_doc_ranking = utils.batch_evaluate(\n                doc_pred, target_docs\n            )\n\n            # answer set cardinality prediction\n            ent_prob = F.sigmoid(ent_pred)\n            num_pred = (ent_prob * (ent_prob &gt; 0.5)).sum(dim=-1)\n            num_target = target_entities_mask.sum(dim=-1)\n            ent_preds.append((ent_ranking, num_pred))\n            ent_targets.append((target_ent_ranking, num_target))\n\n            # document set cardinality prediction\n            doc_prob = F.sigmoid(doc_pred)\n            num_pred = (doc_prob * (doc_prob &gt; 0.5)).sum(dim=-1)\n            num_target = target_docs_mask.sum(dim=-1)\n            doc_preds.append((doc_ranking, num_pred))\n            doc_targets.append((target_doc_ranking, num_target))\n\n        ent_pred = query_utils.cat(ent_preds)\n        ent_target = query_utils.cat(ent_targets)\n        doc_pred = query_utils.cat(doc_preds)\n        doc_target = query_utils.cat(doc_targets)\n\n        ent_pred, ent_target = utils.gather_results(\n            ent_pred, ent_target, rank, world_size, device\n        )\n        doc_pred, doc_target = utils.gather_results(\n            doc_pred, doc_target, rank, world_size, device\n        )\n        ent_metrics = utils.evaluate(ent_pred, ent_target, cfg.task.metric)\n        metrics = {}\n        if rank == 0:\n            doc_metrics = utils.evaluate(doc_pred, doc_target, cfg.task.metric)\n            for key, value in ent_metrics.items():\n                metrics[f\"ent_{key}\"] = value\n            for key, value in doc_metrics.items():\n                metrics[f\"doc_{key}\"] = value\n            metrics[\"mrr\"] = ent_metrics[\"mrr\"]\n            logger.warning(f\"{'-' * 15} Test on {data_name} {'-' * 15}\")\n            query_utils.print_metrics(metrics, logger)\n        else:\n            metrics[\"mrr\"] = ent_metrics[\"mrr\"]\n        all_metrics[data_name] = metrics\n        all_mrr.append(metrics[\"mrr\"])\n    utils.synchronize()\n    all_avg_mrr = np.mean(all_mrr)\n    return all_avg_mrr if not return_metrics else metrics\n\n\n@hydra.main(config_path=\"config\", config_name=\"stage2_qa_finetune\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    output_dir = HydraConfig.get().runtime.output_dir\n    utils.init_distributed_mode(cfg.train.timeout)\n    torch.manual_seed(cfg.seed + utils.get_rank())\n    if utils.get_rank() == 0:\n        logger.info(f\"Config:\\n {OmegaConf.to_yaml(cfg)}\")\n        logger.info(f\"Current working directory: {os.getcwd()}\")\n        logger.info(f\"Output directory: {output_dir}\")\n\n    qa_datasets = utils.get_multi_dataset(cfg)\n    device = utils.get_device()\n\n    rel_emb_dim = {qa_data.rel_emb_dim for qa_data in qa_datasets.values()}\n    assert len(rel_emb_dim) == 1, (\n        \"All datasets should have the same relation embedding dimension\"\n    )\n\n    model = instantiate(cfg.model, rel_emb_dim=rel_emb_dim.pop())\n\n    train_datasets = {}\n    valid_datasets = {}\n\n    for data_name, qa_data in qa_datasets.items():\n        if data_name not in cfg.datasets.train_names + cfg.datasets.valid_names:\n            raise ValueError(f\"Unknown data name: {data_name}\")\n\n        train_data, valid_data = qa_data._data\n        graph = qa_data.kg.to(device)\n        ent2docs = qa_data.ent2docs.to(device)\n        if data_name in cfg.datasets.train_names:\n            train_datasets[data_name] = {\n                \"data\": train_data,\n                \"graph\": graph,\n                \"ent2docs\": ent2docs,\n            }\n        if data_name in cfg.datasets.valid_names:\n            valid_datasets[data_name] = {\n                \"data\": valid_data,\n                \"graph\": graph,\n                \"ent2docs\": ent2docs,\n            }\n\n    if \"checkpoint\" in cfg.train and cfg.train.checkpoint is not None:\n        if os.path.exists(cfg.train.checkpoint):\n            state = torch.load(cfg.train.checkpoint, map_location=\"cpu\")\n            model.load_state_dict(state[\"model\"])\n        # Try to load the model from the remote dictionary\n        else:\n            model, _ = utils.load_model_from_pretrained(cfg.train.checkpoint)\n\n    model = model.to(device)\n    train_and_validate(\n        cfg,\n        output_dir,\n        model,\n        train_datasets,\n        valid_datasets,\n        device=device,\n        batch_per_epoch=cfg.train.batch_per_epoch,\n    )\n\n    if cfg.train.do_eval:\n        if utils.get_rank() == 0:\n            logger.warning(separator)\n            logger.warning(\"Evaluate on valid\")\n        test(cfg, model, valid_datasets, device=device)\n\n    # Save the model into the format for QA inference\n    if (\n        utils.is_main_process()\n        and cfg.train.save_pretrained\n        and cfg.train.num_epoch &gt; 0\n    ):\n        pre_trained_dir = os.path.join(output_dir, \"pretrained\")\n        utils.save_model_to_pretrained(model, cfg, pre_trained_dir)\n\n    utils.synchronize()\n    utils.cleanup()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.stage2_qa_finetune\n# Multi-GPU training\ntorchrun --nproc_per_node=4 gfmrag.workflow.stage2_qa_finetune\n# Multi-node Multi-GPU training\ntorchrun --nproc_per_node=4 --nnodes=2 gfmrag.workflow.stage2_qa_finetune\n</code></pre> <p>You can overwrite the configuration like this:</p> Bash<pre><code>python -m gfmrag.workflow.stage2_qa_finetune train.batch_size=4\n</code></pre>"},{"location":"workflow/training/#gfm-pre-training","title":"GFM Pre-training","text":"<p>During pre-training, the GFM model will sample triples from the KG-index <code>kg.txt</code> to construct synthetic queries and target entities for training.</p> <p>Tip</p> <p>It is only recommended to conduct pre-training when you want to train the model from scratch or when you have a large amount of unlabeled data.</p> <p>Tip</p> <p>It is recommended to conduct fine-tuning after the pre-training to empower the model with the ability to understand user queries and retrieve relevant documents.</p> <p>An example of the KG-index:</p> Text Only<pre><code>fred gehrke,was,american football player\nfred gehrke,was,executive\nfred gehrke,played for,cleveland   los angeles rams\n</code></pre> <p>You need to create a configuration file for pre-training.</p> gfmrag/workflow/config/stage2_kg_pretrain.yaml gfmrag/workflow/config/stage2_kg_pretrain.yaml<pre><code>hydra:\n  run:\n    dir: outputs/kg_pretrain/${now:%Y-%m-%d}/${now:%H-%M-%S} # Output directory\n\ndefaults:\n  - _self_\n  - text_emb_model: mpnet # The text embedding model to use\n\nseed: 1024\n\ndatasets:\n  _target_: gfmrag.datasets.KGDataset # The KG dataset class\n  cfgs:\n    root: ./data # data root directory\n    force_rebuild: False # Whether to force rebuild the dataset\n    text_emb_model_cfgs: ${text_emb_model} # The text embedding model configuration\n  train_names: # List of training dataset names\n    - hotpotqa_train_example\n  valid_names: []\n\n# GFM model configuration\nmodel:\n  _target_: gfmrag.models.QueryGNN\n  entity_model:\n    _target_: gfmrag.ultra.models.EntityNBFNet\n    input_dim: 512\n    hidden_dims: [512, 512, 512, 512, 512, 512]\n    message_func: distmult\n    aggregate_func: sum\n    short_cut: yes\n    layer_norm: yes\n\n# Loss configuration\ntask:\n  num_negative: 256\n  strict_negative: yes\n  adversarial_temperature: 1\n  metric: [mr, mrr, hits@1, hits@3, hits@10]\n\noptimizer:\n  _target_: torch.optim.AdamW\n  lr: 5.0e-4\n\n# Training configuration\ntrain:\n  batch_size: 8\n  num_epoch: 10\n  log_interval: 100\n  fast_test: 500\n  save_best_only: no\n  save_pretrained: no # Save the model for QA inference\n  batch_per_epoch: null\n  timeout: 60 # timeout minutes for multi-gpu training\n  # Checkpoint configuration\n  checkpoint: null\n</code></pre> <p>Details of the configuration parameters are explained in the GFM-RAG Pre-training Config page.</p> <p>You can pre-train the GFM-RAG model on your dataset using the following command:</p> gfmrag/workflow/stage2_kg_pretrain.py <p> gfmrag/workflow/stage2_kg_pretrain.py<pre><code>--8&lt;--\"gfmrag/workflow/stage2_kg_pretrain.py\"\n</code></pre> </p> Bash<pre><code>python -m gfmrag.workflow.stage2_kg_pretrain\n# Multi-GPU training\ntorchrun --nproc_per_node=4 gfmrag.workflow.stage2_kg_pretrain\n# Multi-node Multi-GPU training\ntorchrun --nproc_per_node=4 --nnodes=2 gfmrag.workflow.stage2_kg_pretrain\n</code></pre> <p>You can overwrite the configuration like this:</p> Bash<pre><code>python -m gfmrag.workflow.stage2_kg_pretrain train.batch_size=4\n</code></pre>"}]}